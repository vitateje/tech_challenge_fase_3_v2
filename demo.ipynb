{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BioByIA: Pipeline Completo de Intelig√™ncia Artificial M√©dica\n",
                "\n",
                "Este notebook demonstra o pipeline completo do projeto **BioByIA**, abrangendo desde a prepara√ß√£o de dados e fine-tuning do modelo Llama-3-8B at√© a implementa√ß√£o de um sistema RAG (Retrieval-Augmented Generation) para respostas m√©dicas fundamentadas.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup e Importa√ß√µes\n",
                "\n",
                "Prepare o ambiente instalando as depend√™ncias e importando os m√≥dulos necess√°rios."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.1 INSTALA√á√ÉO\n",
                "print(\"üì¶ Instalando depend√™ncias...\")\n",
                "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install -q --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
                "!pip install -q transformers datasets pinecone-client langchain langchain-community langchain-google-genai google-generativeai python-dotenv tenacity tiktoken sentence-transformers tqdm\n",
                "print(\"‚úÖ Depend√™ncias instaladas!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.2 IMPORTA√á√ïES\n",
                "import os\n",
                "import torch\n",
                "import json\n",
                "import re\n",
                "import google.generativeai as genai\n",
                "from pathlib import Path\n",
                "from datasets import load_dataset\n",
                "from pinecone import Pinecone\n",
                "from unsloth import FastLanguageModel, is_bfloat16_supported, get_chat_template\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
                "\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    COLAB_AVAILABLE = True\n",
                "except ImportError:\n",
                "    COLAB_AVAILABLE = False\n",
                "\n",
                "print(\"‚úÖ Importa√ß√µes conclu√≠das!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configura√ß√£o de Credenciais\n",
                "\n",
                "Para que o RAG e o Gemini funcionem, voc√™ precisa definir suas chaves de API. No Colab, use o menu **Secrets** (√≠cone de chave üîë)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 RECUPERA√á√ÉO DE KEYS\n",
                "def get_api_key(name):\n",
                "    if COLAB_AVAILABLE:\n",
                "        try:\n",
                "            return userdata.get(name)\n",
                "        except:\n",
                "            return os.environ.get(name, \"\")\n",
                "    return os.environ.get(name, \"\")\n",
                "\n",
                "PINECONE_KEY = get_api_key('PINECONE_API_KEY')\n",
                "GEMINI_KEY = get_api_key('GEMINI_API_KEY')\n",
                "\n",
                "if not PINECONE_KEY or not GEMINI_KEY:\n",
                "    print(\"‚ö†Ô∏è  ATEN√á√ÉO: Chaves n√£o encontradas! Certifique-se de adicion√°-las aos 'Secrets' do Colab com os nomes:\")\n",
                "    print(\"   - PINECONE_API_KEY\")\n",
                "    print(\"   - GEMINI_API_KEY\")\n",
                "else:\n",
                "    os.environ[\"PINECONE_API_KEY\"] = PINECONE_KEY\n",
                "    os.environ[\"GEMINI_API_KEY\"] = GEMINI_KEY\n",
                "    print(\"‚úÖ Chaves configuradas com sucesso!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pipeline de Fine-Tuning\n",
                "\n",
                "Prepara√ß√£o do modelo BioByIA com LoRA e ChatML."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1 SETUP DO MODELO\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = MODEL_NAME,\n",
                "    max_seq_length = MAX_SEQ_LENGTH,\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 3407,\n",
                ")\n",
                "print(\"‚úÖ Modelo e LoRA prontos!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Pipeline RAG\n",
                "\n",
                "Configura√ß√£o de busca vetorial no Pinecone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 INDEX & EMBEDDINGS\n",
                "PINECONE_INDEX_NAME = \"biobyia\"\n",
                "PINECONE_NAMESPACE = \"medical_qa\"\n",
                "\n",
                "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\", \"\"))\n",
                "index = pc.Index(PINECONE_INDEX_NAME)\n",
                "\n",
                "embeddings_gen = GoogleGenerativeAIEmbeddings(\n",
                "    model=\"text-embedding-004\",\n",
                "    google_api_key=os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
                ")\n",
                "\n",
                "def query_rag_context(query, top_k=5):\n",
                "    query_vector = embeddings_gen.embed_query(query)\n",
                "    response = index.query(vector=query_vector, top_k=top_k, include_metadata=True, namespace=PINECONE_NAMESPACE)\n",
                "    return [{\n",
                "        \"id\": m['metadata'].get('article_id', 'N/A'),\n",
                "        \"source\": m['metadata'].get('source', 'N/A'), \n",
                "        \"text\": m['metadata'].get('text', ''),\n",
                "        \"score\": m['score']\n",
                "    } for m in response['matches']]\n",
                "print(\"‚úÖ Conex√£o RAG estabelecida!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Infer√™ncia e Demonstra√ß√£o\n",
                "\n",
                "Compara√ß√£o entre o Gemini Puro e o sistema completo BioByIA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 CARREGAR ADAPTER\n",
                "ADAPTER_HF = \"vitateje/biobyai\"\n",
                "print(f\"üì• Carregando Adapter: {ADAPTER_HF}\")\n",
                "model_ft, tokenizer_ft = FastLanguageModel.from_pretrained(\n",
                "    model_name = ADAPTER_HF,\n",
                "    max_seq_length = 2048,\n",
                "    load_in_4bit = True,\n",
                ")\n",
                "FastLanguageModel.for_inference(model_ft)\n",
                "print(\"‚úÖ BioByIA pronto!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def inferencia_gemini_puro(pergunta):\n",
                "    \"\"\"Teste sem Fine-tuning e sem RAG (Gemini Direto).\"\"\"\n",
                "    key = os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
                "    if not key: raise ValueError(\"API Key do Gemini n√£o encontrada!\")\n",
                "    \n",
                "    genai.configure(api_key=key)\n",
                "    model_base = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
                "    \n",
                "    print(\"\\n--- [TESTE 1] GEMINI FLASH-LITE (SEM RAG/FT) ---\")\n",
                "    try:\n",
                "        response = model_base.generate_content(pergunta)\n",
                "        print(f\"Resposta: {response.text.strip()}\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Erro na API do Gemini: {e}\")\n",
                "\n",
                "def inferencia_biobyia_rag(pergunta):\n",
                "    \"\"\"Teste com Fine-tuning + RAG (BioByIA).\"\"\"\n",
                "    print(\"\\n--- [TESTE 2] BIOBYIA (RAG + FINE-TUNING) ---\")\n",
                "    \n",
                "    # 1. Busca Contexto\n",
                "    results = query_rag_context(pergunta)\n",
                "    context_text = \"\\n\\n\".join([f\"[Fonte: {r['id']}]: {r['text']}\" for r in results])\n",
                "    \n",
                "    # 2. Prompt\n",
                "    prompt = f\"\"\"Responda como um assistente m√©dico especialista.\n",
                "Baseie sua resposta no contexto abaixo e cite os IDs das fontes.\n",
                "\n",
                "CONTEXTO:\n",
                "{context_text}\n",
                "\n",
                "PERGUNTA: {pergunta}\n",
                "\n",
                "RESPOSTA:\"\"\"\n",
                "    \n",
                "    inputs = tokenizer_ft(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    outputs = model_ft.generate(**inputs, max_new_tokens=400)\n",
                "    resposta = tokenizer_ft.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    if \"RESPOSTA:\" in resposta: resposta = resposta.split(\"RESPOSTA:\")[-1].strip()\n",
                "        \n",
                "    print(f\"Resposta: {resposta}\")\n",
                "    print(\"\\nRastreabilidade (Fontes Utilizadas):\")\n",
                "    for r in results:\n",
                "        print(f\"- {r['id']} (Similaridade: {r['score']:.4f})\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pergunta_final = \"Explique o conceito de edi√ß√£o gen√©tica de forma simples.\"\n",
                "\n",
                "inferencia_gemini_puro(pergunta_final)\n",
                "inferencia_biobyia_rag(pergunta_final)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
