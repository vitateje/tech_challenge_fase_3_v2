const { Pinecone } = require('@pinecone-database/pinecone');
const { PineconeStore } = require('@langchain/pinecone');
const { GoogleGenerativeAIEmbeddings } = require('@langchain/google-genai');
const { OllamaEmbeddings } = require('@langchain/ollama');
const langchainConfig = require('../config');

/**
 * Pinecone Patient Store
 * Respons√°vel por buscar contexto de pacientes no Pinecone.
 *
 * Configura√ß√£o do √≠ndice Pinecone:
 *  - Index: biobyia
 *  - Metric: cosine
 *  - Dimensions: 1024
 *  - Model: llama-text-embed-v2
 *  - Region: us-east-1 (AWS)
 *  - Type: Dense
 *  - Capacity mode: On-demand
 *  - Host: https://biobyia-c9udx7w.svc.aped-4627-b74a.pinecone.io
 *
 * IMPORTANTE: O √≠ndice foi criado com embeddings do modelo llama-text-embed-v2 (1024 dimens√µes).
 * Para queries, √© recomendado usar o mesmo modelo de embedding ou um compat√≠vel com 1024 dimens√µes.
 *
 * Configura√ß√£o de Embeddings (prioridade):
 *  1. Gemini: Configure GEMINI_API_KEY (usa text-embedding-004, 768 dimens√µes - RECOMENDADO)
 *  2. Ollama: Configure OLLAMA_BASE_URL e EMBEDDING_MODEL (apenas se necess√°rio)
 *
 * IMPORTANTE: O modelo llama-text-embed-v2 pode n√£o estar dispon√≠vel no Ollama.
 * Recomenda-se usar Gemini embeddings (text-embedding-004) que funciona bem mesmo com
 * √≠ndices criados com 1024 dimens√µes devido √† compatibilidade de similaridade de cosseno.
 *
 * Para usar Ollama (se necess√°rio):
 *  - Instale Ollama: https://ollama.ai
 *  - Baixe um modelo de embedding dispon√≠vel: ollama pull mxbai-embed-large (1024 dims)
 *  - Configure no .env:
 *    OLLAMA_BASE_URL=http://localhost:11434
 *    EMBEDDING_MODEL=mxbai-embed-large
 *
 * Configura√ß√£o via vari√°veis de ambiente:
 *  - PINECONE_API_KEY   (token de API)
 *  - PINECONE_INDEX_NAME (nome do √≠ndice, padr√£o: biobyia)
 *  - PINECONE_NAMESPACE (namespace opcional para separar dados)
 *  - EMBEDDING_MODEL    (modelo de embedding para Ollama, padr√£o: llama-text-embed-v2)
 *
 * As entradas devem ter metadata contendo ao menos:
 *  - patient_id: identificador l√≥gico (UUID/string) que ser√° recebido do frontend.
 */

const PINECONE_API_KEY = process.env.PINECONE_API_KEY;
const PINECONE_INDEX_NAME = process.env.PINECONE_INDEX_NAME || 'biobyia';
const PINECONE_NAMESPACE = process.env.PINECONE_NAMESPACE || '';

let pineconeClient = null;
let vectorStore = null;

function getPineconeClient() {
  if (!pineconeClient) {
    pineconeClient = new Pinecone({
      apiKey: PINECONE_API_KEY
    });
  }
  return pineconeClient;
}

async function getEmbeddingsModel() {
  const provider = langchainConfig.provider;
  const providers = langchainConfig.getAvailableProviders();

  // Prioriza Gemini embeddings (geralmente tem API key configurada e funciona bem)
  if (providers.gemini && providers.gemini.apiKey) {
    try {
      return new GoogleGenerativeAIEmbeddings({
        model: 'text-embedding-004',
        apiKey: providers.gemini.apiKey,
      });
    } catch (error) {
      console.warn('‚ö†Ô∏è Erro ao inicializar Gemini embeddings:', error.message);
    }
  }

  // Tenta Ollama embeddings apenas se explicitamente configurado
  if (providers.ollama && process.env.EMBEDDING_MODEL) {
    try {
      const embeddingModel = process.env.EMBEDDING_MODEL;
      return new OllamaEmbeddings({
        model: embeddingModel,
        baseUrl: providers.ollama.baseUrl || 'http://localhost:11434',
      });
    } catch (error) {
      console.warn('‚ö†Ô∏è Erro ao inicializar Ollama embeddings:', error.message);
      console.warn('üí° Dica: Verifique se o modelo est√° instalado: ollama pull ' + process.env.EMBEDDING_MODEL);
      // Continua para tentar Gemini como fallback
    }
  }

  // Se Gemini n√£o funcionou e Ollama n√£o est√° configurado, tenta Gemini novamente como √∫ltima op√ß√£o
  if (providers.gemini && providers.gemini.apiKey) {
    return new GoogleGenerativeAIEmbeddings({
      model: 'text-embedding-004',
      apiKey: providers.gemini.apiKey,
    });
  }

  // Se nenhum provider estiver dispon√≠vel, lan√ßa erro informativo
  throw new Error(
    'Nenhum modelo de embedding configurado ou dispon√≠vel. ' +
    'Configure GEMINI_API_KEY (recomendado) ou configure OLLAMA_BASE_URL com EMBEDDING_MODEL. ' +
    'Nota: O modelo llama-text-embed-v2 pode n√£o estar dispon√≠vel no Ollama. ' +
    'Use Gemini embeddings (text-embedding-004) que funciona bem com o √≠ndice biobyia.'
  );
}

async function getVectorStore() {
  if (vectorStore) return vectorStore;

  try {
    const embeddings = await getEmbeddingsModel();
    const pc = getPineconeClient();
    const index = pc.index(PINECONE_INDEX_NAME);

    vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
      pineconeIndex: index,
      namespace: PINECONE_NAMESPACE || undefined,
    });

    console.log('‚úÖ Pinecone vector store inicializado com sucesso');
    return vectorStore;
  } catch (error) {
    console.error('‚ùå Error initializing Pinecone vector store:', error.message || error);
    
    // Se o erro for relacionado a modelo n√£o encontrado, tenta limpar o cache e usar Gemini
    if (error.message && error.message.includes('not found')) {
      console.warn('‚ö†Ô∏è Modelo de embedding n√£o encontrado. Tentando usar Gemini como fallback...');
      vectorStore = null; // Limpa cache para tentar novamente
      
      // For√ßa uso de Gemini se dispon√≠vel
      const providers = langchainConfig.getAvailableProviders();
      if (providers.gemini && providers.gemini.apiKey) {
        try {
          const embeddings = new GoogleGenerativeAIEmbeddings({
            model: 'text-embedding-004',
            apiKey: providers.gemini.apiKey,
          });
          const pc = getPineconeClient();
          const index = pc.index(PINECONE_INDEX_NAME);
          
          vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
            pineconeIndex: index,
            namespace: PINECONE_NAMESPACE || undefined,
          });
          
          console.log('‚úÖ Pinecone vector store inicializado com Gemini embeddings (fallback)');
          return vectorStore;
        } catch (fallbackError) {
          console.error('‚ùå Erro no fallback para Gemini:', fallbackError.message);
        }
      }
    }
    
    throw error;
  }
}

/**
 * Busca contexto de paciente no Pinecone.
 *
 * @param {string} patientId
 * @param {string} query
 * @param {object} options
 * @returns {Promise<{chunks: Array<{id: string, text: string, metadata: object}>}>}
 */
async function getPatientContext(patientId, query, options = {}) {
  const { nResults = 5 } = options;

  if (!patientId) {
    return { chunks: [] };
  }

  try {
    const store = await getVectorStore();

    // Pinecone usa filter para metadata
    const filter = {
      patient_id: { $eq: patientId }
    };

    // Usa similaritySearch com filter como terceiro par√¢metro
    const results = await store.similaritySearch(
      query || '',
      nResults,
      filter
    );

    const chunks = results.map((doc, idx) => ({
      id: doc.id || `${patientId}-${idx}`,
      text: doc.pageContent,
      metadata: doc.metadata || {},
    }));

    return { chunks };
  } catch (error) {
    console.error('‚ùå Error querying Pinecone patient context:', error.message || error);
    // Fail gracefully: no RAG context, but the main flow can continue
    return { chunks: [] };
  }
}

module.exports = {
  getPatientContext,
};

