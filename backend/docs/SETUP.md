# üöÄ Guia de Setup - quimicAI Backend

## Pr√©-requisitos

- Node.js 18+
- npm ou yarn
- MongoDB (para persist√™ncia de dados)
- Conta com API key do Gemini ou OpenAI (opcional, para LLM)

## Instala√ß√£o R√°pida

### 1. Instalar Depend√™ncias

```bash
cd backend
npm install
```

### 2. Configurar Vari√°veis de Ambiente

Copie o arquivo `.env.example` para `.env` e configure:

```bash
cp .env.example .env
```

### 3. Configura√ß√£o B√°sica (.env)

```bash
# Servidor
BACKEND_PORT=4000
NODE_ENV=development

# MongoDB
MONGODB_URI=mongodb://localhost:27017/quimicai

# CORS
CORS_ORIGIN=http://localhost:3000

# LLM Provider (gemini, openai, ou ollama)
LLM_PROVIDER=gemini

# Gemini (se usar Gemini)
GEMINI_API_KEY=sua_chave_aqui
GEMINI_MODEL=gemini-pro

# OpenAI (se usar OpenAI)
OPENAI_API_KEY=sua_chave_aqui
OPENAI_MODEL=gpt-3.5-turbo

# Ollama (se usar Ollama local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma2:2b

# Vector Store (ChromaDB ou Pinecone)
USE_CHROMADB=true
# ou
USE_PINECONE=true
PINECONE_API_KEY=sua_chave_pinecone
PINECONE_INDEX_NAME=quimicai
```

### 4. Iniciar MongoDB

```bash
# Com Docker
docker run -d -p 27017:27017 --name mongodb mongo:latest

# Ou use MongoDB local/cloud
```

### 5. Executar Aplica√ß√£o

```bash
# Desenvolvimento
npm run dev

# Produ√ß√£o
npm start
```

## Configura√ß√£o de LLM

### Gemini (Recomendado - Gratuito)

1. Acesse: https://makersuite.google.com/app/apikey
2. Crie uma API key
3. Configure no `.env`:
```bash
LLM_PROVIDER=gemini
GEMINI_API_KEY=sua_chave_aqui
GEMINI_MODEL=gemini-pro  # Padr√£o: gemini-pro
GEMINI_TEMPERATURE=0.7   # Opcional
GEMINI_MAX_TOKENS=1000   # Opcional
```

### OpenAI

1. Acesse: https://platform.openai.com/api-keys
2. Crie uma API key
3. Configure no `.env`:
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sua_chave_aqui
OPENAI_MODEL=gpt-3.5-turbo  # Opcional
OPENAI_TEMPERATURE=0.7      # Opcional
OPENAI_MAX_TOKENS=500      # Opcional
```

### Ollama (Local)

1. Instale Ollama: https://ollama.ai
2. Baixe um modelo:
```bash
ollama pull gemma2:2b
# ou
ollama pull llama3
```
3. Configure no `.env`:
```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434  # Padr√£o
OLLAMA_MODEL=gemma2:2b
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=1000
```

## Vari√°veis de Ambiente Importantes

### Caracteres Especiais no .env

Se sua senha ou URI cont√©m caracteres especiais como `#`, use aspas:

```bash
# ‚úÖ CORRETO - Com aspas
MONGODB_URI="mongodb://user:senha#123@localhost:27017/quimicai"

# ‚ùå ERRADO - Sem aspas (o # ser√° interpretado como coment√°rio)
MONGODB_URI=mongodb://user:senha#123@localhost:27017/quimicai
```

## Vector Store

### ChromaDB (Padr√£o)

```bash
USE_CHROMADB=true
CHROMA_DB_PATH=./data/chroma
```

### Pinecone

```bash
USE_PINECONE=true
PINECONE_API_KEY=sua_chave
PINECONE_INDEX_NAME=quimicai
PINECONE_ENVIRONMENT=us-east-1
```

## Verificar Instala√ß√£o

```bash
# Testar LLM
npm test

# Verificar sa√∫de do servidor
curl http://localhost:4000/health
```

## Usu√°rio Padr√£o

Ap√≥s iniciar o servidor, um usu√°rio demo √© criado automaticamente:
- **Email**: `admin@quimicai.com`
- **Senha**: `demo@123`

## Pr√≥ximos Passos

- Configure o frontend: `../frontend/README.md`
- Veja documenta√ß√£o da API: `API.md`
- Guia de ingestion: `RAG_INGESTION.md`

