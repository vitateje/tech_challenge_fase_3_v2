# Planejamento Completo - Medical Assistant

## üìä An√°lise Geral do Projeto

O projeto √© um **Assistente Virtual M√©dico** composto por:

1. **Backend (Node.js/Express)**: API REST com LangChain, MongoDB, Pinecone
2. **Frontend (Vue.js)**: Interface web para chat m√©dico
3. **Fine-Tuning Pipeline (Python)**: Prepara√ß√£o e treinamento de modelo LLM m√©dico
4. **RAG Pipeline (Python)**: Ingest√£o de dados m√©dicos no Pinecone para busca sem√¢ntica

### Estado Atual
- ‚úÖ Backend estruturado com suporte a m√∫ltiplos providers (Gemini, Ollama, BiobyIA)
- ‚úÖ Frontend Vue.js funcional
- ‚úÖ Pipeline de fine-tuning completo (pr√©-processamento + treinamento)
- ‚úÖ Pipeline RAG com scripts modulares
- ‚ö†Ô∏è Modelo fine-tunado ainda n√£o foi treinado
- ‚ö†Ô∏è RAG ainda n√£o foi executado (dados n√£o est√£o no Pinecone)
- ‚ö†Ô∏è Integra√ß√£o entre fine-tuned model e backend n√£o est√° completa
- ‚ö†Ô∏è Integra√ß√£o RAG com backend parcial (s√≥ para pacientes, n√£o para protocolos m√©dicos)

---

## ‚ö†Ô∏è CONSIDERA√á√ïES IMPORTANTES SOBRE HARDWARE

### Configura√ß√£o Atual do Sistema
- **CPU**: Intel Core i7-8665U (8 cores)
- **RAM**: 16 GB
- **GPU**: Intel UHD Graphics 620 (integrada, sem CUDA)
- **Sistema**: Manjaro Linux

### Limita√ß√µes para Fine-Tuning

**‚ùå Problema**: Fine-tuning de modelos LLM requer GPU dedicada com CUDA (m√≠nimo 8GB VRAM recomendado). O sistema atual n√£o possui GPU dedicada.

### Alternativas para Fine-Tuning

#### Op√ß√£o 1: Google Colab Pro (RECOMENDADO)
- **Vantagens**: 
  - GPU gratuita (T4) ou paga (V100/A100)
  - Ambiente pr√©-configurado
  - F√°cil compartilhamento de notebooks
- **Custo**: Gratuito (com limita√ß√µes) ou ~$10/m√™s (Colab Pro)
- **Passos**:
  1. Upload do c√≥digo para Google Drive ou GitHub
  2. Abrir notebook no Colab
  3. Instalar depend√™ncias
  4. Executar fine-tuning
  5. Download do modelo treinado

#### Op√ß√£o 2: Kaggle Notebooks
- **Vantagens**: 
  - GPU gratuita (P100) por 30h/semana
  - Ambiente similar ao Colab
- **Limita√ß√µes**: Tempo limitado de GPU
- **Passos**: Similar ao Colab

#### Op√ß√£o 3: Fine-Tuning em CPU (N√ÉO RECOMENDADO)
- **Poss√≠vel mas muito lento**: Pode levar dias ou semanas
- **Modelos menores**: Usar modelos muito pequenos (ex: Phi-3-mini)
- **LoRA com rank baixo**: Reduzir par√¢metros trein√°veis
- **Tempo estimado**: 3-7 dias para dataset pequeno

#### Op√ß√£o 4: Servi√ßos Cloud (AWS, GCP, Azure)
- **Vantagens**: Controle total, m√∫ltiplas op√ß√µes de GPU
- **Custo**: ~$0.50-2.00/hora para inst√¢ncias com GPU
- **Recomendado**: AWS EC2 (g4dn.xlarge) ou Google Cloud (n1-standard-4 + GPU)

#### Op√ß√£o 5: Usar Modelo Pr√©-Treinado (ALTERNATIVA)
- **Vantagem**: N√£o precisa treinar
- **Op√ß√µes**:
  - Usar modelo m√©dico j√° fine-tunado do Hugging Face
  - Usar modelo base com RAG robusto (pode compensar falta de fine-tuning)

### Recomenda√ß√£o para Este Projeto

**Estrat√©gia H√≠brida**:
1. **Curto prazo**: Executar fine-tuning no Google Colab Pro
2. **Upload para Hugging Face**: Publicar modelo treinado
3. **Consumo**: Usar Hugging Face Inference API ou Ollama local (para infer√™ncia, CPU √© suficiente)

---

## üéØ Fase 1: Fine-Tuning do Modelo (PRIORIDADE 1)

### Objetivo
Treinar um modelo LLM especializado em medicina usando o dataset PubMedQA.

### Pr√©-requisitos
- ‚ö†Ô∏è **GPU com CUDA** (8GB+ VRAM) - **N√ÉO DISPON√çVEL NO SISTEMA ATUAL**
- **Alternativa**: Google Colab Pro ou servi√ßo cloud
- Python 3.8+

### Passos Detalhados

#### 1.1 Preparar Ambiente Local (Pr√©-processamento)
```bash
cd fine_tuning
pip install -r requirements.txt  # Criar requirements.txt se n√£o existir
```

**Depend√™ncias necess√°rias para pr√©-processamento:**
- `json`
- `pathlib`
- `re`
- `python-dotenv`

**Nota**: Pr√©-processamento pode ser feito localmente (n√£o precisa GPU).

#### 1.2 Executar Pipeline de Pr√©-processamento (LOCAL)
```bash
# Executa: pr√©-processamento + valida√ß√£o + formata√ß√£o Alpaca
python run_pipeline.py --all
```

**Arquivos gerados:**
- `medical_tuning_data.json` (dados processados)
- `formatted_medical_dataset.json` (formato Alpaca)

**Verifica√ß√£o:**
- Validar que os arquivos foram criados
- Verificar estat√≠sticas de valida√ß√£o

#### 1.3 Preparar para Fine-Tuning no Colab

**Op√ß√£o A: Adaptar Notebook para Colab**

1. Criar vers√£o do notebook para Colab:
   - Upload de `formatted_medical_dataset.json` para Google Drive
   - Adaptar caminhos no notebook
   - Instalar depend√™ncias no Colab

2. Passos no Colab:
```python
# C√©lula 1: Instalar depend√™ncias
!pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'
!pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes
!pip install transformers datasets

# C√©lula 2: Montar Google Drive (se dataset estiver l√°)
from google.colab import drive
drive.mount('/content/drive')

# C√©lula 3: Carregar dataset
import json
with open('/content/drive/MyDrive/formatted_medical_dataset.json', 'r') as f:
    dataset = json.load(f)

# C√©lula 4: Executar fine-tuning (usar c√≥digo do notebook existente)
```

**Op√ß√£o B: Script Python Adaptado**

Criar `fine_tuning/training/finetuning_colab.py` que:
- Detecta se est√° rodando no Colab
- Ajusta caminhos automaticamente
- Faz upload do modelo para Google Drive ao final

#### 1.4 Executar Fine-Tuning (NO COLAB)

**Configura√ß√µes importantes** (`training/model_config.py`):
- Modelo base: `unsloth/llama-3-8b-bnb-4bit` (padr√£o)
- LoRA rank: 16
- Learning rate: 2e-4
- Max steps: 100 (ajustar conforme dataset)

**Tempo estimado no Colab (T4)**: 2-4 horas

#### 1.5 Download do Modelo Treinado

Ap√≥s fine-tuning no Colab:
```python
# No Colab, ap√≥s treinamento
from google.colab import files
import shutil

# Compactar modelo
shutil.make_archive('lora_model_medical', 'zip', 'outputs/lora_model_medical')

# Download
files.download('lora_model_medical.zip')
```

Ou fazer upload direto para Hugging Face (ver Fase 2).

#### 1.6 Validar Modelo Treinado (LOCAL)
```bash
# Ap√≥s download do Colab
cd fine_tuning
unzip lora_model_medical.zip -d outputs/
python inference/test_model.py --model_path outputs/lora_model_medical --mode examples
```

**Nota**: Infer√™ncia pode ser feita em CPU (mais lenta, mas funcional).

---

## üöÄ Fase 2: Publicar Modelo no Hugging Face

### Objetivo
Tornar o modelo acess√≠vel via Hugging Face para uso em produ√ß√£o.

### Passos Detalhados

#### 2.1 Preparar Modelo para Upload
**Op√ß√£o A: Upload de Adaptadores LoRA (Recomendado)**
- Upload apenas dos adaptadores LoRA (menor tamanho)
- Requer modelo base dispon√≠vel no Hugging Face

**Op√ß√£o B: Merge e Upload Completo**
- Fazer merge dos adaptadores com modelo base
- Upload do modelo completo (maior tamanho)

#### 2.2 Criar Conta e Reposit√≥rio no Hugging Face
1. Criar conta em https://huggingface.co
2. Criar novo reposit√≥rio (ex: `seu-usuario/biobyia-medical-lora`)
3. Obter token de acesso (Settings ‚Üí Access Tokens)

#### 2.3 Instalar e Configurar Hugging Face CLI
```bash
pip install huggingface_hub
huggingface-cli login
```

#### 2.4 Script de Upload
Criar script `fine_tuning/upload_to_hf.py`:

```python
from huggingface_hub import HfApi, login
from pathlib import Path

# Login
login(token="seu_token_aqui")

# Upload
api = HfApi()
model_path = Path("outputs/lora_model_medical")
repo_id = "seu-usuario/biobyia-medical-lora"

api.upload_folder(
    folder_path=str(model_path),
    repo_id=repo_id,
    repo_type="model"
)
```

#### 2.5 Consumir Modelo via Hugging Face Inference API
**Op√ß√£o 1: Usar Inference API (gratuito para modelos p√∫blicos)**
```python
import requests

API_URL = "https://api-inference.huggingface.co/models/seu-usuario/biobyia-medical-lora"
headers = {"Authorization": f"Bearer {HF_TOKEN}"}

def query_model(prompt):
    response = requests.post(API_URL, headers=headers, json={"inputs": prompt})
    return response.json()
```

**Op√ß√£o 2: Baixar e usar localmente via Ollama**
- Converter modelo para formato Ollama
- Servir via Ollama localmente
- Backend consome via API Ollama

---

## üîç Fase 3: Executar Pipeline RAG

### Objetivo
Ingerir dados m√©dicos do PubMedQA no Pinecone para busca sem√¢ntica.

### Pr√©-requisitos
- Conta Pinecone (https://app.pinecone.io)
- API Key do Gemini (para embeddings) ou Ollama rodando
- Dataset `ori_pqal.json` no caminho correto

### Passos Detalhados

#### 3.1 Configurar Vari√°veis de Ambiente
Criar `rag_medical/.env`:
```env
PINECONE_API_KEY=sua_chave_aqui
PINECONE_INDEX_NAME=biobyia
PINECONE_NAMESPACE=medical_qa

GEMINI_API_KEY=sua_chave_gemini
EMBEDDING_MODEL=text-embedding-004

MEDICAL_DATA_PATH=../context/pubmedqa-master/data/ori_pqal.json
CHUNK_SIZE=512
CHUNK_OVERLAP=50
```

#### 3.2 Instalar Depend√™ncias
```bash
cd rag_medical
pip install -r requirements.txt
```

#### 3.3 Executar Pipeline (Notebooks)
Execute na ordem:

1. **01-load-and-explore-data.ipynb**
   - Carrega e explora dataset
   - Valida estrutura

2. **02-process-medical-data.ipynb**
   - Processa e anonimiza dados
   - Divide em chunks

3. **03-embed-and-ingest-pinecone.ipynb**
   - Gera embeddings
   - Ingesta no Pinecone
   - ‚ö†Ô∏è **Pode levar 20-30 minutos**

4. **04-test-rag-query.ipynb**
   - Testa queries RAG
   - Valida recupera√ß√£o

#### 3.4 Verificar Ingest√£o
```python
from scripts.rag_query import query_medical_rag

results = query_medical_rag("Do mitochondria play a role?", top_k=5)
print(f"Encontrados {len(results)} resultados")
```

---

## üîó Fase 4: Integra√ß√£o Backend + RAG + Modelo Fine-Tunado

### Objetivo
Integrar RAG e modelo fine-tunado ao backend para uso no chat m√©dico.

### Arquivos Principais a Modificar

#### 4.1 Criar Servi√ßo RAG no Backend
**Arquivo:** `backend/src/services/ragService.js`

**Funcionalidades:**
- Buscar contexto m√©dico no Pinecone
- Formatar contexto para LLM
- Integrar com `medicalAssistantChain`

#### 4.2 Integrar RAG no MedicalAssistantChain
**Arquivo:** `backend/src/langchain/chains/medicalAssistantChain.js`

**Modifica√ß√µes:**
- Adicionar busca RAG para protocolos m√©dicos (n√£o apenas pacientes)
- Combinar contexto RAG com contexto do paciente
- Usar modelo fine-tunado quando dispon√≠vel

**Fluxo:**
```
Query ‚Üí RAG (Pinecone) ‚Üí Contexto M√©dico ‚Üí LLM (Fine-tuned) ‚Üí Resposta
```

#### 4.3 Adicionar Provider Hugging Face
**Arquivo:** `backend/src/providers/HuggingFaceProvider.js` (novo)

**Op√ß√µes de implementa√ß√£o:**

**Op√ß√£o A: Inference API**
```javascript
const axios = require('axios');

class HuggingFaceProvider extends BaseProvider {
  async generate(prompt, options = {}) {
    const response = await axios.post(
      `https://api-inference.huggingface.co/models/${this.config.modelId}`,
      { inputs: prompt },
      { headers: { Authorization: `Bearer ${this.config.apiKey}` } }
    );
    return response.data[0].generated_text;
  }
}
```

**Op√ß√£o B: Via Ollama (Recomendado)**
- Modelo fine-tunado servido via Ollama
- Backend usa provider Ollama existente
- Configurar `BIOBYIA_MODEL` no `.env`

#### 4.4 Atualizar Configura√ß√£o LLM
**Arquivo:** `backend/src/config/llmConfig.js`

Adicionar suporte a Hugging Face:
```javascript
if (process.env.HUGGINGFACE_API_KEY) {
  providers.huggingface = {
    type: 'huggingface',
    name: 'Hugging Face',
    apiKey: process.env.HUGGINGFACE_API_KEY,
    modelId: process.env.HUGGINGFACE_MODEL_ID || 'seu-usuario/biobyia-medical-lora'
  };
}
```

#### 4.5 Integrar RAG no Prompt M√©dico
**Arquivo:** `backend/src/langchain/prompts/medical/medicalPrompts.js`

Modificar prompt para incluir contexto RAG:
```javascript
CONTEXTO DE PROTOCOLOS M√âDICOS (RAG):
{rag_context}

CONTEXTO DO PACIENTE:
{patient_context}
```

#### 4.6 Criar Endpoint para Teste
**Arquivo:** `backend/src/routes/medicalAssistantRoutes.js`

Adicionar endpoint de teste:
```javascript
router.post('/test-rag', async (req, res) => {
  // Testa RAG + modelo fine-tunado
});
```

---

## üí¨ Fase 5: Integra√ß√£o Frontend + Chat

### Objetivo
Conectar frontend ao backend com RAG e modelo fine-tunado.

### Passos Detalhados

#### 5.1 Atualizar Configura√ß√£o API
**Arquivo:** `frontend/src/config/api.js`

Verificar se est√° apontando para backend correto:
```javascript
const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 'http://localhost:4000';
```

#### 5.2 Atualizar ChatInterface
**Arquivo:** `frontend/src/components/ChatInterface.vue`

**Melhorias:**
- Mostrar fontes RAG na resposta
- Indicar quando usa modelo fine-tunado
- Exibir contexto recuperado (opcional, para debug)

#### 5.3 Adicionar Seletor de Provider
**Arquivo:** `frontend/src/components/ChatInterface.vue` (novo componente)

Permitir escolher entre:
- Gemini (padr√£o)
- Modelo Fine-Tunado (BiobyIA)
- Ollama

---

## üìã Checklist de Execu√ß√£o

### Fase 1: Fine-Tuning
- [ ] Preparar ambiente local (pr√©-processamento)
- [ ] Executar `run_pipeline.py --all` (local)
- [ ] Validar dados processados
- [ ] Preparar notebook/script para Colab
- [ ] Executar fine-tuning no Colab
- [ ] Download do modelo treinado
- [ ] Validar modelo treinado com `test_model.py` (local, CPU)

### Fase 2: Hugging Face
- [ ] Criar conta e reposit√≥rio no Hugging Face
- [ ] Criar script de upload
- [ ] Fazer upload do modelo
- [ ] Testar Inference API
- [ ] Decidir: Inference API ou Ollama local?

### Fase 3: RAG Pipeline
- [ ] Configurar `.env` do RAG
- [ ] Instalar depend√™ncias
- [ ] Executar notebooks 01-04
- [ ] Validar ingest√£o no Pinecone
- [ ] Testar queries RAG

### Fase 4: Integra√ß√£o Backend
- [ ] Criar `ragService.js`
- [ ] Modificar `medicalAssistantChain.js` para usar RAG
- [ ] Adicionar provider Hugging Face (ou configurar Ollama)
- [ ] Atualizar prompts m√©dicos
- [ ] Testar endpoint `/api/medical/query` com RAG

### Fase 5: Integra√ß√£o Frontend
- [ ] Verificar configura√ß√£o API
- [ ] Atualizar ChatInterface para mostrar fontes
- [ ] Adicionar seletor de provider (opcional)
- [ ] Testar chat completo end-to-end

---

## üîß Configura√ß√µes Necess√°rias

### Backend `.env`
```env
# LLM
LLM_PROVIDER=biobyia  # ou huggingface
GEMINI_API_KEY=...  # fallback

# BiobyIA (modelo fine-tunado via Ollama)
BIOBYIA_BASE_URL=http://localhost:11434
BIOBYIA_MODEL=biobyia-medical-lora
BIOBYIA_TEMPERATURE=0.7

# Ou Hugging Face
HUGGINGFACE_API_KEY=...
HUGGINGFACE_MODEL_ID=seu-usuario/biobyia-medical-lora

# Pinecone (RAG)
USE_PINECONE=true
PINECONE_API_KEY=...
PINECONE_INDEX_NAME=biobyia
PINECONE_NAMESPACE=medical_qa

# MongoDB
MONGODB_URI=...
```

### Frontend `.env`
```env
VITE_API_BASE_URL=http://localhost:4000
```

---

## üìä Diagrama de Fluxo Completo

```mermaid
graph TD
    A[Usu√°rio faz pergunta no Frontend] --> B[Backend recebe query]
    B --> C{Usar RAG?}
    C -->|Sim| D[Buscar no Pinecone]
    D --> E[Recuperar contexto m√©dico]
    E --> F[Combinar com contexto paciente]
    F --> G[Formatar prompt]
    G --> H{Qual LLM?}
    H -->|Fine-tuned| I[Modelo BiobyIA via Ollama/HF]
    H -->|Fallback| J[Gemini]
    I --> K[Gerar resposta]
    J --> K
    K --> L[Aplicar guardrails]
    L --> M[Salvar em auditoria]
    M --> N[Retornar resposta ao Frontend]
    N --> O[Exibir resposta + fontes]
```

---

## ‚ö†Ô∏è Pontos de Aten√ß√£o

1. **GPU para Fine-Tuning**: ‚ö†Ô∏è **N√ÉO DISPON√çVEL** - Usar Google Colab Pro ou servi√ßo cloud
2. **Custos Pinecone**: Verificar limites do plano gratuito
3. **Tempo de Ingest√£o RAG**: Pode levar 20-30 minutos para dataset completo
4. **Modelo Fine-Tunado**: Decidir entre Inference API (gratuito mas limitado) ou Ollama local (requer servidor)
5. **Integra√ß√£o RAG**: Atualmente s√≥ busca contexto de pacientes, precisa adicionar busca de protocolos m√©dicos
6. **Infer√™ncia em CPU**: Funcional mas lenta - considerar usar Hugging Face Inference API para produ√ß√£o

---

## üéØ Ordem Recomendada de Execu√ß√£o

1. **Fase 1** (Fine-Tuning) - Base para tudo
   - Pr√©-processamento local
   - Fine-tuning no Colab
   - Download e valida√ß√£o local

2. **Fase 2** (Hugging Face) - Tornar modelo acess√≠vel
   - Upload do modelo
   - Testar Inference API

3. **Fase 3** (RAG) - Paralelo com Fase 2, pode ser feito antes
   - Executar pipeline completo
   - Validar ingest√£o

4. **Fase 4** (Backend) - Integra tudo
   - Criar servi√ßos
   - Integrar RAG e modelo

5. **Fase 5** (Frontend) - Interface final
   - Atualizar componentes
   - Testar end-to-end

---

## üìù Pr√≥ximos Passos Imediatos

1. ‚úÖ **Decis√£o sobre Fine-Tuning**: Escolher Google Colab Pro (recomendado)
2. **Pr√©-processamento**: Executar `run_pipeline.py --all` localmente
3. **Preparar Colab**: Adaptar notebook para Google Colab
4. **Configurar credenciais**: Pinecone, Gemini, Hugging Face
5. **Come√ßar pela Fase 1**: Pr√©-processamento local ‚Üí Fine-tuning no Colab

---

## üîó Links √öteis

- [Google Colab](https://colab.research.google.com/)
- [Hugging Face](https://huggingface.co/)
- [Pinecone](https://www.pinecone.io/)
- [Unsloth Documentation](https://github.com/unslothai/unsloth)

---

**√öltima atualiza√ß√£o**: 2024
**Vers√£o do planejamento**: 1.0

