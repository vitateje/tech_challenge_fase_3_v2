{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 02: Processamento de Dados M√©dicos\n",
        "\n",
        "Este notebook processa os dados m√©dicos carregados no notebook anterior. Aqui vamos:\n",
        "\n",
        "1. **Anonimizar** dados sens√≠veis (conformidade LGPD/HIPAA)\n",
        "2. **Processar** cada entrada do dataset\n",
        "3. **Limpar** e normalizar textos\n",
        "4. **Dividir** textos em chunks otimizados\n",
        "5. **Validar** qualidade dos dados processados\n",
        "\n",
        "## üìã Pr√©-requisitos\n",
        "\n",
        "- Notebook 01 executado com sucesso (dataset carregado)\n",
        "- Vari√°veis de ambiente configuradas\n",
        "- Depend√™ncias instaladas\n",
        "\n",
        "## üîÑ Ordem de Execu√ß√£o\n",
        "\n",
        "Execute as c√©lulas **sequencialmente** (de cima para baixo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configura√ß√µes validadas com sucesso!\n",
            "üìÅ Arquivo de dados: /Users/vitorteixeira/Developer/projects/tech_challenge_fase_3_v2/rag_medical/ori_pqal.json\n",
            "\n",
            "‚úÖ Bibliotecas importadas com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 0: IMPORTA√á√ÉO DE BIBLIOTECAS E CONFIGURA√á√ïES\n",
        "# ============================================================================\n",
        "# Esta c√©lula importa todas as bibliotecas necess√°rias e configura o ambiente\n",
        "# Execute esta c√©lula PRIMEIRO antes de qualquer outra opera√ß√£o\n",
        "\n",
        "import sys\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "\n",
        "# Adiciona o diret√≥rio raiz ao path para importar m√≥dulos\n",
        "# Quando executado do diret√≥rio notebooks/, o parent √© rag_medical/\n",
        "root_dir = Path.cwd().parent\n",
        "if str(root_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(root_dir))\n",
        "\n",
        "# Recarrega m√≥dulos se j√° foram importados (√∫til durante desenvolvimento)\n",
        "# IMPORTANTE: Recarrega na ordem correta (depend√™ncias primeiro)\n",
        "if 'utils.anonymizer' in sys.modules:\n",
        "    importlib.reload(sys.modules['utils.anonymizer'])\n",
        "if 'utils' in sys.modules:\n",
        "    importlib.reload(sys.modules['utils'])\n",
        "if 'scripts.data_processor' in sys.modules:\n",
        "    importlib.reload(sys.modules['scripts.data_processor'])\n",
        "if 'scripts.data_loader' in sys.modules:\n",
        "    importlib.reload(sys.modules['scripts.data_loader'])\n",
        "if 'config.settings' in sys.modules:\n",
        "    importlib.reload(sys.modules['config.settings'])\n",
        "if 'config' in sys.modules:\n",
        "    importlib.reload(sys.modules['config'])\n",
        "\n",
        "# Importa m√≥dulos do pipeline RAG\n",
        "from scripts.data_loader import load_medical_dataset\n",
        "from scripts.data_processor import (\n",
        "    process_medical_entry,\n",
        "    process_batch,\n",
        "    filter_valid_entries\n",
        ")\n",
        "from config.settings import get_settings\n",
        "\n",
        "# Tenta importar text_splitter (pode falhar se depend√™ncias n√£o estiverem instaladas)\n",
        "try:\n",
        "    from scripts.text_splitter import MedicalTextSplitter, create_text_splitter\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è  Aviso: text_splitter n√£o dispon√≠vel: {e}\")\n",
        "    print(\"   Voc√™ pode continuar sem ele, mas n√£o poder√° dividir textos em chunks\")\n",
        "    MedicalTextSplitter = None\n",
        "    create_text_splitter = None\n",
        "\n",
        "# Carrega configura√ß√µes\n",
        "settings = get_settings()\n",
        "\n",
        "# Valida configura√ß√µes b√°sicas (strict=False permite explora√ß√£o sem Pinecone/Embeddings)\n",
        "is_valid, errors = settings.validate(strict=False)\n",
        "if not is_valid:\n",
        "    print(\"‚ùå ERROS DE CONFIGURA√á√ÉO:\")\n",
        "    for error in errors:\n",
        "        print(f\"   - {error}\")\n",
        "    print(\"\\nüí° Configure as vari√°veis de ambiente no arquivo .env\")\n",
        "    raise ValueError(\"Configura√ß√µes inv√°lidas\")\n",
        "else:\n",
        "    print(\"‚úÖ Configura√ß√µes validadas com sucesso!\")\n",
        "    print(f\"üìÅ Arquivo de dados: {settings.MEDICAL_DATA_PATH}\")\n",
        "\n",
        "print(\"\\n‚úÖ Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Carregando dataset do arquivo...\n",
            "‚úÖ Dataset carregado: 1000 entradas\n",
            "üìÅ Arquivo: /Users/vitorteixeira/Developer/projects/tech_challenge_fase_3_v2/rag_medical/ori_pqal.json\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 1: CARREGAMENTO DO DATASET\n",
        "# ============================================================================\n",
        "# Carrega o dataset m√©dico. Se voc√™ j√° executou o notebook 01 e tem\n",
        "# raw_data em mem√≥ria, esta c√©lula detectar√° e reutilizar√°.\n",
        "# Caso contr√°rio, carregar√° do arquivo automaticamente.\n",
        "\n",
        "# Verifica se raw_data j√° est√° carregado (do notebook anterior)\n",
        "if 'raw_data' not in globals():\n",
        "    print(\"üìÇ Carregando dataset do arquivo...\")\n",
        "    data_path = settings.MEDICAL_DATA_PATH\n",
        "    raw_data = load_medical_dataset(data_path)\n",
        "    print(f\"‚úÖ Dataset carregado: {len(raw_data)} entradas\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset j√° carregado (do notebook anterior): {len(raw_data)} entradas\")\n",
        "\n",
        "print(f\"üìÅ Arquivo: {settings.MEDICAL_DATA_PATH}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîÑ PROCESSANDO DADOS M√âDICOS\n",
            "================================================================================\n",
            "Total de entradas a processar: 1000\n",
            "Anonimiza√ß√£o: Habilitada (conformidade LGPD/HIPAA)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processando dados: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 3774.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Processamento conclu√≠do!\n",
            "   Entradas processadas: 1000\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 2: PROCESSAMENTO DE DADOS M√âDICOS\n",
        "# ============================================================================\n",
        "# Esta etapa processa cada entrada do dataset:\n",
        "# - Combina m√∫ltiplos contextos em texto √∫nico\n",
        "# - Aplica anonimiza√ß√£o de dados sens√≠veis\n",
        "# - Formata metadados estruturados\n",
        "# - Prepara dados para embedding\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîÑ PROCESSANDO DADOS M√âDICOS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total de entradas a processar: {len(raw_data)}\")\n",
        "print(f\"Anonimiza√ß√£o: Habilitada (conformidade LGPD/HIPAA)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Processa todas as entradas em lote\n",
        "processed_entries = process_batch(\n",
        "    raw_data,\n",
        "    anonymize=True,  # Habilita anonimiza√ß√£o\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Processamento conclu√≠do!\")\n",
        "print(f\"   Entradas processadas: {len(processed_entries)}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîç FILTRANDO ENTRADAS V√ÅLIDAS\n",
            "================================================================================\n",
            "Entradas antes da filtragem: 1000\n",
            "Entradas ap√≥s filtragem: 1000\n",
            "Entradas removidas: 0 (muito curtas ou inv√°lidas)\n",
            "Taxa de reten√ß√£o: 100.0%\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 3: FILTRAGEM DE ENTRADAS V√ÅLIDAS\n",
        "# ============================================================================\n",
        "# Remove entradas muito curtas ou inv√°lidas que n√£o s√£o adequadas\n",
        "# para embedding e busca\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîç FILTRANDO ENTRADAS V√ÅLIDAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "total_before = len(processed_entries)\n",
        "min_text_length = 50  # Tamanho m√≠nimo em caracteres\n",
        "\n",
        "valid_entries = filter_valid_entries(\n",
        "    processed_entries,\n",
        "    min_text_length=min_text_length\n",
        ")\n",
        "\n",
        "total_after = len(valid_entries)\n",
        "removed = total_before - total_after\n",
        "\n",
        "print(f\"Entradas antes da filtragem: {total_before}\")\n",
        "print(f\"Entradas ap√≥s filtragem: {total_after}\")\n",
        "print(f\"Entradas removidas: {removed} (muito curtas ou inv√°lidas)\")\n",
        "print(f\"Taxa de reten√ß√£o: {total_after/total_before*100:.1f}%\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "‚úÇÔ∏è  DIVIDINDO TEXTOS EM CHUNKS\n",
            "================================================================================\n",
            "Chunk size: 512 caracteres\n",
            "Chunk overlap: 50 caracteres\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dividindo em chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 20590.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Divis√£o em chunks conclu√≠da!\n",
            "   Entradas originais: 1000\n",
            "   Total de chunks gerados: 5320\n",
            "   M√©dia de chunks por entrada: 5.32\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 4: DIVIS√ÉO EM CHUNKS\n",
        "# ============================================================================\n",
        "# Divide textos longos em chunks menores para otimizar:\n",
        "# - Embedding (chunks menores s√£o mais eficientes)\n",
        "# - Busca (recupera√ß√£o mais precisa)\n",
        "# - Armazenamento (melhor uso do Pinecone)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÇÔ∏è  DIVIDINDO TEXTOS EM CHUNKS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# VALIDA√á√ÉO: Verifica se valid_entries existe (deve ter sido criado na c√©lula 4)\n",
        "# ============================================================================\n",
        "# Esta verifica√ß√£o DEVE ser executada ANTES de qualquer uso de valid_entries\n",
        "try:\n",
        "    entries_available = len(valid_entries) > 0\n",
        "    if not entries_available:\n",
        "        raise ValueError(\"valid_entries est√° vazio. Execute a c√©lula 4 primeiro.\")\n",
        "except NameError:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚ùå ERRO: Vari√°vel 'valid_entries' n√£o encontrada!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"A vari√°vel 'valid_entries' n√£o est√° definida.\")\n",
        "    print(\"\\nüí° SOLU√á√ÉO:\")\n",
        "    print(\"Execute as c√©lulas anteriores na ordem:\")\n",
        "    print(\"   1. C√©lula 1: Importa√ß√£o de bibliotecas (ETAPA 0)\")\n",
        "    print(\"   2. C√©lula 2: Carregamento do dataset (ETAPA 1)\")\n",
        "    print(\"   3. C√©lula 3: Processamento de dados (ETAPA 2)\")\n",
        "    print(\"   4. C√©lula 4: Filtragem de entradas (ETAPA 3) ‚Üê Esta cria 'valid_entries'\")\n",
        "    print(\"   5. C√©lula 5: Divis√£o em chunks (ETAPA 4) ‚Üê Esta c√©lula\")\n",
        "    print(\"=\" * 80)\n",
        "    raise NameError(\n",
        "        \"Vari√°vel 'valid_entries' n√£o encontrada. \"\n",
        "        \"Execute a c√©lula 4 (ETAPA 3: FILTRAGEM DE ENTRADAS V√ÅLIDAS) primeiro.\"\n",
        "    )\n",
        "\n",
        "# Verifica se text_splitter est√° dispon√≠vel\n",
        "# Usa verifica√ß√£o segura para evitar NameError se a vari√°vel n√£o foi definida\n",
        "try:\n",
        "    text_splitter_available = create_text_splitter is not None\n",
        "except NameError:\n",
        "    text_splitter_available = False\n",
        "    print(\"‚ö†Ô∏è  ATEN√á√ÉO: Vari√°vel 'create_text_splitter' n√£o encontrada!\")\n",
        "    print(\"   Execute a c√©lula 1 (ETAPA 0) primeiro para importar as bibliotecas.\")\n",
        "    print(\"   Continuando sem text_splitter...\")\n",
        "\n",
        "if not text_splitter_available:\n",
        "    print(\"‚ö†Ô∏è  text_splitter n√£o dispon√≠vel (depend√™ncias n√£o instaladas ou n√£o importado)\")\n",
        "    print(\"   Pulando divis√£o em chunks...\")\n",
        "    print(\"   Usando entradas completas como chunks √∫nicos\")\n",
        "    # Cria chunks simples (uma entrada = um chunk)\n",
        "    # Nota: valid_entries j√° foi verificado acima, ent√£o est√° dispon√≠vel aqui\n",
        "    all_chunks = []\n",
        "    for entry in valid_entries:\n",
        "        chunk = {\n",
        "            'text': entry['text'],\n",
        "            'article_id': entry['article_id'],\n",
        "            'chunk_index': 0,\n",
        "            'metadata': entry['metadata']\n",
        "        }\n",
        "        all_chunks.append(chunk)\n",
        "    print(f\"\\n‚úÖ Usando entradas completas como chunks\")\n",
        "    print(f\"   Total de chunks: {len(all_chunks)}\")\n",
        "else:\n",
        "    print(f\"Chunk size: {settings.CHUNK_SIZE} caracteres\")\n",
        "    print(f\"Chunk overlap: {settings.CHUNK_OVERLAP} caracteres\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Cria divisor de texto\n",
        "    text_splitter = create_text_splitter(\n",
        "        chunk_size=settings.CHUNK_SIZE,\n",
        "        chunk_overlap=settings.CHUNK_OVERLAP\n",
        "    )\n",
        "    \n",
        "    # Divide todas as entradas em chunks\n",
        "    all_chunks = text_splitter.split_batch(\n",
        "        valid_entries,\n",
        "        preserve_metadata=True,\n",
        "        show_progress=True\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úÖ Divis√£o em chunks conclu√≠da!\")\n",
        "    print(f\"   Entradas originais: {len(valid_entries)}\")\n",
        "    print(f\"   Total de chunks gerados: {len(all_chunks)}\")\n",
        "    if len(valid_entries) > 0:\n",
        "        print(f\"   M√©dia de chunks por entrada: {len(all_chunks)/len(valid_entries):.2f}\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 6: SALVAMENTO DOS CHUNKS (OPCIONAL)\n",
        "# ============================================================================\n",
        "# Salva os chunks processados em um arquivo JSON para uso posterior\n",
        "# Isso permite carregar os chunks no notebook 03 mesmo ap√≥s reiniciar o kernel\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Verifica se all_chunks existe\n",
        "try:\n",
        "    chunks_count = len(all_chunks)\n",
        "    \n",
        "    # Define o caminho do arquivo (no diret√≥rio raiz do projeto)\n",
        "    chunks_file = root_dir / 'processed_chunks.json'\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"üíæ SALVANDO CHUNKS PROCESSADOS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total de chunks a salvar: {chunks_count:,}\")\n",
        "    print(f\"Arquivo de destino: {chunks_file}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Salva os chunks em arquivo JSON\n",
        "    with open(chunks_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    # Verifica o tamanho do arquivo\n",
        "    file_size = chunks_file.stat().st_size\n",
        "    file_size_mb = file_size / (1024 * 1024)\n",
        "    \n",
        "    print(f\"‚úÖ Chunks salvos com sucesso!\")\n",
        "    print(f\"   Arquivo: {chunks_file}\")\n",
        "    print(f\"   Tamanho: {file_size_mb:.2f} MB\")\n",
        "    print(f\"   Total de chunks: {chunks_count:,}\")\n",
        "    print(\"\\nüí° Agora voc√™ pode:\")\n",
        "    print(\"   - Carregar este arquivo no notebook 03\")\n",
        "    print(\"   - Compartilhar os chunks processados\")\n",
        "    print(\"   - Reutilizar sem reprocessar os dados\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "except NameError:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚ö†Ô∏è  ATEN√á√ÉO: Vari√°vel 'all_chunks' n√£o encontrada!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Execute a c√©lula 5 (ETAPA 4: DIVIS√ÉO EM CHUNKS) primeiro.\")\n",
        "    print(\"=\" * 80)\n",
        "except Exception as e:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"‚ùå ERRO ao salvar chunks: {e}\")\n",
        "    print(\"=\" * 80)\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üìÑ EXEMPLOS DE CHUNKS PROCESSADOS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "CHUNK 1\n",
            "================================================================================\n",
            "Article ID: 21645374\n",
            "Chunk Index: 0\n",
            "Tamanho: 423 caracteres\n",
            "\n",
            "Texto (primeiros 300 caracteres):\n",
            "Context: Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs i...\n",
            "\n",
            "Metadados:\n",
            "  article_id: 21645374\n",
            "  question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
            "  source: pubmedqa\n",
            "  type: medical_qa\n",
            "  year: 2011\n",
            "  meshes: Alismataceae, Apoptosis, Cell Differentiation, Mitochondria, Plant Leaves\n",
            "  labels: BACKGROUND, RESULTS\n",
            "  final_decision: yes\n",
            "  reasoning_required: yes\n",
            "  chunk_index: 0\n",
            "\n",
            "================================================================================\n",
            "CHUNK 2\n",
            "================================================================================\n",
            "Article ID: 21645374\n",
            "Chunk Index: 1\n",
            "Tamanho: 305 caracteres\n",
            "\n",
            "Texto (primeiros 300 caracteres):\n",
            "approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants. The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascarie...\n",
            "\n",
            "Metadados:\n",
            "  article_id: 21645374\n",
            "  question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
            "  source: pubmedqa\n",
            "  type: medical_qa\n",
            "  year: 2011\n",
            "  meshes: Alismataceae, Apoptosis, Cell Differentiation, Mitochondria, Plant Leaves\n",
            "  labels: BACKGROUND, RESULTS\n",
            "  final_decision: yes\n",
            "  reasoning_required: yes\n",
            "  chunk_index: 1\n",
            "\n",
            "================================================================================\n",
            "CHUNK 3\n",
            "================================================================================\n",
            "Article ID: 21645374\n",
            "Chunk Index: 2\n",
            "Tamanho: 383 caracteres\n",
            "\n",
            "Texto (primeiros 300 caracteres):\n",
            "regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage ...\n",
            "\n",
            "Metadados:\n",
            "  article_id: 21645374\n",
            "  question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
            "  source: pubmedqa\n",
            "  type: medical_qa\n",
            "  year: 2011\n",
            "  meshes: Alismataceae, Apoptosis, Cell Differentiation, Mitochondria, Plant Leaves\n",
            "  labels: BACKGROUND, RESULTS\n",
            "  final_decision: yes\n",
            "  reasoning_required: yes\n",
            "  chunk_index: 2\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Visualiza√ß√£o conclu√≠da!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 5: VISUALIZA√á√ÉO DE EXEMPLOS PROCESSADOS\n",
        "# ============================================================================\n",
        "# Exibe exemplos de chunks processados para verifica√ß√£o visual\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üìÑ EXEMPLOS DE CHUNKS PROCESSADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Mostra alguns exemplos\n",
        "for i, chunk in enumerate(all_chunks[:3], 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CHUNK {i}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Article ID: {chunk['article_id']}\")\n",
        "    print(f\"Chunk Index: {chunk['chunk_index']}\")\n",
        "    print(f\"Tamanho: {len(chunk['text'])} caracteres\")\n",
        "    print(f\"\\nTexto (primeiros 300 caracteres):\")\n",
        "    print(f\"{chunk['text'][:300]}...\")\n",
        "    print(f\"\\nMetadados:\")\n",
        "    for key, value in chunk['metadata'].items():\n",
        "        if isinstance(value, str) and len(value) > 100:\n",
        "            print(f\"  {key}: {value[:100]}...\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"‚úÖ Visualiza√ß√£o conclu√≠da!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üíæ SALVANDO CHUNKS PROCESSADOS\n",
            "================================================================================\n",
            "Total de chunks a salvar: 5,320\n",
            "Arquivo de destino: /Users/vitorteixeira/Developer/projects/tech_challenge_fase_3_v2/rag_medical/processed_chunks.json\n",
            "--------------------------------------------------------------------------------\n",
            "‚úÖ Chunks salvos com sucesso!\n",
            "   Arquivo: /Users/vitorteixeira/Developer/projects/tech_challenge_fase_3_v2/rag_medical/processed_chunks.json\n",
            "   Tamanho: 5.76 MB\n",
            "   Total de chunks: 5,320\n",
            "\n",
            "üí° Agora voc√™ pode:\n",
            "   - Carregar este arquivo no notebook 03\n",
            "   - Compartilhar os chunks processados\n",
            "   - Reutilizar sem reprocessar os dados\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 6: SALVAMENTO DOS CHUNKS (OPCIONAL)\n",
        "# ============================================================================\n",
        "# Salva os chunks processados em um arquivo JSON para uso posterior\n",
        "# Isso permite carregar os chunks no notebook 03 mesmo ap√≥s reiniciar o kernel\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Verifica se all_chunks existe\n",
        "try:\n",
        "    chunks_count = len(all_chunks)\n",
        "    \n",
        "    # Define o caminho do arquivo (no diret√≥rio raiz do projeto)\n",
        "    chunks_file = root_dir / 'processed_chunks.json'\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"üíæ SALVANDO CHUNKS PROCESSADOS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total de chunks a salvar: {chunks_count:,}\")\n",
        "    print(f\"Arquivo de destino: {chunks_file}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Salva os chunks em arquivo JSON\n",
        "    with open(chunks_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    # Verifica o tamanho do arquivo\n",
        "    file_size = chunks_file.stat().st_size\n",
        "    file_size_mb = file_size / (1024 * 1024)\n",
        "    \n",
        "    print(f\"‚úÖ Chunks salvos com sucesso!\")\n",
        "    print(f\"   Arquivo: {chunks_file}\")\n",
        "    print(f\"   Tamanho: {file_size_mb:.2f} MB\")\n",
        "    print(f\"   Total de chunks: {chunks_count:,}\")\n",
        "    print(\"\\nüí° Agora voc√™ pode:\")\n",
        "    print(\"   - Carregar este arquivo no notebook 03\")\n",
        "    print(\"   - Compartilhar os chunks processados\")\n",
        "    print(\"   - Reutilizar sem reprocessar os dados\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "except NameError:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚ö†Ô∏è  ATEN√á√ÉO: Vari√°vel 'all_chunks' n√£o encontrada!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Execute a c√©lula 5 (ETAPA 4: DIVIS√ÉO EM CHUNKS) primeiro.\")\n",
        "    print(\"=\" * 80)\n",
        "except Exception as e:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"‚ùå ERRO ao salvar chunks: {e}\")\n",
        "    print(\"=\" * 80)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Conclus√£o da Etapa 2\n",
        "\n",
        "Neste notebook voc√™:\n",
        "- ‚úÖ Processou todas as entradas do dataset\n",
        "- ‚úÖ Aplicou anonimiza√ß√£o de dados sens√≠veis\n",
        "- ‚úÖ Filtrou entradas inv√°lidas\n",
        "- ‚úÖ Dividiu textos em chunks otimizados\n",
        "- ‚úÖ Validou a qualidade dos dados processados\n",
        "\n",
        "## üìå Pr√≥ximos Passos\n",
        "\n",
        "Agora voc√™ est√° pronto para o pr√≥ximo notebook:\n",
        "- **Notebook 03**: Gera√ß√£o de embeddings e ingest√£o no Pinecone\n",
        "- Conex√£o com Pinecone\n",
        "- Gera√ß√£o de embeddings\n",
        "- Ingest√£o em lotes\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
