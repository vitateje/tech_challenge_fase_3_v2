{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 02: Processamento de Dados M√©dicos\n",
        "\n",
        "Este notebook processa os dados m√©dicos carregados no notebook anterior. Aqui vamos:\n",
        "\n",
        "1. **Anonimizar** dados sens√≠veis (conformidade LGPD/HIPAA)\n",
        "2. **Processar** cada entrada do dataset\n",
        "3. **Limpar** e normalizar textos\n",
        "4. **Dividir** textos em chunks otimizados\n",
        "5. **Validar** qualidade dos dados processados\n",
        "\n",
        "## üìã Pr√©-requisitos\n",
        "\n",
        "- Notebook 01 executado com sucesso (dataset carregado)\n",
        "- Vari√°veis de ambiente configuradas\n",
        "- Depend√™ncias instaladas\n",
        "\n",
        "## üîÑ Ordem de Execu√ß√£o\n",
        "\n",
        "Execute as c√©lulas **sequencialmente** (de cima para baixo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 0: IMPORTA√á√ÉO DE BIBLIOTECAS E CONFIGURA√á√ïES\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Adiciona o diret√≥rio raiz ao path\n",
        "root_dir = Path.cwd().parent\n",
        "sys.path.insert(0, str(root_dir))\n",
        "\n",
        "# Importa m√≥dulos do pipeline RAG\n",
        "from scripts.data_loader import load_medical_dataset\n",
        "from scripts.data_processor import (\n",
        "    process_medical_entry,\n",
        "    process_batch,\n",
        "    filter_valid_entries\n",
        ")\n",
        "from scripts.text_splitter import MedicalTextSplitter, create_text_splitter\n",
        "from config.settings import get_settings\n",
        "\n",
        "# Carrega configura√ß√µes\n",
        "settings = get_settings()\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 1: CARREGAMENTO DO DATASET (SE N√ÉO J√Å CARREGADO)\n",
        "# ============================================================================\n",
        "# Se voc√™ executou o notebook 01, pode pular esta c√©lula\n",
        "# Caso contr√°rio, execute para carregar o dataset\n",
        "\n",
        "# Descomente as linhas abaixo se necess√°rio:\n",
        "# data_path = settings.MEDICAL_DATA_PATH\n",
        "# raw_data = load_medical_dataset(data_path)\n",
        "# print(f\"‚úÖ Dataset carregado: {len(raw_data)} entradas\")\n",
        "\n",
        "# Se voc√™ j√° tem raw_data do notebook anterior, apenas confirme:\n",
        "try:\n",
        "    print(f\"‚úÖ Dataset j√° carregado: {len(raw_data)} entradas\")\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  Execute a c√©lula acima para carregar o dataset\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 2: PROCESSAMENTO DE DADOS M√âDICOS\n",
        "# ============================================================================\n",
        "# Esta etapa processa cada entrada do dataset:\n",
        "# - Combina m√∫ltiplos contextos em texto √∫nico\n",
        "# - Aplica anonimiza√ß√£o de dados sens√≠veis\n",
        "# - Formata metadados estruturados\n",
        "# - Prepara dados para embedding\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîÑ PROCESSANDO DADOS M√âDICOS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total de entradas a processar: {len(raw_data)}\")\n",
        "print(f\"Anonimiza√ß√£o: Habilitada (conformidade LGPD/HIPAA)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Processa todas as entradas em lote\n",
        "processed_entries = process_batch(\n",
        "    raw_data,\n",
        "    anonymize=True,  # Habilita anonimiza√ß√£o\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Processamento conclu√≠do!\")\n",
        "print(f\"   Entradas processadas: {len(processed_entries)}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 3: FILTRAGEM DE ENTRADAS V√ÅLIDAS\n",
        "# ============================================================================\n",
        "# Remove entradas muito curtas ou inv√°lidas que n√£o s√£o adequadas\n",
        "# para embedding e busca\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîç FILTRANDO ENTRADAS V√ÅLIDAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "total_before = len(processed_entries)\n",
        "min_text_length = 50  # Tamanho m√≠nimo em caracteres\n",
        "\n",
        "valid_entries = filter_valid_entries(\n",
        "    processed_entries,\n",
        "    min_text_length=min_text_length\n",
        ")\n",
        "\n",
        "total_after = len(valid_entries)\n",
        "removed = total_before - total_after\n",
        "\n",
        "print(f\"Entradas antes da filtragem: {total_before}\")\n",
        "print(f\"Entradas ap√≥s filtragem: {total_after}\")\n",
        "print(f\"Entradas removidas: {removed} (muito curtas ou inv√°lidas)\")\n",
        "print(f\"Taxa de reten√ß√£o: {total_after/total_before*100:.1f}%\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 4: DIVIS√ÉO EM CHUNKS\n",
        "# ============================================================================\n",
        "# Divide textos longos em chunks menores para otimizar:\n",
        "# - Embedding (chunks menores s√£o mais eficientes)\n",
        "# - Busca (recupera√ß√£o mais precisa)\n",
        "# - Armazenamento (melhor uso do Pinecone)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÇÔ∏è  DIVIDINDO TEXTOS EM CHUNKS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Chunk size: {settings.CHUNK_SIZE} caracteres\")\n",
        "print(f\"Chunk overlap: {settings.CHUNK_OVERLAP} caracteres\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Cria divisor de texto\n",
        "text_splitter = create_text_splitter(\n",
        "    chunk_size=settings.CHUNK_SIZE,\n",
        "    chunk_overlap=settings.CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Divide todas as entradas em chunks\n",
        "all_chunks = text_splitter.split_batch(\n",
        "    valid_entries,\n",
        "    preserve_metadata=True,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Divis√£o em chunks conclu√≠da!\")\n",
        "print(f\"   Entradas originais: {len(valid_entries)}\")\n",
        "print(f\"   Total de chunks gerados: {len(all_chunks)}\")\n",
        "print(f\"   M√©dia de chunks por entrada: {len(all_chunks)/len(valid_entries):.2f}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 5: VISUALIZA√á√ÉO DE EXEMPLOS PROCESSADOS\n",
        "# ============================================================================\n",
        "# Exibe exemplos de chunks processados para verifica√ß√£o visual\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üìÑ EXEMPLOS DE CHUNKS PROCESSADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Mostra alguns exemplos\n",
        "for i, chunk in enumerate(all_chunks[:3], 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CHUNK {i}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Article ID: {chunk['article_id']}\")\n",
        "    print(f\"Chunk Index: {chunk['chunk_index']}\")\n",
        "    print(f\"Tamanho: {len(chunk['text'])} caracteres\")\n",
        "    print(f\"\\nTexto (primeiros 300 caracteres):\")\n",
        "    print(f\"{chunk['text'][:300]}...\")\n",
        "    print(f\"\\nMetadados:\")\n",
        "    for key, value in chunk['metadata'].items():\n",
        "        if isinstance(value, str) and len(value) > 100:\n",
        "            print(f\"  {key}: {value[:100]}...\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"‚úÖ Visualiza√ß√£o conclu√≠da!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Conclus√£o da Etapa 2\n",
        "\n",
        "Neste notebook voc√™:\n",
        "- ‚úÖ Processou todas as entradas do dataset\n",
        "- ‚úÖ Aplicou anonimiza√ß√£o de dados sens√≠veis\n",
        "- ‚úÖ Filtrou entradas inv√°lidas\n",
        "- ‚úÖ Dividiu textos em chunks otimizados\n",
        "- ‚úÖ Validou a qualidade dos dados processados\n",
        "\n",
        "## üìå Pr√≥ximos Passos\n",
        "\n",
        "Agora voc√™ est√° pronto para o pr√≥ximo notebook:\n",
        "- **Notebook 03**: Gera√ß√£o de embeddings e ingest√£o no Pinecone\n",
        "- Conex√£o com Pinecone\n",
        "- Gera√ß√£o de embeddings\n",
        "- Ingest√£o em lotes\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
