{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03: Embeddings e Ingest√£o no Pinecone\n",
        "\n",
        "Este notebook realiza a ingest√£o dos dados processados no Pinecone. Aqui vamos:\n",
        "\n",
        "1. **Conectar** com o √≠ndice Pinecone (biobyia)\n",
        "2. **Configurar** o gerenciador de embeddings (Gemini ou Ollama)\n",
        "3. **Gerar** embeddings para todos os chunks\n",
        "4. **Ingerir** dados no Pinecone em lotes\n",
        "5. **Verificar** ingest√£o bem-sucedida\n",
        "\n",
        "## üìã Pr√©-requisitos\n",
        "\n",
        "- Notebook 02 executado com sucesso (chunks processados)\n",
        "- Vari√°veis de ambiente configuradas (PINECONE_API_KEY, GEMINI_API_KEY, etc.)\n",
        "- √çndice Pinecone criado (biobyia)\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANTE\n",
        "\n",
        "- Este processo pode levar v√°rios minutos dependendo do tamanho do dataset\n",
        "- Certifique-se de ter cr√©ditos suficientes no Pinecone\n",
        "- A ingest√£o √© feita em lotes para otimizar performance\n",
        "\n",
        "## üîÑ Ordem de Execu√ß√£o\n",
        "\n",
        "Execute as c√©lulas **sequencialmente** (de cima para baixo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: python-dotenv in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (1.2.1)\n",
            "Requirement already satisfied: tenacity in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (9.1.2)\n",
            "Requirement already satisfied: pinecone in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (7.3.0)\n",
            "Requirement already satisfied: langchain in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-google-genai in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (2.0.10)\n",
            "Requirement already satisfied: google-generativeai in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (0.8.5)\n",
            "Requirement already satisfied: tiktoken in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (0.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (5.1.2)\n",
            "Requirement already satisfied: tqdm in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pinecone) (4.14.1)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pinecone) (2025.8.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.45)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (2.12.0)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (0.4.37)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.81)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: protobuf in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: google-api-python-client in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-generativeai) (2.41.1)\n",
            "Requirement already satisfied: google-api-core in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-generativeai) (2.26.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: scipy in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-api-core->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-api-core->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: filelock in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (3.11.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: anyio in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.12.0)\n",
            "Requirement already satisfied: idna in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: pydantic-core==2.41.1 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.5.3->pinecone) (1.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/vitorteixeira/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv tenacity pinecone langchain langchain-community langchain-google-genai google-generativeai tiktoken sentence-transformers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install python-dotenv tenacity pinecone langchain langchain-community langchain-google-genai google-generativeai tiktoken sentence-transformers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Arquivo .env carregado: /Users/vitorteixeira/Developer/projects/tech_challenge_fase_3_v2/rag_medical/.env\n",
            "üîÑ M√≥dulo recarregado: config.settings\n",
            "üîÑ M√≥dulo recarregado: config\n",
            "üîÑ M√≥dulo recarregado: scripts.embeddings_manager\n",
            "üîÑ M√≥dulo recarregado: scripts.pinecone_ingester\n",
            "\n",
            "‚úÖ Bibliotecas importadas com sucesso!\n",
            "================================================================================\n",
            "‚öôÔ∏è  CONFIGURA√á√ÉO DO PIPELINE RAG\n",
            "================================================================================\n",
            "Pinecone Index: biobyia\n",
            "Pinecone Namespace: medical_qa\n",
            "Embedding Provider: gemini\n",
            "Embedding Model: text-embedding-004\n",
            "Data Path: /Users/vitorteixeira/Developer/projects/tech_challenge_fase_3_v2/rag_medical/ori_pqal.json\n",
            "Chunk Size: 512\n",
            "Chunk Overlap: 50\n",
            "Batch Size: 100\n",
            "Top K Results: 5\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 0: IMPORTA√á√ÉO DE BIBLIOTECAS E CONFIGURA√á√ïES\n",
        "# ============================================================================\n",
        "# Esta c√©lula importa todas as bibliotecas necess√°rias e configura o ambiente\n",
        "# Execute esta c√©lula PRIMEIRO antes de qualquer outra opera√ß√£o\n",
        "\n",
        "import sys\n",
        "import importlib\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Adiciona o diret√≥rio raiz ao path para importar m√≥dulos\n",
        "# Quando executado do diret√≥rio notebooks/, o parent √© rag_medical/\n",
        "root_dir = Path.cwd().parent\n",
        "if str(root_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(root_dir))\n",
        "\n",
        "# ============================================================================\n",
        "# CARREGA VARI√ÅVEIS DE AMBIENTE DO ARQUIVO .env\n",
        "# ============================================================================\n",
        "# Garante que o .env seja carregado do diret√≥rio raiz do projeto (rag_medical/)\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    \n",
        "    # Caminho para o arquivo .env no diret√≥rio raiz do projeto\n",
        "    env_path = root_dir / '.env'\n",
        "    \n",
        "    if env_path.exists():\n",
        "        load_dotenv(dotenv_path=str(env_path))\n",
        "        print(f\"‚úÖ Arquivo .env carregado: {env_path}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Arquivo .env n√£o encontrado em: {env_path}\")\n",
        "        print(f\"   Crie o arquivo .env baseado em env.example\")\n",
        "        print(f\"   Localiza√ß√£o esperada: {env_path}\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  python-dotenv n√£o instalado. Vari√°veis de ambiente do sistema ser√£o usadas.\")\n",
        "    print(\"   Instale com: pip install python-dotenv\")\n",
        "\n",
        "# Recarrega m√≥dulos se j√° foram importados (√∫til durante desenvolvimento)\n",
        "# IMPORTANTE: Recarrega na ordem correta (depend√™ncias primeiro)\n",
        "modules_to_reload = [\n",
        "    'config.settings',\n",
        "    'config',\n",
        "    'scripts.embeddings_manager',\n",
        "    'scripts.pinecone_ingester'\n",
        "]\n",
        "\n",
        "for module_name in modules_to_reload:\n",
        "    if module_name in sys.modules:\n",
        "        try:\n",
        "            importlib.reload(sys.modules[module_name])\n",
        "            print(f\"üîÑ M√≥dulo recarregado: {module_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Aviso ao recarregar {module_name}: {e}\")\n",
        "\n",
        "# Importa configura√ß√µes\n",
        "from config.settings import get_settings\n",
        "\n",
        "# Carrega configura√ß√µes\n",
        "settings = get_settings()\n",
        "\n",
        "# Tenta importar m√≥dulos do pipeline RAG (podem falhar se depend√™ncias n√£o estiverem instaladas)\n",
        "try:\n",
        "    from scripts.embeddings_manager import EmbeddingsManager\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Erro ao importar EmbeddingsManager: {e}\")\n",
        "    print(f\"   Instale as depend√™ncias no ambiente virtual:\")\n",
        "    print(f\"   cd {root_dir}\")\n",
        "    print(f\"   pip install -r requirements.txt\")\n",
        "    EmbeddingsManager = None\n",
        "\n",
        "try:\n",
        "    from scripts.pinecone_ingester import PineconeIngester\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Erro ao importar PineconeIngester: {e}\")\n",
        "    print(f\"   Instale as depend√™ncias no ambiente virtual:\")\n",
        "    print(f\"   cd {root_dir}\")\n",
        "    print(f\"   pip install -r requirements.txt\")\n",
        "    print(f\"   Especificamente: pip install tenacity pinecone\")\n",
        "    PineconeIngester = None\n",
        "\n",
        "# Verifica se os m√≥dulos foram importados com sucesso ANTES de validar configura√ß√µes\n",
        "if EmbeddingsManager is None or PineconeIngester is None:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚ùå M√ìDULOS NECESS√ÅRIOS N√ÉO FORAM IMPORTADOS!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"üì¶ Instale as depend√™ncias no ambiente virtual:\")\n",
        "    print(f\"   1. Ative o ambiente virtual\")\n",
        "    print(f\"   2. cd {root_dir}\")\n",
        "    print(f\"   3. pip install -r requirements.txt\")\n",
        "    print(\"=\" * 80)\n",
        "    raise ImportError(\"Depend√™ncias n√£o instaladas\")\n",
        "\n",
        "# Valida configura√ß√µes (strict=True porque este notebook precisa de Pinecone e Embeddings)\n",
        "is_valid, errors = settings.validate(strict=True)\n",
        "if not is_valid:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚ùå ERROS DE CONFIGURA√á√ÉO:\")\n",
        "    print(\"=\" * 80)\n",
        "    for error in errors:\n",
        "        print(f\"   - {error}\")\n",
        "    print(\"\\nüí° SOLU√á√ÉO:\")\n",
        "    print(f\"   1. Crie o arquivo .env em: {root_dir / '.env'}\")\n",
        "    print(f\"   2. Copie de: {root_dir / 'env.example'}\")\n",
        "    print(f\"   3. Preencha com suas credenciais reais\")\n",
        "    print(\"=\" * 80)\n",
        "    raise ValueError(\"Configura√ß√µes inv√°lidas\")\n",
        "\n",
        "print(\"\\n‚úÖ Bibliotecas importadas com sucesso!\")\n",
        "settings.print_config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Chunks j√° carregados na mem√≥ria: 5320 chunks prontos para ingest√£o\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 1: VERIFICA√á√ÉO E CARREGAMENTO DE CHUNKS\n",
        "# ============================================================================\n",
        "# Tenta carregar chunks de m√∫ltiplas fontes:\n",
        "# 1. Vari√°vel all_chunks na mem√≥ria (se notebook 02 foi executado)\n",
        "# 2. Arquivo JSON salvo (se chunks foram salvos anteriormente)\n",
        "# 3. Reprocessamento dos dados (se necess√°rio)\n",
        "\n",
        "import json\n",
        "\n",
        "# Verifica se all_chunks j√° existe na mem√≥ria\n",
        "try:\n",
        "    chunks_count = len(all_chunks)\n",
        "    print(f\"‚úÖ Chunks j√° carregados na mem√≥ria: {chunks_count} chunks prontos para ingest√£o\")\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  Vari√°vel 'all_chunks' n√£o encontrada na mem√≥ria\")\n",
        "    print(\"   Tentando carregar de arquivo salvo...\")\n",
        "    \n",
        "    # Tenta carregar de arquivo JSON salvo\n",
        "    chunks_loaded = False\n",
        "    possible_chunk_files = [\n",
        "        root_dir / 'processed_chunks.json',\n",
        "        root_dir / 'chunks.json',\n",
        "        root_dir / 'data' / 'processed_chunks.json',\n",
        "    ]\n",
        "    \n",
        "    for chunk_file in possible_chunk_files:\n",
        "        if chunk_file.exists():\n",
        "            try:\n",
        "                print(f\"   üìÇ Tentando carregar: {chunk_file}\")\n",
        "                with open(chunk_file, 'r', encoding='utf-8') as f:\n",
        "                    all_chunks = json.load(f)\n",
        "                chunks_count = len(all_chunks)\n",
        "                print(f\"‚úÖ Chunks carregados do arquivo: {chunks_count} chunks\")\n",
        "                chunks_loaded = True\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è  Erro ao carregar {chunk_file}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    # Se n√£o conseguiu carregar de arquivo, exibe instru√ß√µes\n",
        "    if not chunks_loaded:\n",
        "        chunk_file_path = root_dir / 'processed_chunks.json'\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"‚ö†Ô∏è  CHUNKS N√ÉO ENCONTRADOS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"A vari√°vel 'all_chunks' n√£o est√° dispon√≠vel.\")\n",
        "        print(\"\\nüí° SOLU√á√ïES:\")\n",
        "        print(\"\\nüìù Op√ß√£o 1: Execute o notebook 02 completo\")\n",
        "        print(\"   1. Abra o notebook 02-process-medical-data.ipynb\")\n",
        "        print(\"   2. Execute TODAS as c√©lulas sequencialmente:\")\n",
        "        print(\"      - C√©lula 1: Importa√ß√£o de bibliotecas\")\n",
        "        print(\"      - C√©lula 2: Carregamento do dataset\")\n",
        "        print(\"      - C√©lula 3: Processamento de dados\")\n",
        "        print(\"      - C√©lula 4: Filtragem de entradas\")\n",
        "        print(\"      - C√©lula 5: Divis√£o em chunks (cria 'all_chunks')\")\n",
        "        print(\"   3. Depois, volte para este notebook e execute esta c√©lula novamente\")\n",
        "        print(\"\\nüíæ Op√ß√£o 2: Carregue de um arquivo JSON salvo\")\n",
        "        print(\"   Se voc√™ j√° processou os dados antes, salve os chunks:\")\n",
        "        print(\"   (Execute no notebook 02, ap√≥s a c√©lula 5)\")\n",
        "        print(\"   import json\")\n",
        "        print(f\"   with open('{chunk_file_path}', 'w') as f:\")\n",
        "        print(\"       json.dump(all_chunks, f, ensure_ascii=False, indent=2)\")\n",
        "        print(\"\\n   Depois, coloque o arquivo em um destes locais:\")\n",
        "        for chunk_file in possible_chunk_files:\n",
        "            print(f\"   - {chunk_file}\")\n",
        "        print(\"=\" * 80)\n",
        "        raise NameError(\n",
        "            \"Vari√°vel 'all_chunks' n√£o encontrada. \"\n",
        "            \"Execute o notebook 02 primeiro (especialmente a c√©lula 5 que cria 'all_chunks') \"\n",
        "            \"ou carregue os chunks de um arquivo JSON.\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîß CONFIGURANDO EMBEDDINGS\n",
            "================================================================================\n",
            "‚úÖ Embeddings Gemini inicializados: models/text-embedding-004\n",
            "\n",
            "Dimens√£o dos embeddings: 768\n",
            "Provider: gemini\n",
            "Model: text-embedding-004\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 2: INICIALIZA√á√ÉO DO GERENCIADOR DE EMBEDDINGS\n",
        "# ============================================================================\n",
        "# Configura o provider de embeddings (Gemini ou Ollama)\n",
        "# Gemini √© recomendado por ser mais r√°pido e confi√°vel\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîß CONFIGURANDO EMBEDDINGS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Cria gerenciador de embeddings\n",
        "# Se n√£o especificar provider, detecta automaticamente das configura√ß√µes\n",
        "embeddings_manager = EmbeddingsManager()\n",
        "\n",
        "# Exibe informa√ß√µes sobre o provider\n",
        "embedding_dim = embeddings_manager.get_embedding_dimension()\n",
        "print(f\"\\nDimens√£o dos embeddings: {embedding_dim}\")\n",
        "print(f\"Provider: {embeddings_manager.provider}\")\n",
        "print(f\"Model: {embeddings_manager.model_name}\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîå CONECTANDO COM PINECONE\n",
            "================================================================================\n",
            "‚úÖ Pinecone inicializado: √≠ndice 'biobyia'\n",
            "   Namespace: medical_qa\n",
            "   Dimens√£o dos embeddings: 768\n",
            "   Dimens√£o do √≠ndice Pinecone: 768\n",
            "   ‚úÖ Dimens√µes compat√≠veis!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 3: INICIALIZA√á√ÉO DO INGESTER DO PINECONE\n",
        "# ============================================================================\n",
        "# Conecta com o √≠ndice Pinecone e prepara para ingest√£o\n",
        "#\n",
        "# ‚ö†Ô∏è IMPORTANTE: Se voc√™ modificou o c√≥digo de pinecone_ingester.py,\n",
        "# execute a c√©lula 3 novamente para recarregar o m√≥dulo antes desta c√©lula\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîå CONECTANDO COM PINECONE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Recarrega o m√≥dulo para garantir que temos a vers√£o mais recente\n",
        "if 'scripts.pinecone_ingester' in sys.modules:\n",
        "    importlib.reload(sys.modules['scripts.pinecone_ingester'])\n",
        "    # Re-importa para garantir que temos a classe atualizada\n",
        "    from scripts.pinecone_ingester import PineconeIngester\n",
        "\n",
        "# Cria ingester do Pinecone\n",
        "ingester = PineconeIngester(\n",
        "    embeddings_manager=embeddings_manager,\n",
        "    index_name=settings.PINECONE_INDEX_NAME,\n",
        "    namespace=settings.PINECONE_NAMESPACE,\n",
        "    api_key=settings.PINECONE_API_KEY\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üöÄ INICIANDO INGEST√ÉO NO PINECONE\n",
            "================================================================================\n",
            "üìä Informa√ß√µes da Ingest√£o:\n",
            "   ‚Ä¢ Total de chunks: 5,320\n",
            "   ‚Ä¢ Tamanho do lote: 100\n",
            "   ‚Ä¢ Total de lotes: 54\n",
            "   ‚Ä¢ √çndice Pinecone: biobyia\n",
            "   ‚Ä¢ Namespace: medical_qa\n",
            "   ‚Ä¢ Provider de embeddings: gemini\n",
            "   ‚Ä¢ Modelo: text-embedding-004\n",
            "\n",
            "‚è±Ô∏è  Tempo estimado: ~1 min\n",
            "‚è≥ Este processo pode levar v√°rios minutos...\n",
            "================================================================================\n",
            "\n",
            "üìã Checkpoint encontrado! Retomando de √≠ndice 0\n",
            "   J√° processados: 0/5320 chunks\n",
            "\n",
            "üöÄ Iniciando ingest√£o de 5320 chunks no Pinecone...\n",
            "   Batch size: 100\n",
            "   √çndice: biobyia\n",
            "   Namespace: medical_qa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 0/5320 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 1/5320 [00:04<6:56:32,  4.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 2/5320 [00:08<5:47:22,  3.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 3/5320 [00:12<5:58:57,  4.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 4/5320 [00:15<5:25:31,  3.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 5/5320 [00:18<5:07:08,  3.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 6/5320 [00:21<4:59:42,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 7/5320 [00:24<4:55:47,  3.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 8/5320 [00:29<5:25:16,  3.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 9/5320 [00:32<5:06:46,  3.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 10/5320 [00:35<5:01:47,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Checkpoint salvo: 1000/5320 chunks processados\n",
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 11/5320 [00:39<5:01:01,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 12/5320 [00:42<5:01:38,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 13/5320 [00:45<4:55:58,  3.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 14/5320 [00:49<4:57:05,  3.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 15/5320 [00:52<4:54:05,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 16/5320 [00:55<4:51:27,  3.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 17/5320 [00:59<4:58:06,  3.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 18/5320 [01:02<5:00:01,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 19/5320 [01:06<5:03:39,  3.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 20/5320 [01:09<5:06:37,  3.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Checkpoint salvo: 2000/5320 chunks processados\n",
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 21/5320 [01:12<5:01:54,  3.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 22/5320 [01:16<5:02:46,  3.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 23/5320 [01:19<4:59:04,  3.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 24/5320 [01:22<4:54:11,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 25/5320 [01:26<4:55:10,  3.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   0%|          | 26/5320 [01:29<4:58:03,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 27/5320 [01:32<4:53:52,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 28/5320 [01:36<4:53:43,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 29/5320 [01:39<4:53:53,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 30/5320 [01:43<5:00:11,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Checkpoint salvo: 3000/5320 chunks processados\n",
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 31/5320 [01:46<4:59:00,  3.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 32/5320 [01:49<5:01:18,  3.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 33/5320 [01:53<4:57:26,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 34/5320 [01:56<4:53:28,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 35/5320 [01:59<4:53:46,  3.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 36/5320 [02:03<4:51:54,  3.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 37/5320 [02:06<4:55:21,  3.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 38/5320 [02:10<4:59:25,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 39/5320 [02:13<5:01:11,  3.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 40/5320 [02:17<5:03:07,  3.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Checkpoint salvo: 4000/5320 chunks processados\n",
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 41/5320 [02:20<5:01:10,  3.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 42/5320 [02:23<4:56:06,  3.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 43/5320 [02:27<5:19:06,  3.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 44/5320 [02:30<4:56:05,  3.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 45/5320 [02:33<4:45:48,  3.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 46/5320 [02:36<4:41:03,  3.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 47/5320 [02:39<4:34:57,  3.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 48/5320 [02:42<4:32:38,  3.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 49/5320 [02:45<4:29:59,  3.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 50/5320 [02:48<4:28:26,  3.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Checkpoint salvo: 5000/5320 chunks processados\n",
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 51/5320 [02:51<4:26:56,  3.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 52/5320 [02:54<4:25:56,  3.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 100 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 53/5320 [02:57<4:28:32,  3.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Gerando embeddings para 20 chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingerindo chunks:   1%|          | 54/5320 [02:58<4:50:34,  3.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Ingest√£o conclu√≠da!\n",
            "   Vetores inseridos: 5320/5320\n",
            "\n",
            "================================================================================\n",
            "üìä ESTAT√çSTICAS DA INGEST√ÉO\n",
            "================================================================================\n",
            "‚úÖ Conclu√≠da com sucesso\n",
            "\n",
            "üìà Resultados:\n",
            "   ‚Ä¢ Total de chunks processados: 5,320\n",
            "   ‚Ä¢ Total de vetores inseridos: 5,320\n",
            "   ‚Ä¢ Total de lotes processados: 54\n",
            "   ‚Ä¢ Taxa de sucesso: 100.0%\n",
            "\n",
            "‚è±Ô∏è  Tempo decorrido: 2min 58s\n",
            "\n",
            "‚úÖ Nenhum erro encontrado!\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 4: INGEST√ÉO NO PINECONE\n",
        "# ============================================================================\n",
        "# Esta √© a etapa principal: gera embeddings e ingere todos os chunks\n",
        "# no Pinecone em lotes otimizados\n",
        "#\n",
        "# ‚ö†Ô∏è ATEN√á√ÉO: Este processo pode levar v√°rios minutos!\n",
        "# - Para ~10.000 chunks: ~10-15 minutos\n",
        "# - Para ~100.000 chunks: ~2-3 horas\n",
        "#\n",
        "# O processo √© feito em lotes para:\n",
        "# - Otimizar uso de API\n",
        "# - Evitar rate limiting\n",
        "# - Permitir retry em caso de erro\n",
        "#\n",
        "# üíæ CHECKPOINTING AUTOM√ÅTICO:\n",
        "# - O processo salva automaticamente o progresso a cada 10 lotes\n",
        "# - Se interrompido (Ctrl+C), voc√™ pode retomar executando esta c√©lula novamente\n",
        "# - O checkpoint √© salvo em: rag_medical/checkpoints/\n",
        "# - O processo continuar√° automaticamente de onde parou\n",
        "\n",
        "import time\n",
        "\n",
        "# Valida√ß√£o pr√©-ingest√£o\n",
        "try:\n",
        "    chunks_available = len(all_chunks) > 0\n",
        "except NameError:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚ùå ERRO: Chunks n√£o encontrados\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"A vari√°vel 'all_chunks' n√£o est√° definida.\")\n",
        "    print(\"\\nüí° SOLU√á√ÉO:\")\n",
        "    print(\"   1. Execute a c√©lula 4 (ETAPA 1) primeiro para carregar os chunks\")\n",
        "    print(\"   2. Ou execute o notebook 02 para processar os dados\")\n",
        "    print(\"=\" * 80)\n",
        "    raise NameError(\n",
        "        \"Vari√°vel 'all_chunks' n√£o encontrada. \"\n",
        "        \"Execute a c√©lula 4 (ETAPA 1) primeiro ou o notebook 02.\"\n",
        "    )\n",
        "\n",
        "if not chunks_available:\n",
        "    raise ValueError(\"Nenhum chunk dispon√≠vel para ingest√£o. Execute o notebook 02 primeiro.\")\n",
        "\n",
        "total_chunks = len(all_chunks)\n",
        "batch_size = settings.BATCH_SIZE\n",
        "total_batches = (total_chunks + batch_size - 1) // batch_size\n",
        "\n",
        "# Estimativa de tempo (baseada em ~1-2 segundos por lote)\n",
        "estimated_minutes = max(1, int(total_batches * 1.5 / 60))\n",
        "if estimated_minutes < 60:\n",
        "    estimated_time = f\"~{estimated_minutes} min\"\n",
        "else:\n",
        "    hours = estimated_minutes // 60\n",
        "    mins = estimated_minutes % 60\n",
        "    estimated_time = f\"~{hours}h {mins}min\" if mins > 0 else f\"~{hours}h\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üöÄ INICIANDO INGEST√ÉO NO PINECONE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìä Informa√ß√µes da Ingest√£o:\")\n",
        "print(f\"   ‚Ä¢ Total de chunks: {total_chunks:,}\")\n",
        "print(f\"   ‚Ä¢ Tamanho do lote: {batch_size}\")\n",
        "print(f\"   ‚Ä¢ Total de lotes: {total_batches:,}\")\n",
        "print(f\"   ‚Ä¢ √çndice Pinecone: {settings.PINECONE_INDEX_NAME}\")\n",
        "if settings.PINECONE_NAMESPACE:\n",
        "    print(f\"   ‚Ä¢ Namespace: {settings.PINECONE_NAMESPACE}\")\n",
        "print(f\"   ‚Ä¢ Provider de embeddings: {embeddings_manager.provider}\")\n",
        "print(f\"   ‚Ä¢ Modelo: {embeddings_manager.model_name}\")\n",
        "print(f\"\\n‚è±Ô∏è  Tempo estimado: {estimated_time}\")\n",
        "print(f\"‚è≥ Este processo pode levar v√°rios minutos...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verifica se o m√≥dulo foi recarregado corretamente\n",
        "# Se voc√™ receber erro sobre par√¢metros inesperados, execute as c√©lulas 3 e 6 novamente\n",
        "import inspect\n",
        "ingest_method = getattr(ingester, 'ingest_chunks', None)\n",
        "if ingest_method:\n",
        "    sig = inspect.signature(ingest_method)\n",
        "    has_checkpoint_params = 'resume_from_checkpoint' in sig.parameters\n",
        "    if not has_checkpoint_params:\n",
        "        print(\"‚ö†Ô∏è  AVISO: O m√≥dulo pinecone_ingester n√£o foi recarregado!\")\n",
        "        print(\"   Execute as c√©lulas 3 e 6 novamente para carregar a vers√£o atualizada.\")\n",
        "        print(\"   Ou reinicie o kernel e execute todas as c√©lulas novamente.\")\n",
        "        raise RuntimeError(\n",
        "            \"M√≥dulo n√£o atualizado. Execute as c√©lulas 3 e 6 novamente, \"\n",
        "            \"ou reinicie o kernel.\"\n",
        "        )\n",
        "\n",
        "# Registra tempo de in√≠cio\n",
        "start_time = time.time()\n",
        "\n",
        "# Realiza a ingest√£o com suporte a checkpointing\n",
        "# O checkpointing permite retomar de onde parou em caso de interrup√ß√£o\n",
        "try:\n",
        "    ingestion_stats = ingester.ingest_chunks(\n",
        "        all_chunks,\n",
        "        batch_size=batch_size,\n",
        "        show_progress=True,\n",
        "        resume_from_checkpoint=True,  # Retoma de checkpoint se existir\n",
        "        checkpoint_interval=10  # Salva checkpoint a cada 10 lotes\n",
        "    )\n",
        "    \n",
        "    # Calcula tempo decorrido\n",
        "    elapsed_time = time.time() - start_time\n",
        "    elapsed_minutes = int(elapsed_time // 60)\n",
        "    elapsed_seconds = int(elapsed_time % 60)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìä ESTAT√çSTICAS DA INGEST√ÉO\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Status da ingest√£o\n",
        "    success_rate = (ingestion_stats['total_vectors'] / ingestion_stats['total_chunks'] * 100) if ingestion_stats['total_chunks'] > 0 else 0\n",
        "    \n",
        "    if ingestion_stats.get('interrupted', False):\n",
        "        status = \"‚è∏Ô∏è  Interrompida (checkpoint salvo)\"\n",
        "    elif success_rate == 100:\n",
        "        status = \"‚úÖ Conclu√≠da com sucesso\"\n",
        "    elif success_rate >= 95:\n",
        "        status = \"‚ö†Ô∏è  Conclu√≠da com alguns erros\"\n",
        "    else:\n",
        "        status = \"‚ùå Conclu√≠da com muitos erros\"\n",
        "    \n",
        "    print(f\"{status}\")\n",
        "    print(f\"\\nüìà Resultados:\")\n",
        "    print(f\"   ‚Ä¢ Total de chunks processados: {ingestion_stats['total_chunks']:,}\")\n",
        "    print(f\"   ‚Ä¢ Total de vetores inseridos: {ingestion_stats['total_vectors']:,}\")\n",
        "    print(f\"   ‚Ä¢ Total de lotes processados: {ingestion_stats['batches']:,}\")\n",
        "    print(f\"   ‚Ä¢ Taxa de sucesso: {success_rate:.1f}%\")\n",
        "    print(f\"\\n‚è±Ô∏è  Tempo decorrido: {elapsed_minutes}min {elapsed_seconds}s\")\n",
        "    \n",
        "    if ingestion_stats.get('interrupted', False):\n",
        "        print(f\"\\nüíæ Checkpoint salvo em: {ingestion_stats.get('checkpoint_path', 'N/A')}\")\n",
        "        print(f\"   Para retomar, execute esta c√©lula novamente.\")\n",
        "        print(f\"   O processo continuar√° automaticamente de onde parou.\")\n",
        "    \n",
        "    if ingestion_stats['errors']:\n",
        "        print(f\"\\n‚ö†Ô∏è  Erros encontrados: {len(ingestion_stats['errors'])}\")\n",
        "        print(\"   Primeiros erros:\")\n",
        "        for i, error in enumerate(ingestion_stats['errors'][:5], 1):\n",
        "            print(f\"   {i}. {error}\")\n",
        "        if len(ingestion_stats['errors']) > 5:\n",
        "            print(f\"   ... e mais {len(ingestion_stats['errors']) - 5} erros\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ Nenhum erro encontrado!\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚ö†Ô∏è  INGEST√ÉO INTERROMPIDA PELO USU√ÅRIO\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"A ingest√£o foi cancelada. Um checkpoint foi salvo automaticamente.\")\n",
        "    print(\"\\nüí° Para retomar:\")\n",
        "    print(\"   1. Execute esta c√©lula novamente\")\n",
        "    print(\"   2. O processo continuar√° automaticamente de onde parou\")\n",
        "    print(\"   3. Os chunks j√° processados foram inseridos no Pinecone\")\n",
        "    print(\"=\" * 80)\n",
        "    # N√£o re-raise para permitir que o usu√°rio veja a mensagem\n",
        "    ingestion_stats = {\"interrupted\": True, \"total_vectors\": 0, \"total_chunks\": total_chunks}\n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚ùå ERRO DURANTE A INGEST√ÉO\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Erro: {e}\")\n",
        "    print(\"\\nüí° Poss√≠veis solu√ß√µes:\")\n",
        "    print(\"   1. Verifique sua conex√£o com a internet\")\n",
        "    print(\"   2. Verifique suas credenciais do Pinecone e Gemini\")\n",
        "    print(\"   3. Verifique se h√° cr√©ditos suficientes no Pinecone\")\n",
        "    print(\"   4. Tente executar novamente com um batch_size menor\")\n",
        "    print(\"   5. Se foi interrompido, execute novamente para retomar de checkpoint\")\n",
        "    print(\"=\" * 80)\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîç VERIFICANDO INGEST√ÉO\n",
            "================================================================================\n",
            "Query de teste: 'mitochondria apoptosis'\n",
            "--------------------------------------------------------------------------------\n",
            "‚úÖ Query executada com sucesso!\n",
            "   Resultados encontrados: 3\n",
            "\n",
            "üìÑ Primeiro resultado:\n",
            "   Article ID: 21645374\n",
            "   Score: 0.600\n",
            "   Texto (primeiros 200 caracteres):\n",
            "   dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ŒîŒ®m). A ...\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ETAPA 5: VERIFICA√á√ÉO DA INGEST√ÉO\n",
        "# ============================================================================\n",
        "# Verifica se os dados foram ingeridos corretamente fazendo uma query de teste\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîç VERIFICANDO INGEST√ÉO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Importa fun√ß√£o de query\n",
        "from scripts.rag_query import query_medical_rag\n",
        "\n",
        "# Faz uma query de teste\n",
        "test_query = \"mitochondria apoptosis\"\n",
        "print(f\"Query de teste: '{test_query}'\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "try:\n",
        "    results = query_medical_rag(\n",
        "        test_query,\n",
        "        embeddings_manager=embeddings_manager,\n",
        "        top_k=3\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Query executada com sucesso!\")\n",
        "    print(f\"   Resultados encontrados: {len(results)}\")\n",
        "    \n",
        "    if results:\n",
        "        print(\"\\nüìÑ Primeiro resultado:\")\n",
        "        print(f\"   Article ID: {results[0]['article_id']}\")\n",
        "        print(f\"   Score: {results[0]['score']:.3f}\")\n",
        "        print(f\"   Texto (primeiros 200 caracteres):\")\n",
        "        print(f\"   {results[0]['text'][:200]}...\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Nenhum resultado encontrado. Verifique se a ingest√£o foi bem-sucedida.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao executar query de teste: {e}\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Conclus√£o da Etapa 3\n",
        "\n",
        "Neste notebook voc√™:\n",
        "- ‚úÖ Configurou o gerenciador de embeddings\n",
        "- ‚úÖ Conectou com o √≠ndice Pinecone\n",
        "- ‚úÖ Gerou embeddings para todos os chunks\n",
        "- ‚úÖ Ingeriu dados no Pinecone em lotes\n",
        "- ‚úÖ Verificou a ingest√£o bem-sucedida\n",
        "\n",
        "## üìå Pr√≥ximos Passos\n",
        "\n",
        "Agora voc√™ est√° pronto para o pr√≥ximo notebook:\n",
        "- **Notebook 04**: Testes de queries RAG\n",
        "- Valida√ß√£o de recupera√ß√£o de contexto\n",
        "- Exemplos pr√°ticos de uso\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
