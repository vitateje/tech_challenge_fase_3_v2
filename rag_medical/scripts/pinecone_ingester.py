"""
MÃ³dulo para ingestÃ£o de dados no Pinecone.

Este mÃ³dulo gerencia a conexÃ£o com Pinecone e a ingestÃ£o de documentos
com embeddings e metadados estruturados.
"""

from typing import List, Dict, Any, Optional
import time
import json
import os
from pathlib import Path
from tenacity import retry, stop_after_attempt, wait_exponential

from config.settings import Settings
from .embeddings_manager import EmbeddingsManager


class PineconeIngester:
    """
    Classe para ingestÃ£o de documentos no Pinecone.
    
    Gerencia:
    - ConexÃ£o com Ã­ndice Pinecone
    - GeraÃ§Ã£o de embeddings
    - IngestÃ£o em lotes com retry logic
    - Metadados estruturados
    """
    
    def __init__(
        self,
        embeddings_manager: Optional[EmbeddingsManager] = None,
        index_name: Optional[str] = None,
        namespace: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """
        Inicializa o ingester do Pinecone.
        
        Args:
            embeddings_manager: Gerenciador de embeddings. Se None, cria um novo.
            index_name: Nome do Ã­ndice Pinecone. Se None, usa das configuraÃ§Ãµes.
            namespace: Namespace do Pinecone. Se None, usa das configuraÃ§Ãµes.
            api_key: API key do Pinecone. Se None, usa das configuraÃ§Ãµes.
        """
        self.settings = Settings()
        
        # ConfiguraÃ§Ãµes do Pinecone
        self.index_name = index_name or self.settings.PINECONE_INDEX_NAME
        self.namespace = namespace or self.settings.PINECONE_NAMESPACE
        self.api_key = api_key or self.settings.PINECONE_API_KEY
        
        if not self.api_key:
            raise ValueError(
                "PINECONE_API_KEY nÃ£o configurada. "
                "Configure no arquivo .env ou passe como parÃ¢metro."
            )
        
        # Inicializa embeddings manager
        if embeddings_manager is None:
            self.embeddings_manager = EmbeddingsManager()
        else:
            self.embeddings_manager = embeddings_manager
        
        # Inicializa cliente Pinecone
        self._init_pinecone()
        
        # Valida compatibilidade de dimensÃµes
        self._validate_dimensions()
        
        # DiretÃ³rio para checkpoints (usa o diretÃ³rio raiz do projeto)
        # Tenta usar MEDICAL_DATA_PATH se disponÃ­vel, senÃ£o usa o diretÃ³rio do mÃ³dulo
        if hasattr(self.settings, 'MEDICAL_DATA_PATH') and self.settings.MEDICAL_DATA_PATH:
            checkpoint_base = Path(self.settings.MEDICAL_DATA_PATH).parent
        else:
            # Fallback: usa o diretÃ³rio raiz do projeto (onde estÃ¡ config/)
            checkpoint_base = Path(__file__).parent.parent
        
        self.checkpoint_dir = checkpoint_base / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)
    
    def _init_pinecone(self):
        """Inicializa cliente Pinecone."""
        try:
            from pinecone import Pinecone
            
            self.pinecone_client = Pinecone(api_key=self.api_key)
            self.index = self.pinecone_client.Index(self.index_name)
            
            print(f"âœ… Pinecone inicializado: Ã­ndice '{self.index_name}'")
            if self.namespace:
                print(f"   Namespace: {self.namespace}")
            
        except ImportError:
            raise ImportError(
                "pinecone nÃ£o instalado. "
                "Instale com: pip install pinecone"
            )
        except Exception as e:
            raise RuntimeError(f"Erro ao inicializar Pinecone: {e}")
    
    def _validate_dimensions(self):
        """Valida compatibilidade de dimensÃµes entre embeddings e Ã­ndice."""
        try:
            # ObtÃ©m estatÃ­sticas do Ã­ndice
            stats = self.index.describe_index_stats()
            
            # ObtÃ©m dimensÃ£o dos embeddings
            embedding_dim = self.embeddings_manager.get_embedding_dimension()
            
            # Tenta obter a dimensÃ£o do Ã­ndice
            # A dimensÃ£o pode estar em diferentes lugares dependendo da versÃ£o da API
            index_dimension = None
            
            # Tenta obter do stats (formato mais recente)
            if hasattr(stats, 'dimension'):
                index_dimension = stats.dimension
            elif isinstance(stats, dict) and 'dimension' in stats:
                index_dimension = stats['dimension']
            # Tenta obter do index_info (formato alternativo)
            elif hasattr(self.index, 'describe_index'):
                try:
                    index_info = self.index.describe_index()
                    if hasattr(index_info, 'dimension'):
                        index_dimension = index_info.dimension
                    elif isinstance(index_info, dict) and 'dimension' in index_info:
                        index_dimension = index_info['dimension']
                except:
                    pass
            
            print(f"   DimensÃ£o dos embeddings: {embedding_dim}")
            
            # Se conseguiu obter a dimensÃ£o do Ã­ndice, valida compatibilidade
            if index_dimension is not None:
                print(f"   DimensÃ£o do Ã­ndice Pinecone: {index_dimension}")
                
                if embedding_dim != index_dimension:
                    print("\n" + "=" * 80)
                    print("âš ï¸  INCOMPATIBILIDADE DE DIMENSÃ•ES DETECTADA!")
                    print("=" * 80)
                    print(f"   DimensÃ£o dos embeddings: {embedding_dim}")
                    print(f"   DimensÃ£o do Ã­ndice Pinecone: {index_dimension}")
                    print("\nðŸ’¡ SOLUÃ‡Ã•ES:")
                    print("   1. Use um modelo de embedding compatÃ­vel:")
                    if index_dimension == 1024:
                        print("      - Configure Ollama com modelo de 1024 dimensÃµes")
                        print("      - Ou recrie o Ã­ndice Pinecone com 768 dimensÃµes")
                    elif index_dimension == 768:
                        print("      - O modelo atual (Gemini text-embedding-004) estÃ¡ correto")
                    print("\n   2. Se o Ã­ndice foi criado com 'llama-text-embed-v2' (1024 dims):")
                    print("      - Use Ollama com modelo compatÃ­vel (ex: mxbai-embed-large)")
                    print("      - Ou recrie o Ã­ndice com 768 dimensÃµes para usar Gemini")
                    print("=" * 80)
                    print("\nâš ï¸  Continuando, mas resultados podem ser subÃ³timos.")
                    print("   Recomenda-se usar embeddings com a mesma dimensÃ£o do Ã­ndice.\n")
                else:
                    print("   âœ… DimensÃµes compatÃ­veis!")
            else:
                print("   âš ï¸  NÃ£o foi possÃ­vel obter a dimensÃ£o do Ã­ndice automaticamente")
                print("   Verifique manualmente se as dimensÃµes sÃ£o compatÃ­veis")
            
        except Exception as e:
            print(f"âš ï¸  Aviso: NÃ£o foi possÃ­vel validar dimensÃµes: {e}")
            print(f"   DimensÃ£o dos embeddings: {self.embeddings_manager.get_embedding_dimension()}")
    
    def _create_vector_id(self, article_id: str, chunk_index: int) -> str:
        """
        Cria ID Ãºnico para um vetor no Pinecone.
        
        Args:
            article_id: ID do artigo.
            chunk_index: Ãndice do chunk.
            
        Returns:
            ID Ãºnico no formato: article_{article_id}_chunk_{chunk_index}
        """
        return f"article_{article_id}_chunk_{chunk_index}"
    
    def _prepare_vectors(
        self,
        chunks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Prepara vetores para ingestÃ£o no Pinecone.
        
        Args:
            chunks: Lista de chunks com campos "text", "article_id", "metadata".
            
        Returns:
            Lista de dicionÃ¡rios no formato Pinecone:
                {
                    "id": str,
                    "values": List[float],
                    "metadata": Dict[str, Any]
                }
        """
        # Extrai textos dos chunks
        texts = [chunk["text"] for chunk in chunks]
        
        # Gera embeddings em lote
        print(f"   Gerando embeddings para {len(texts)} chunks...")
        embeddings = self.embeddings_manager.embed_documents(texts)
        
        # Prepara vetores
        vectors = []
        
        for chunk, embedding in zip(chunks, embeddings):
            vector_id = self._create_vector_id(
                chunk["article_id"],
                chunk["chunk_index"]
            )
            
            # Prepara metadados (Pinecone requer valores primitivos)
            metadata = self._prepare_metadata(chunk.get("metadata", {}))
            
            # Adiciona texto aos metadados para recuperaÃ§Ã£o
            metadata["text"] = chunk["text"]
            
            vectors.append({
                "id": vector_id,
                "values": embedding,
                "metadata": metadata,
            })
        
        return vectors
    
    def _prepare_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Prepara metadados para formato compatÃ­vel com Pinecone.
        
        Pinecone aceita apenas tipos primitivos (str, int, float, bool, list).
        
        Args:
            metadata: Metadados originais.
            
        Returns:
            Metadados formatados.
        """
        prepared = {}
        
        for key, value in metadata.items():
            # Converte tipos nÃ£o primitivos para string
            if isinstance(value, (str, int, float, bool)):
                prepared[key] = value
            elif isinstance(value, list):
                # Listas sÃ£o permitidas se contiverem apenas primitivos
                prepared[key] = [
                    str(v) if not isinstance(v, (str, int, float, bool)) else v
                    for v in value
                ]
            else:
                prepared[key] = str(value)
        
        return prepared
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    def _upsert_batch(self, vectors: List[Dict[str, Any]]):
        """
        Insere/atualiza um lote de vetores no Pinecone (com retry).
        
        Args:
            vectors: Lista de vetores para inserir.
        """
        try:
            if self.namespace:
                self.index.upsert(vectors=vectors, namespace=self.namespace)
            else:
                self.index.upsert(vectors=vectors)
        except Exception as e:
            print(f"âš ï¸  Erro ao inserir lote: {e}")
            raise
    
    def _get_checkpoint_path(self) -> Path:
        """Retorna o caminho do arquivo de checkpoint."""
        checkpoint_name = f"ingestion_checkpoint_{self.index_name}_{self.namespace or 'default'}.json"
        return self.checkpoint_dir / checkpoint_name
    
    def _save_checkpoint(self, processed_indices: List[int], total_chunks: int):
        """Salva checkpoint do progresso."""
        checkpoint_data = {
            "processed_indices": processed_indices,
            "total_chunks": total_chunks,
            "index_name": self.index_name,
            "namespace": self.namespace,
            "timestamp": time.time()
        }
        checkpoint_path = self._get_checkpoint_path()
        with open(checkpoint_path, 'w') as f:
            json.dump(checkpoint_data, f, indent=2)
    
    def _load_checkpoint(self) -> Optional[Dict[str, Any]]:
        """Carrega checkpoint do progresso."""
        checkpoint_path = self._get_checkpoint_path()
        if checkpoint_path.exists():
            try:
                with open(checkpoint_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  Erro ao carregar checkpoint: {e}")
        return None
    
    def _clear_checkpoint(self):
        """Remove checkpoint."""
        checkpoint_path = self._get_checkpoint_path()
        if checkpoint_path.exists():
            checkpoint_path.unlink()
    
    def ingest_chunks(
        self,
        chunks: List[Dict[str, Any]],
        batch_size: Optional[int] = None,
        show_progress: bool = True,
        resume_from_checkpoint: bool = True,
        checkpoint_interval: int = 10
    ) -> Dict[str, Any]:
        """
        Ingere chunks no Pinecone em lotes com suporte a checkpointing.
        
        Args:
            chunks: Lista de chunks para ingerir.
            batch_size: Tamanho do lote. Se None, usa das configuraÃ§Ãµes.
            show_progress: Se True, exibe barra de progresso.
            resume_from_checkpoint: Se True, tenta retomar de checkpoint existente.
            checkpoint_interval: Intervalo (em lotes) para salvar checkpoint.
            
        Returns:
            DicionÃ¡rio com estatÃ­sticas da ingestÃ£o:
                - total_chunks: Total de chunks processados
                - total_vectors: Total de vetores inseridos
                - batches: NÃºmero de lotes
                - errors: Lista de erros (se houver)
                - interrupted: Se True, processo foi interrompido
        """
        if not chunks:
            return {
                "total_chunks": 0,
                "total_vectors": 0,
                "batches": 0,
                "errors": [],
                "interrupted": False,
            }
        
        batch_size = batch_size or self.settings.BATCH_SIZE
        total_chunks = len(chunks)
        total_vectors = 0
        errors = []
        processed_indices = []
        start_index = 0
        interrupted = False
        
        # Tenta carregar checkpoint
        if resume_from_checkpoint:
            checkpoint = self._load_checkpoint()
            if checkpoint:
                if (checkpoint.get("total_chunks") == total_chunks and
                    checkpoint.get("index_name") == self.index_name and
                    checkpoint.get("namespace") == self.namespace):
                    processed_indices = checkpoint.get("processed_indices", [])
                    start_index = max(processed_indices) + 1 if processed_indices else 0
                    total_vectors = len(processed_indices)
                    print(f"\nðŸ“‹ Checkpoint encontrado! Retomando de Ã­ndice {start_index}")
                    print(f"   JÃ¡ processados: {total_vectors}/{total_chunks} chunks")
                else:
                    print("âš ï¸  Checkpoint incompatÃ­vel (diferentes chunks/Ã­ndice). Ignorando...")
                    self._clear_checkpoint()
        
        print(f"\nðŸš€ Iniciando ingestÃ£o de {total_chunks} chunks no Pinecone...")
        print(f"   Batch size: {batch_size}")
        print(f"   Ãndice: {self.index_name}")
        if self.namespace:
            print(f"   Namespace: {self.namespace}")
        if start_index > 0:
            print(f"   Retomando de: {start_index}/{total_chunks}")
        
        # Processa em lotes
        try:
            from tqdm import tqdm
            iterator = range(start_index, total_chunks, batch_size)
            if show_progress:
                iterator = tqdm(iterator, desc="Ingerindo chunks", initial=start_index, total=total_chunks)
        except ImportError:
            iterator = range(start_index, total_chunks, batch_size)
        
        try:
            batch_num = 0
            for i in iterator:
                batch_chunks = chunks[i:i + batch_size]
                batch_num += 1
                
                try:
                    # Prepara vetores do lote
                    vectors = self._prepare_vectors(batch_chunks)
                    
                    # Insere no Pinecone
                    self._upsert_batch(vectors)
                    
                    # Marca Ã­ndices como processados
                    batch_indices = list(range(i, min(i + batch_size, total_chunks)))
                    processed_indices.extend(batch_indices)
                    total_vectors += len(vectors)
                    
                    # Salva checkpoint periodicamente
                    if batch_num % checkpoint_interval == 0:
                        self._save_checkpoint(processed_indices, total_chunks)
                        if show_progress:
                            print(f"\nðŸ’¾ Checkpoint salvo: {total_vectors}/{total_chunks} chunks processados")
                    
                    # Pequena pausa para evitar rate limiting
                    if i + batch_size < total_chunks:
                        time.sleep(0.1)
                        
                except KeyboardInterrupt:
                    # Salva checkpoint antes de interromper
                    print(f"\n\nâš ï¸  InterrupÃ§Ã£o detectada! Salvando checkpoint...")
                    self._save_checkpoint(processed_indices, total_chunks)
                    interrupted = True
                    raise
                except Exception as e:
                    error_msg = f"Erro no lote {i//batch_size + 1}: {e}"
                    errors.append(error_msg)
                    print(f"âš ï¸  {error_msg}")
                    # Continua com prÃ³ximo lote mesmo em caso de erro
                    continue
            
            # Salva checkpoint final
            self._save_checkpoint(processed_indices, total_chunks)
            
            # Remove checkpoint se concluÃ­do com sucesso
            if not interrupted:
                self._clear_checkpoint()
                print(f"\nâœ… IngestÃ£o concluÃ­da!")
            else:
                print(f"\nâ¸ï¸  IngestÃ£o interrompida!")
            
            print(f"   Vetores inseridos: {total_vectors}/{total_chunks}")
            if errors:
                print(f"   Erros: {len(errors)}")
            
        except KeyboardInterrupt:
            # Salva checkpoint antes de sair
            if not interrupted:  # Evita salvar duas vezes
                print(f"\n\nâš ï¸  InterrupÃ§Ã£o detectada! Salvando checkpoint...")
                self._save_checkpoint(processed_indices, total_chunks)
                interrupted = True
            print(f"\nâ¸ï¸  Processo interrompido pelo usuÃ¡rio")
            print(f"   Progresso salvo: {total_vectors}/{total_chunks} chunks")
            print(f"   Para retomar, execute novamente com resume_from_checkpoint=True")
        
        return {
            "total_chunks": total_chunks,
            "total_vectors": total_vectors,
            "batches": (total_chunks + batch_size - 1) // batch_size,
            "errors": errors,
            "interrupted": interrupted,
            "checkpoint_path": str(self._get_checkpoint_path()) if interrupted else None,
        }
    
    def delete_all(self, namespace: Optional[str] = None):
        """
        Deleta todos os vetores do namespace (use com cuidado!).
        
        Args:
            namespace: Namespace a limpar. Se None, usa o namespace configurado.
        """
        namespace = namespace or self.namespace
        
        print(f"âš ï¸  ATENÃ‡ÃƒO: Deletando todos os vetores do namespace '{namespace}'...")
        response = input("Tem certeza? Digite 'SIM' para confirmar: ")
        
        if response != "SIM":
            print("OperaÃ§Ã£o cancelada.")
            return
        
        try:
            if namespace:
                self.index.delete(delete_all=True, namespace=namespace)
            else:
                self.index.delete(delete_all=True)
            
            print("âœ… Todos os vetores foram deletados.")
        except Exception as e:
            print(f"âŒ Erro ao deletar vetores: {e}")

