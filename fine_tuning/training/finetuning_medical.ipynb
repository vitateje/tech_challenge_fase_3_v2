{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning de Modelo LLM para Dom√≠nio M√©dico\n",
        "\n",
        "Este notebook implementa o pipeline completo de fine-tuning de um modelo LLM para tarefas de question-answering m√©dico baseado em evid√™ncias cient√≠ficas.\n",
        "\n",
        "## Objetivos:\n",
        "1. Carregar dataset m√©dico formatado no padr√£o Alpaca\n",
        "2. Carregar modelo base pr√©-quantizado (Unsloth)\n",
        "3. Configurar LoRA para treinamento eficiente\n",
        "4. Treinar modelo com dados m√©dicos\n",
        "5. Testar e salvar modelo treinado\n",
        "\n",
        "## Requisitos:\n",
        "- GPU com pelo menos 8GB VRAM (recomendado 16GB+)\n",
        "- CUDA instalado\n",
        "- Bibliotecas: unsloth, transformers, datasets, trl\n",
        "\n",
        "## Ordem de Execu√ß√£o:\n",
        "Execute as c√©lulas **sequencialmente** (de cima para baixo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 1: INSTALA√á√ÉO DE DEPEND√äNCIAS (OPCIONAL)\n",
        "# ============================================================================\n",
        "# Execute esta c√©lula apenas se estiver usando Google Colab ou se as\n",
        "# bibliotecas n√£o estiverem instaladas no seu ambiente local.\n",
        "#\n",
        "# Para ambiente local, instale via pip no terminal:\n",
        "#   pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'\n",
        "#   pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "#   pip install transformers datasets\n",
        "\n",
        "# Descomente as linhas abaixo se precisar instalar:\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "# !pip install transformers datasets\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias verificadas. Se houver erro, instale as bibliotecas acima.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2: IMPORTA√á√ïES E CONFIGURA√á√ïES\n",
        "# ============================================================================\n",
        "# Esta c√©lula importa todas as bibliotecas necess√°rias e carrega as\n",
        "# configura√ß√µes centralizadas do m√≥dulo model_config.py\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Adiciona diret√≥rio raiz ao path para imports\n",
        "sys.path.append(str(Path().absolute().parent.parent))\n",
        "\n",
        "# Importa bibliotecas principais\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "\n",
        "# Importa configura√ß√µes e utilit√°rios locais\n",
        "from training.model_config import (\n",
        "    get_model_config, get_lora_config, get_training_config,\n",
        "    get_dataset_config, get_inference_config\n",
        ")\n",
        "from utils.prompts import get_medical_alpaca_prompt, get_instruction_only\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 3: CONFIGURA√á√ïES E CAMINHOS\n",
        "# ============================================================================\n",
        "# Define caminhos e carrega configura√ß√µes centralizadas\n",
        "\n",
        "# Obt√©m configura√ß√µes\n",
        "model_config = get_model_config()\n",
        "lora_config = get_lora_config()\n",
        "training_config = get_training_config()\n",
        "dataset_config = get_dataset_config()\n",
        "\n",
        "# Define caminhos (ajuste conforme necess√°rio)\n",
        "BASE_DIR = Path().absolute().parent.parent\n",
        "FORMATTED_DATASET_PATH = BASE_DIR / \"formatted_medical_dataset.json\"\n",
        "MODEL_OUTPUT_DIR = BASE_DIR / \"lora_model_medical\"\n",
        "TRAINING_OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURA√á√ïES DE FINE-TUNING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Modelo: {model_config['default_model']}\")\n",
        "print(f\"Max sequence length: {model_config['max_seq_length']}\")\n",
        "print(f\"LoRA rank: {lora_config['r']}\")\n",
        "print(f\"Learning rate: {training_config['learning_rate']}\")\n",
        "print(f\"Max steps: {training_config['max_steps']}\")\n",
        "print(f\"Dataset: {FORMATTED_DATASET_PATH}\")\n",
        "print(f\"Output model: {MODEL_OUTPUT_DIR}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4: CARREGAMENTO DO DATASET FORMATADO\n",
        "# ============================================================================\n",
        "# Carrega o dataset m√©dico j√° formatado no padr√£o Alpaca.\n",
        "# Este dataset deve ter sido gerado pelo script format_dataset.py\n",
        "\n",
        "if not FORMATTED_DATASET_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Dataset n√£o encontrado: {FORMATTED_DATASET_PATH}\\n\"\n",
        "        f\"Execute primeiro: python preprocessing/format_dataset.py\"\n",
        "    )\n",
        "\n",
        "print(f\"üì¶ Carregando dataset de: {FORMATTED_DATASET_PATH}\")\n",
        "\n",
        "# load_dataset do Hugging Face carrega JSON diretamente\n",
        "dataset = load_dataset(\"json\", data_files=str(FORMATTED_DATASET_PATH), split=\"train\")\n",
        "\n",
        "print(f\"‚úÖ Dataset carregado: {len(dataset)} exemplos\")\n",
        "print(f\"   Estrutura: {dataset.features}\")\n",
        "print(f\"\\nExemplo de entrada:\")\n",
        "print(f\"   Instruction: {dataset[0]['instruction'][:100]}...\")\n",
        "print(f\"   Input: {dataset[0]['input'][:100]}...\")\n",
        "print(f\"   Output: {dataset[0]['output'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 5: CARREGAMENTO DO MODELO BASE\n",
        "# ============================================================================\n",
        "# Carrega modelo pr√©-quantizado do Unsloth.\n",
        "# Unsloth fornece modelos otimizados que reduzem uso de mem√≥ria em ~75%\n",
        "# mantendo qualidade pr√≥xima ao modelo original.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CARREGANDO MODELO BASE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_config['default_model'],\n",
        "    max_seq_length=model_config['max_seq_length'],\n",
        "    dtype=model_config['dtype'],\n",
        "    load_in_4bit=model_config['load_in_4bit'],\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo carregado!\")\n",
        "print(f\"   Par√¢metros totais: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 6: CONFIGURA√á√ÉO LoRA\n",
        "# ============================================================================\n",
        "# LoRA (Low-Rank Adaptation) permite treinar apenas ~1-5% dos par√¢metros,\n",
        "# reduzindo drasticamente mem√≥ria e tempo de treinamento.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURANDO LoRA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_config['r'],\n",
        "    target_modules=lora_config['target_modules'],\n",
        "    lora_alpha=lora_config['lora_alpha'],\n",
        "    lora_dropout=lora_config['lora_dropout'],\n",
        "    bias=lora_config['bias'],\n",
        "    use_gradient_checkpointing=lora_config['use_gradient_checkpointing'],\n",
        "    random_state=lora_config['random_state'],\n",
        "    use_rslora=lora_config['use_rslora'],\n",
        "    loftq_config=lora_config['loftq_config'],\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"‚úÖ LoRA configurado!\")\n",
        "print(f\"   Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "print(f\"   Par√¢metros totais: {total_params:,}\")\n",
        "print(f\"   Fra√ß√£o trein√°vel: {(trainable_params/total_params)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 7: DEFINI√á√ÉO DO PROMPT ALPACA M√âDICO\n",
        "# ============================================================================\n",
        "# Define a fun√ß√£o que formata exemplos do dataset para o formato Alpaca.\n",
        "# Esta fun√ß√£o ser√° aplicada a cada exemplo durante o treinamento.\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Formata exemplos para o formato Alpaca m√©dico\n",
        "    \n",
        "    Combina instruction, input e output em um √∫nico texto formatado\n",
        "    que o modelo aprender√° a gerar.\n",
        "    \"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    \n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        # Usa template Alpaca m√©dico\n",
        "        text = get_medical_alpaca_prompt(instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    \n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"‚úÖ Fun√ß√£o de formata√ß√£o definida!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 8: PREPARA√á√ÉO DO DATASET PARA TREINAMENTO\n",
        "# ============================================================================\n",
        "# Aplica formata√ß√£o de prompts a todos os exemplos do dataset\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FORMATANDO DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "formatted_dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset formatado: {len(formatted_dataset)} exemplos\")\n",
        "print(f\"   Estrutura: {formatted_dataset.features}\")\n",
        "\n",
        "# Mostra exemplo formatado\n",
        "print(f\"\\nExemplo de texto formatado (primeiros 500 caracteres):\")\n",
        "print(\"-\" * 80)\n",
        "print(formatted_dataset[0]['text'][:500] + \"...\")\n",
        "print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 9: CONFIGURA√á√ÉO DO TRAINER\n",
        "# ============================================================================\n",
        "# Configura o SFTTrainer (Supervised Fine-Tuning Trainer) que gerencia\n",
        "# todo o processo de treinamento.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURANDO TRAINER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=training_config['per_device_train_batch_size'],\n",
        "    gradient_accumulation_steps=training_config['gradient_accumulation_steps'],\n",
        "    warmup_steps=training_config['warmup_steps'],\n",
        "    max_steps=training_config['max_steps'],\n",
        "    learning_rate=training_config['learning_rate'],\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=training_config['logging_steps'],\n",
        "    optim=training_config['optim'],\n",
        "    weight_decay=training_config['weight_decay'],\n",
        "    lr_scheduler_type=training_config['lr_scheduler_type'],\n",
        "    seed=training_config['seed'],\n",
        "    output_dir=str(TRAINING_OUTPUT_DIR),\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=model_config['max_seq_length'],\n",
        "    dataset_num_proc=dataset_config['dataset_num_proc'],\n",
        "    packing=dataset_config['packing'],\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configurado!\")\n",
        "print(f\"   Batch efetivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Max steps: {training_args.max_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 10: TREINAMENTO DO MODELO\n",
        "# ============================================================================\n",
        "# Inicia o processo de treinamento. Este processo pode levar v√°rios minutos\n",
        "# ou horas dependendo do tamanho do dataset e performance da GPU.\n",
        "#\n",
        "# Durante o treinamento, voc√™ ver√° logs mostrando:\n",
        "# - Loss (deve diminuir ao longo do tempo)\n",
        "# - Learning rate atual\n",
        "# - Progresso (steps completados)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INICIANDO TREINAMENTO\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚ö†Ô∏è  Este processo pode levar v√°rios minutos ou horas...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ TREINAMENTO CONCLU√çDO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Loss final: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Steps completados: {trainer_stats.global_step}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 11: TESTE DO MODELO TREINADO\n",
        "# ============================================================================\n",
        "# Testa o modelo com um exemplo m√©dico para verificar a qualidade\n",
        "# das respostas geradas.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTANDO MODELO TREINADO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepara modelo para infer√™ncia\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Exemplo de teste\n",
        "example_instruction = get_instruction_only()\n",
        "example_input = \"\"\"Contexto: Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant produces perforations in its leaves through PCD.\n",
        "Pergunta: Do mitochondria play a role in remodelling plant leaves during programmed cell death?\"\"\"\n",
        "\n",
        "# Formata prompt (sem resposta, queremos que o modelo gere)\n",
        "prompt = get_medical_alpaca_prompt(example_instruction, example_input, \"\")\n",
        "\n",
        "# Tokeniza e gera\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "inference_cfg = get_inference_config()\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=inference_cfg['max_new_tokens'],\n",
        "    use_cache=inference_cfg['use_cache'],\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(\"Prompt de entrada:\")\n",
        "print(\"-\" * 80)\n",
        "print(prompt[:300] + \"...\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nResposta gerada:\")\n",
        "print(\"-\" * 80)\n",
        "print(generated_text)\n",
        "print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 12: SALVAMENTO DO MODELO\n",
        "# ============================================================================\n",
        "# Salva o modelo treinado (apenas adaptadores LoRA) e o tokenizer.\n",
        "# O modelo salvo pode ser carregado depois para infer√™ncia.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SALVANDO MODELO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "tokenizer.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "\n",
        "print(f\"‚úÖ Modelo salvo em: {MODEL_OUTPUT_DIR}\")\n",
        "print(\"\\nPara carregar o modelo depois, use:\")\n",
        "print(f\"  model, tokenizer = FastLanguageModel.from_pretrained('{MODEL_OUTPUT_DIR}')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 13: CARREGAMENTO E TESTE DO MODELO SALVO\n",
        "# ============================================================================\n",
        "# Demonstra como carregar o modelo salvo e fazer infer√™ncia.\n",
        "# Esta c√©lula √© √∫til para testar o modelo ap√≥s reiniciar o ambiente.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CARREGANDO MODELO SALVO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Carrega modelo salvo\n",
        "loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(MODEL_OUTPUT_DIR),\n",
        "    max_seq_length=model_config['max_seq_length'],\n",
        "    dtype=model_config['dtype'],\n",
        "    load_in_4bit=model_config['load_in_4bit'],\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(loaded_model)\n",
        "\n",
        "print(\"‚úÖ Modelo carregado com sucesso!\")\n",
        "\n",
        "# Testa com outro exemplo\n",
        "example_instruction = get_instruction_only()\n",
        "example_input = \"\"\"Contexto: Assessment of visual acuity depends on the optotypes used for measurement.\n",
        "Pergunta: What are the differences between Landolt C and Snellen E acuity in strabismus amblyopia?\"\"\"\n",
        "\n",
        "prompt = get_medical_alpaca_prompt(example_instruction, example_input, \"\")\n",
        "inputs = loaded_tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Usa TextStreamer para visualizar gera√ß√£o em tempo real\n",
        "text_streamer = TextStreamer(loaded_tokenizer)\n",
        "_ = loaded_model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=inference_cfg['max_new_tokens']\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
