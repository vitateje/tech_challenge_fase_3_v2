{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning de Modelo LLM para Dom√≠nio M√©dico\n",
        "\n",
        "Este notebook implementa o pipeline completo de fine-tuning de um modelo LLM para tarefas de question-answering m√©dico baseado em evid√™ncias cient√≠ficas.\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANTE: Pr√©-requisito ANTES de usar este notebook\n",
        "\n",
        "**Voc√™ PRECISA processar o dataset m√©dico ANTES de usar este notebook!**\n",
        "\n",
        "### Passo 1: Preparar Dataset (FAZER LOCALMENTE, ANTES DO COLAB)\n",
        "\n",
        "No seu computador local, execute:\n",
        "\n",
        "```bash\n",
        "cd fine_tuning\n",
        "python run_pipeline.py --all\n",
        "```\n",
        "\n",
        "Isso ir√°:\n",
        "1. Processar o dataset m√©dico (`ori_pqal.json`)\n",
        "2. Anonimizar dados sens√≠veis\n",
        "3. Formatar no padr√£o Alpaca\n",
        "4. Gerar o arquivo: `formatted_medical_dataset.json`\n",
        "\n",
        "### Passo 2: Usar no Google Colab\n",
        "\n",
        "1. Fa√ßa upload do arquivo `formatted_medical_dataset.json` para o Colab (c√©lula 4)\n",
        "2. Execute todas as c√©lulas sequencialmente\n",
        "3. O modelo treinado ser√° salvo e voc√™ pode fazer download\n",
        "\n",
        "## Objetivos:\n",
        "1. Carregar dataset m√©dico formatado no padr√£o Alpaca\n",
        "2. Carregar modelo base pr√©-quantizado (Unsloth)\n",
        "3. Configurar LoRA para treinamento eficiente\n",
        "4. Treinar modelo com dados m√©dicos\n",
        "5. Testar e salvar modelo treinado\n",
        "\n",
        "## Requisitos:\n",
        "- **Google Colab** (recomendado) ou ambiente com GPU\n",
        "- GPU com pelo menos 8GB VRAM (recomendado 16GB+)\n",
        "- Dataset formatado: `formatted_medical_dataset.json` (gerado pelo `run_pipeline.py`)\n",
        "\n",
        "## Ordem de Execu√ß√£o:\n",
        "Execute as c√©lulas **sequencialmente** (de cima para baixo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 1: INSTALA√á√ÉO DE DEPEND√äNCIAS (OBRIGAT√ìRIO NO COLAB)\n",
        "# ============================================================================\n",
        "# Esta c√©lula instala todas as bibliotecas necess√°rias para o fine-tuning.\n",
        "# Execute esta c√©lula primeiro se estiver usando Google Colab.\n",
        "\n",
        "print(\"üì¶ Instalando depend√™ncias...\")\n",
        "print(\"‚è≥ Isso pode levar alguns minutos na primeira execu√ß√£o...\")\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets\n",
        "\n",
        "print(\"\\n‚úÖ Depend√™ncias instaladas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2: IMPORTA√á√ïES DE BIBLIOTECAS\n",
        "# ============================================================================\n",
        "# Importa todas as bibliotecas necess√°rias para o fine-tuning\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  GPU n√£o detectada! Fine-tuning ser√° muito lento em CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 3: CONFIGURA√á√ïES COMPLETAS (TUDO INLINE)\n",
        "# ============================================================================\n",
        "# Todas as configura√ß√µes est√£o definidas aqui - n√£o precisa de arquivos externos\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES DO MODELO BASE\n",
        "# ============================================================================\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "DTYPE = None  # Auto-detect\n",
        "LOAD_IN_4BIT = True\n",
        "DEFAULT_MODEL = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES LoRA\n",
        "# ============================================================================\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 16,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0,\n",
        "    \"bias\": \"none\",\n",
        "    \"target_modules\": [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    \"use_gradient_checkpointing\": \"unsloth\",\n",
        "    \"random_state\": 3407,\n",
        "    \"use_rslora\": False,\n",
        "    \"loftq_config\": None,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# HIPERPAR√ÇMETROS DE TREINAMENTO\n",
        "# ============================================================================\n",
        "TRAINING_CONFIG = {\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"warmup_steps\": 5,\n",
        "    \"max_steps\": 100,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"optim\": \"adamw_8bit\",\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"lr_scheduler_type\": \"linear\",\n",
        "    \"seed\": 3407,\n",
        "    \"output_dir\": \"outputs\",\n",
        "    \"logging_steps\": 1,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES DE DATASET\n",
        "# ============================================================================\n",
        "DATASET_CONFIG = {\n",
        "    \"dataset_num_proc\": 2,\n",
        "    \"packing\": False,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES DE INFER√äNCIA\n",
        "# ============================================================================\n",
        "INFERENCE_CONFIG = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"use_cache\": True,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.9,\n",
        "    \"top_k\": 50,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# FUN√á√ïES DE PROMPTS (INLINE)\n",
        "# ============================================================================\n",
        "MEDICAL_ALPACA_PROMPT = \"\"\"Below is a medical instruction that describes a task, paired with medical context and a question. Write a response that appropriately completes the request based on the provided medical evidence.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def get_medical_alpaca_prompt(instruction: str, input_text: str, response: str = \"\") -> str:\n",
        "    \"\"\"Formata um prompt m√©dico usando o template Alpaca\"\"\"\n",
        "    return MEDICAL_ALPACA_PROMPT.format(instruction, input_text, response)\n",
        "\n",
        "def get_instruction_only() -> str:\n",
        "    \"\"\"Retorna apenas a instru√ß√£o padr√£o para tarefas m√©dicas\"\"\"\n",
        "    return \"Responda √† pergunta baseando-se nos contextos fornecidos.\"\n",
        "\n",
        "# ============================================================================\n",
        "# CAMINHOS (AJUSTE PARA COLAB)\n",
        "# ============================================================================\n",
        "# No Colab, voc√™ pode:\n",
        "# 1. Fazer upload do arquivo diretamente (use o caminho abaixo)\n",
        "# 2. Montar Google Drive e usar caminho do Drive\n",
        "# 3. Usar caminho absoluto do arquivo\n",
        "\n",
        "FORMATTED_DATASET_PATH = Path(\"formatted_medical_dataset.json\")  # Arquivo na raiz do Colab\n",
        "MODEL_OUTPUT_DIR = Path(\"lora_model_medical\")\n",
        "TRAINING_OUTPUT_DIR = Path(\"outputs\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURA√á√ïES DE FINE-TUNING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Modelo: {DEFAULT_MODEL}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"LoRA rank: {LORA_CONFIG['r']}\")\n",
        "print(f\"Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
        "print(f\"Max steps: {TRAINING_CONFIG['max_steps']}\")\n",
        "print(f\"Dataset: {FORMATTED_DATASET_PATH}\")\n",
        "print(f\"Output model: {MODEL_OUTPUT_DIR}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4: UPLOAD DO DATASET FORMATADO\n",
        "# ============================================================================\n",
        "# ‚ö†Ô∏è IMPORTANTE: Este notebook espera que voc√™ j√° tenha processado o dataset!\n",
        "#\n",
        "# ANTES de usar este notebook, voc√™ DEVE ter executado localmente:\n",
        "#   cd fine_tuning\n",
        "#   python run_pipeline.py --all\n",
        "#\n",
        "# Isso gera o arquivo: formatted_medical_dataset.json\n",
        "#\n",
        "# Agora, fa√ßa upload desse arquivo para o Colab:\n",
        "\n",
        "# Verifica se est√° no Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Verifica se o arquivo j√° existe\n",
        "if not FORMATTED_DATASET_PATH.exists():\n",
        "    if IN_COLAB:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"üì§ UPLOAD DO DATASET FORMATADO\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"\\n‚ö†Ô∏è  Arquivo n√£o encontrado!\")\n",
        "        print(\"\\nüìã INSTRU√á√ïES:\")\n",
        "        print(\"   1. Voc√™ deve ter processado o dataset ANTES de usar este notebook\")\n",
        "        print(\"   2. Execute localmente: python run_pipeline.py --all\")\n",
        "        print(\"   3. Isso gera: formatted_medical_dataset.json\")\n",
        "        print(\"   4. Agora fa√ßa upload desse arquivo abaixo:\")\n",
        "        print(\"\\n\" + \"-\" * 80)\n",
        "        \n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        # Verifica se o arquivo foi enviado\n",
        "        if 'formatted_medical_dataset.json' in uploaded:\n",
        "            print(\"\\n‚úÖ Arquivo enviado com sucesso!\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\n",
        "                \"\\n‚ùå Arquivo 'formatted_medical_dataset.json' n√£o foi encontrado no upload.\\n\"\n",
        "                \"Certifique-se de que:\\n\"\n",
        "                \"  1. Voc√™ executou 'python run_pipeline.py --all' localmente\\n\"\n",
        "                \"  2. O arquivo gerado se chama exatamente 'formatted_medical_dataset.json'\\n\"\n",
        "                \"  3. Voc√™ fez upload do arquivo correto\"\n",
        "            )\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"\\n‚ùå Dataset n√£o encontrado: {FORMATTED_DATASET_PATH}\\n\\n\"\n",
        "            f\"üìã INSTRU√á√ïES:\\n\"\n",
        "            f\"   1. Execute localmente: python run_pipeline.py --all\\n\"\n",
        "            f\"   2. Isso gera: formatted_medical_dataset.json\\n\"\n",
        "            f\"   3. Coloque o arquivo no diret√≥rio atual ou ajuste FORMATTED_DATASET_PATH\"\n",
        "        )\n",
        "else:\n",
        "    print(f\"‚úÖ Arquivo j√° existe: {FORMATTED_DATASET_PATH}\")\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 4.1: CARREGAMENTO DO DATASET FORMATADO\n",
        "# ============================================================================\n",
        "# Carrega o dataset m√©dico j√° formatado no padr√£o Alpaca.\n",
        "# Este arquivo foi gerado pelo pipeline run_pipeline.py --all\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì¶ CARREGANDO DATASET FORMATADO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Arquivo: {FORMATTED_DATASET_PATH}\")\n",
        "\n",
        "# load_dataset do Hugging Face carrega JSON diretamente\n",
        "dataset = load_dataset(\"json\", data_files=str(FORMATTED_DATASET_PATH), split=\"train\")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset carregado: {len(dataset)} exemplos\")\n",
        "print(f\"   Estrutura: {dataset.features}\")\n",
        "\n",
        "# Valida estrutura esperada (formato Alpaca)\n",
        "expected_fields = ['instruction', 'input', 'output']\n",
        "if not all(field in dataset.features for field in expected_fields):\n",
        "    raise ValueError(\n",
        "        f\"‚ùå Dataset n√£o est√° no formato Alpaca esperado!\\n\"\n",
        "        f\"   Campos esperados: {expected_fields}\\n\"\n",
        "        f\"   Campos encontrados: {list(dataset.features.keys())}\\n\"\n",
        "        f\"   Certifique-se de que executou 'python run_pipeline.py --all' corretamente.\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nüìÑ Exemplo de entrada:\")\n",
        "print(f\"   Instruction: {dataset[0]['instruction'][:100]}...\")\n",
        "print(f\"   Input: {dataset[0]['input'][:100]}...\")\n",
        "print(f\"   Output: {dataset[0]['output'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 5: CARREGAMENTO DO MODELO BASE\n",
        "# ============================================================================\n",
        "# Carrega modelo pr√©-quantizado do Unsloth.\n",
        "# Unsloth fornece modelos otimizados que reduzem uso de mem√≥ria em ~75%\n",
        "# mantendo qualidade pr√≥xima ao modelo original.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CARREGANDO MODELO BASE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚è≥ Isso pode levar alguns minutos na primeira execu√ß√£o...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=DEFAULT_MODEL,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo carregado!\")\n",
        "print(f\"   Par√¢metros totais: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 6: CONFIGURA√á√ÉO LoRA\n",
        "# ============================================================================\n",
        "# LoRA (Low-Rank Adaptation) permite treinar apenas ~1-5% dos par√¢metros,\n",
        "# reduzindo drasticamente mem√≥ria e tempo de treinamento.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURANDO LoRA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_CONFIG['r'],\n",
        "    target_modules=LORA_CONFIG['target_modules'],\n",
        "    lora_alpha=LORA_CONFIG['lora_alpha'],\n",
        "    lora_dropout=LORA_CONFIG['lora_dropout'],\n",
        "    bias=LORA_CONFIG['bias'],\n",
        "    use_gradient_checkpointing=LORA_CONFIG['use_gradient_checkpointing'],\n",
        "    random_state=LORA_CONFIG['random_state'],\n",
        "    use_rslora=LORA_CONFIG['use_rslora'],\n",
        "    loftq_config=LORA_CONFIG['loftq_config'],\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"‚úÖ LoRA configurado!\")\n",
        "print(f\"   Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "print(f\"   Par√¢metros totais: {total_params:,}\")\n",
        "print(f\"   Fra√ß√£o trein√°vel: {(trainable_params/total_params)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 7: DEFINI√á√ÉO DO PROMPT ALPACA M√âDICO\n",
        "# ============================================================================\n",
        "# Define a fun√ß√£o que formata exemplos do dataset para o formato Alpaca.\n",
        "# Esta fun√ß√£o ser√° aplicada a cada exemplo durante o treinamento.\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Formata exemplos para o formato Alpaca m√©dico\n",
        "    \n",
        "    Combina instruction, input e output em um √∫nico texto formatado\n",
        "    que o modelo aprender√° a gerar.\n",
        "    \"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    \n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        # Usa template Alpaca m√©dico\n",
        "        text = get_medical_alpaca_prompt(instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    \n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"‚úÖ Fun√ß√£o de formata√ß√£o definida!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 8: PREPARA√á√ÉO DO DATASET PARA TREINAMENTO\n",
        "# ============================================================================\n",
        "# Aplica formata√ß√£o de prompts a todos os exemplos do dataset\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FORMATANDO DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "formatted_dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset formatado: {len(formatted_dataset)} exemplos\")\n",
        "print(f\"   Estrutura: {formatted_dataset.features}\")\n",
        "\n",
        "# Mostra exemplo formatado\n",
        "print(f\"\\nExemplo de texto formatado (primeiros 500 caracteres):\")\n",
        "print(\"-\" * 80)\n",
        "print(formatted_dataset[0]['text'][:500] + \"...\")\n",
        "print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 9: CONFIGURA√á√ÉO DO TRAINER\n",
        "# ============================================================================\n",
        "# Configura o SFTTrainer (Supervised Fine-Tuning Trainer) que gerencia\n",
        "# todo o processo de treinamento.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURANDO TRAINER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=TRAINING_CONFIG['per_device_train_batch_size'],\n",
        "    gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
        "    warmup_steps=TRAINING_CONFIG['warmup_steps'],\n",
        "    max_steps=TRAINING_CONFIG['max_steps'],\n",
        "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=TRAINING_CONFIG['logging_steps'],\n",
        "    optim=TRAINING_CONFIG['optim'],\n",
        "    weight_decay=TRAINING_CONFIG['weight_decay'],\n",
        "    lr_scheduler_type=TRAINING_CONFIG['lr_scheduler_type'],\n",
        "    seed=TRAINING_CONFIG['seed'],\n",
        "    output_dir=str(TRAINING_OUTPUT_DIR),\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=DATASET_CONFIG['dataset_num_proc'],\n",
        "    packing=DATASET_CONFIG['packing'],\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configurado!\")\n",
        "print(f\"   Batch efetivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Max steps: {training_args.max_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 10: TREINAMENTO DO MODELO\n",
        "# ============================================================================\n",
        "# Inicia o processo de treinamento. Este processo pode levar v√°rios minutos\n",
        "# ou horas dependendo do tamanho do dataset e performance da GPU.\n",
        "#\n",
        "# Durante o treinamento, voc√™ ver√° logs mostrando:\n",
        "# - Loss (deve diminuir ao longo do tempo)\n",
        "# - Learning rate atual\n",
        "# - Progresso (steps completados)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INICIANDO TREINAMENTO\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚ö†Ô∏è  Este processo pode levar v√°rios minutos ou horas...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ TREINAMENTO CONCLU√çDO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Loss final: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Steps completados: {trainer_stats.global_step}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 11: TESTE DO MODELO TREINADO\n",
        "# ============================================================================\n",
        "# Testa o modelo com um exemplo m√©dico para verificar a qualidade\n",
        "# das respostas geradas.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTANDO MODELO TREINADO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepara modelo para infer√™ncia\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Exemplo de teste\n",
        "example_instruction = get_instruction_only()\n",
        "example_input = \"\"\"Contexto: Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant produces perforations in its leaves through PCD.\n",
        "Pergunta: Do mitochondria play a role in remodelling plant leaves during programmed cell death?\"\"\"\n",
        "\n",
        "# Formata prompt (sem resposta, queremos que o modelo gere)\n",
        "prompt = get_medical_alpaca_prompt(example_instruction, example_input, \"\")\n",
        "\n",
        "# Tokeniza e gera\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=INFERENCE_CONFIG['max_new_tokens'],\n",
        "    use_cache=INFERENCE_CONFIG['use_cache'],\n",
        "    temperature=INFERENCE_CONFIG['temperature'],\n",
        "    top_p=INFERENCE_CONFIG['top_p'],\n",
        "    top_k=INFERENCE_CONFIG['top_k'],\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(\"Prompt de entrada:\")\n",
        "print(\"-\" * 80)\n",
        "print(prompt[:300] + \"...\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nResposta gerada:\")\n",
        "print(\"-\" * 80)\n",
        "print(generated_text)\n",
        "print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 12: SALVAMENTO DO MODELO\n",
        "# ============================================================================\n",
        "# Salva o modelo treinado (apenas adaptadores LoRA) e o tokenizer.\n",
        "# O modelo salvo pode ser carregado depois para infer√™ncia.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SALVANDO MODELO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "tokenizer.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "\n",
        "print(f\"‚úÖ Modelo salvo em: {MODEL_OUTPUT_DIR}\")\n",
        "print(\"\\nPara carregar o modelo depois, use:\")\n",
        "print(f\"  model, tokenizer = FastLanguageModel.from_pretrained('{MODEL_OUTPUT_DIR}')\")\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 12.1: DOWNLOAD DO MODELO (OPCIONAL - APENAS NO COLAB)\n",
        "# ============================================================================\n",
        "# No Google Colab, voc√™ pode fazer download do modelo treinado.\n",
        "# Descomente as linhas abaixo para fazer download.\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import shutil\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPARANDO DOWNLOAD DO MODELO\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Compacta o modelo\n",
        "    shutil.make_archive('lora_model_medical', 'zip', str(MODEL_OUTPUT_DIR))\n",
        "    \n",
        "    # Faz download\n",
        "    files.download('lora_model_medical.zip')\n",
        "    \n",
        "    print(\"‚úÖ Download iniciado!\")\n",
        "except ImportError:\n",
        "    print(\"\\nüí° Para fazer download no Colab, descomente o c√≥digo acima nesta c√©lula.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 15: MERGE LoRA PARA OLLAMA (OPCIONAL)\n",
        "# ============================================================================\n",
        "# Esta c√©lula faz merge do modelo LoRA treinado com o modelo base e prepara\n",
        "# para uso no Ollama. Isso permite usar o modelo fine-tunado localmente.\n",
        "#\n",
        "# ‚ö†Ô∏è IMPORTANTE:\n",
        "# - Esta c√©lula √© OPCIONAL - s√≥ execute se quiser usar o modelo no Ollama\n",
        "# - Requer bastante mem√≥ria RAM (16GB+ recomendado)\n",
        "# - O processo pode demorar v√°rios minutos\n",
        "# - O modelo merged ser√° salvo para download\n",
        "#\n",
        "# üìã PR√â-REQUISITOS:\n",
        "# 1. Voc√™ deve ter executado as c√©lulas anteriores (treinamento completo)\n",
        "# 2. O modelo LoRA deve estar salvo em MODEL_OUTPUT_DIR\n",
        "# 3. Voc√™ precisa ter acesso ao modelo base (meta-llama/Meta-Llama-3-8B)\n",
        "#\n",
        "# üéØ O QUE ESTA C√âLULA FAZ:\n",
        "# 1. Instala depend√™ncias necess√°rias (peft, transformers, etc.)\n",
        "# 2. Autentica no HuggingFace (se necess√°rio)\n",
        "# 3. Carrega modelo base\n",
        "# 4. Faz merge do LoRA treinado com modelo base\n",
        "# 5. Salva modelo merged\n",
        "# 6. Cria Modelfile para Ollama\n",
        "# 7. Prepara para download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 15: MERGE LoRA PARA OLLAMA\n",
        "# ============================================================================\n",
        "# Faz merge do LoRA treinado com modelo base e prepara para Ollama\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MERGE LoRA PARA OLLAMA\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚ö†Ô∏è  Esta c√©lula √© OPCIONAL - s√≥ execute se quiser usar no Ollama\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES\n",
        "# ============================================================================\n",
        "# Modelo base (deve corresponder ao modelo usado no treinamento)\n",
        "BASE_MODEL_ID = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "# Diret√≥rio de sa√≠da para modelo merged\n",
        "MERGED_OUTPUT_DIR = Path(\"merged_model_for_ollama\")\n",
        "MERGED_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Nome do modelo para Ollama\n",
        "OLLAMA_MODEL_NAME = \"biobyia\"\n",
        "\n",
        "# ============================================================================\n",
        "# VERIFICA√á√ïES INICIAIS\n",
        "# ============================================================================\n",
        "print(\"\\nüìã Verificando pr√©-requisitos...\")\n",
        "\n",
        "# Verifica se modelo LoRA existe\n",
        "if not MODEL_OUTPUT_DIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"\\n‚ùå Modelo LoRA n√£o encontrado: {MODEL_OUTPUT_DIR}\\n\"\n",
        "        f\"   Execute as c√©lulas anteriores primeiro (treinamento completo).\"\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Modelo LoRA encontrado: {MODEL_OUTPUT_DIR}\")\n",
        "\n",
        "# Verifica se est√° no Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úÖ Executando no Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ÑπÔ∏è  Executando localmente\")\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALA√á√ÉO DE DEPEND√äNCIAS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì¶ INSTALANDO DEPEND√äNCIAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    from huggingface_hub import login, HfApi\n",
        "    print(\"‚úÖ Depend√™ncias j√° instaladas\")\n",
        "except ImportError:\n",
        "    print(\"üì• Instalando depend√™ncias...\")\n",
        "    if IN_COLAB:\n",
        "        import subprocess\n",
        "        subprocess.check_call([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "            \"peft\", \"transformers\", \"accelerate\", \"huggingface_hub\"\n",
        "        ])\n",
        "    else:\n",
        "        import subprocess\n",
        "        subprocess.check_call([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "            \"peft\", \"transformers\", \"accelerate\", \"huggingface_hub\"\n",
        "        ])\n",
        "    from peft import PeftModel\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    from huggingface_hub import login, HfApi\n",
        "    print(\"‚úÖ Depend√™ncias instaladas\")\n",
        "\n",
        "# ============================================================================\n",
        "# AUTENTICA√á√ÉO HUGGINGFACE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîê AUTENTICA√á√ÉO HUGGINGFACE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Tenta usar token de vari√°vel de ambiente\n",
        "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\") or os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "if hf_token:\n",
        "    print(\"‚úÖ Token encontrado nas vari√°veis de ambiente\")\n",
        "    login(token=hf_token)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Token n√£o encontrado. Voc√™ ser√° solicitado a fazer login.\")\n",
        "    print(\"üí° Voc√™ pode definir HUGGINGFACE_API_KEY ou HF_TOKEN\")\n",
        "    login()\n",
        "\n",
        "print(\"‚úÖ Autenticado no HuggingFace!\")\n",
        "\n",
        "# ============================================================================\n",
        "# CARREGAR MODELO BASE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì• CARREGANDO MODELO BASE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Modelo: {BASE_MODEL_ID}\")\n",
        "print(\"‚ö†Ô∏è  Isso pode demorar e requer bastante mem√≥ria RAM...\")\n",
        "print(\"‚è≥ Aguarde...\")\n",
        "\n",
        "try:\n",
        "    print(\"üì• Baixando/carregando modelo base...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    base_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Modelo base carregado!\")\n",
        "    print(f\"   Par√¢metros: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Erro ao carregar modelo base: {e}\")\n",
        "    print(f\"\\nüí° Verifique:\")\n",
        "    print(f\"   1. Se voc√™ tem acesso ao modelo: https://huggingface.co/{BASE_MODEL_ID}\")\n",
        "    print(f\"   2. Se voc√™ aceitou as condi√ß√µes do modelo\")\n",
        "    print(f\"   3. Se sua API key tem permiss√£o\")\n",
        "    raise\n",
        "\n",
        "# ============================================================================\n",
        "# CARREGAR E FAZER MERGE DO LoRA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîÑ FAZENDO MERGE DO LoRA COM MODELO BASE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    print(f\"üì• Carregando adaptadores LoRA de: {MODEL_OUTPUT_DIR}\")\n",
        "    \n",
        "    # Carrega LoRA sobre o modelo base\n",
        "    lora_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        str(MODEL_OUTPUT_DIR),\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    \n",
        "    print(\"üîÑ Fazendo merge dos adaptadores...\")\n",
        "    print(\"‚è≥ Isso pode demorar alguns minutos...\")\n",
        "    \n",
        "    # Faz merge\n",
        "    merged_model = lora_model.merge_and_unload()\n",
        "    \n",
        "    print(\"üíæ Salvando modelo merged...\")\n",
        "    merged_model.save_pretrained(\n",
        "        str(MERGED_OUTPUT_DIR),\n",
        "        safe_serialization=True\n",
        "    )\n",
        "    base_tokenizer.save_pretrained(str(MERGED_OUTPUT_DIR))\n",
        "    \n",
        "    print(f\"‚úÖ Modelo merged salvo em: {MERGED_OUTPUT_DIR}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Erro ao fazer merge: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# ============================================================================\n",
        "# CRIAR MODELFILE PARA OLLAMA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìù CRIANDO MODELFILE PARA OLLAMA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "modelfile_path = MERGED_OUTPUT_DIR / \"Modelfile\"\n",
        "\n",
        "modelfile_content = f\"\"\"# Modelfile para {OLLAMA_MODEL_NAME}\n",
        "# Modelo base: {BASE_MODEL_ID}\n",
        "# LoRA treinado: {MODEL_OUTPUT_DIR}\n",
        "\n",
        "FROM {MERGED_OUTPUT_DIR}\n",
        "\n",
        "# Template do sistema\n",
        "SYSTEM \\\"\\\"\\\"Voc√™ √© um assistente m√©dico especializado em question-answering baseado em evid√™ncias cient√≠ficas.\n",
        "Voc√™ responde perguntas m√©dicas baseando-se em contextos fornecidos, sempre citando evid√™ncias.\n",
        "Seja preciso, claro e baseado em evid√™ncias cient√≠ficas.\\\"\\\"\\\"\n",
        "\n",
        "# Par√¢metros\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER top_k 40\n",
        "PARAMETER num_predict 2048\n",
        "\n",
        "# Template de prompt (Alpaca format)\n",
        "TEMPLATE \\\"\\\"\\\"{{{{ if .System }}}}System: {{{{ .System }}}}\n",
        "{{{{ end }}}}{{{{ if .Prompt }}}}Instruction: {{{{ .Prompt }}}}\n",
        "{{{{ end }}}}Response:\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "with open(modelfile_path, 'w') as f:\n",
        "    f.write(modelfile_content)\n",
        "\n",
        "print(f\"‚úÖ Modelfile criado: {modelfile_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PREPARAR PARA DOWNLOAD (COLAB)\n",
        "# ============================================================================\n",
        "if IN_COLAB:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üì¶ PREPARANDO PARA DOWNLOAD\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    import shutil\n",
        "    \n",
        "    # Compacta o modelo merged\n",
        "    zip_path = f\"{OLLAMA_MODEL_NAME}_merged.zip\"\n",
        "    print(f\"üì¶ Compactando modelo merged...\")\n",
        "    shutil.make_archive(OLLAMA_MODEL_NAME + \"_merged\", 'zip', str(MERGED_OUTPUT_DIR))\n",
        "    \n",
        "    print(f\"‚úÖ Arquivo compactado: {zip_path}\")\n",
        "    print(f\"\\nüí° Para fazer download, execute a c√©lula abaixo ou use:\")\n",
        "    print(f\"   files.download('{zip_path}')\")\n",
        "\n",
        "# ============================================================================\n",
        "# INSTRU√á√ïES FINAIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ MERGE CONCLU√çDO COM SUCESSO!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüìã PR√ìXIMOS PASSOS PARA USAR NO OLLAMA:\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"OP√á√ÉO 1: Download e uso local\")\n",
        "print(\"=\" * 80)\n",
        "if IN_COLAB:\n",
        "    print(\"1. Fa√ßa download do arquivo compactado (c√©lula abaixo)\")\n",
        "    print(\"2. Extraia o arquivo no seu computador\")\n",
        "    print(\"3. Siga as instru√ß√µes em: fine_tuning/MERGE_OLLAMA_GUIDE.md\")\n",
        "else:\n",
        "    print(f\"1. Modelo merged salvo em: {MERGED_OUTPUT_DIR}\")\n",
        "    print(\"2. Siga as instru√ß√µes em: fine_tuning/MERGE_OLLAMA_GUIDE.md\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OP√á√ÉO 2: Converter para GGUF (recomendado para Ollama)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. Instale llama.cpp:\")\n",
        "print(\"   git clone https://github.com/ggerganov/llama.cpp\")\n",
        "print(\"   cd llama.cpp && make\")\n",
        "print(f\"\\n2. Converta para GGUF:\")\n",
        "print(f\"   python llama.cpp/convert_hf_to_gguf.py {MERGED_OUTPUT_DIR} --outdir {MERGED_OUTPUT_DIR}/gguf\")\n",
        "print(f\"\\n3. Quantize (opcional):\")\n",
        "print(f\"   ./llama.cpp/quantize {MERGED_OUTPUT_DIR}/gguf/model.gguf {MERGED_OUTPUT_DIR}/gguf/biobyia-q4_0.gguf q4_0\")\n",
        "print(f\"\\n4. Importe no Ollama:\")\n",
        "print(f\"   ollama create {OLLAMA_MODEL_NAME} -f {modelfile_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONFIGURA√á√ÉO NO .ENV\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Adicione ao seu backend/.env:\")\n",
        "print(f\"BIOBYIA_MODEL={OLLAMA_MODEL_NAME}\")\n",
        "print(\"BIOBYIA_BASE_URL=http://localhost:11434\")\n",
        "print(\"LLM_PROVIDER=biobyia\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LOCALIZA√á√ÉO DOS ARQUIVOS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Modelo merged: {MERGED_OUTPUT_DIR}\")\n",
        "print(f\"Modelfile: {modelfile_path}\")\n",
        "if IN_COLAB:\n",
        "    print(f\"Arquivo compactado: {zip_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 16: DOWNLOAD DO MODELO MERGED (APENAS NO COLAB)\n",
        "# ============================================================================\n",
        "# Faz download do modelo merged compactado para uso local\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"üì• DOWNLOAD DO MODELO MERGED\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    zip_path = f\"{OLLAMA_MODEL_NAME}_merged.zip\"\n",
        "    \n",
        "    if Path(zip_path).exists():\n",
        "        print(f\"üì¶ Fazendo download de: {zip_path}\")\n",
        "        print(\"‚è≥ Aguarde...\")\n",
        "        files.download(zip_path)\n",
        "        print(f\"\\n‚úÖ Download conclu√≠do!\")\n",
        "        print(f\"\\nüí° Pr√≥ximos passos:\")\n",
        "        print(f\"   1. Extraia o arquivo {zip_path} no seu computador\")\n",
        "        print(f\"   2. Siga as instru√ß√µes em: fine_tuning/MERGE_OLLAMA_GUIDE.md\")\n",
        "        print(f\"   3. Configure o Ollama e use no backend\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Arquivo n√£o encontrado: {zip_path}\")\n",
        "        print(f\"   Execute a c√©lula anterior primeiro (merge do LoRA)\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è  Esta c√©lula √© apenas para Google Colab\")\n",
        "    print(f\"   No ambiente local, o modelo est√° em: {MERGED_OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 14: UPLOAD PARA HUGGING FACE (OPCIONAL)\n",
        "# ============================================================================\n",
        "# Faz upload dos adaptadores LoRA para o Hugging Face.\n",
        "# Isso permite usar o modelo via Inference API ou baixar depois.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"UPLOAD PARA HUGGING FACE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Instala huggingface_hub se necess√°rio\n",
        "try:\n",
        "    from huggingface_hub import HfApi, login, create_repo\n",
        "    from huggingface_hub.utils import HfHubHTTPError\n",
        "except ImportError:\n",
        "    print(\"üì¶ Instalando huggingface_hub...\")\n",
        "    !pip install huggingface_hub\n",
        "    from huggingface_hub import HfApi, login, create_repo\n",
        "    from huggingface_hub.utils import HfHubHTTPError\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ÉO - AJUSTE AQUI\n",
        "# ============================================================================\n",
        "# Substitua pelos seus dados:\n",
        "HF_REPO_ID = \"seu-usuario/biobyia-medical-lora\"  # Ex: \"joao-silva/biobyia-medical-lora\"\n",
        "HF_TOKEN = None  # Ou coloque seu token aqui: \"hf_xxxxxxxxxxxxx\"\n",
        "PRIVATE_REPO = False  # True para reposit√≥rio privado\n",
        "\n",
        "# ============================================================================\n",
        "# UPLOAD\n",
        "# ============================================================================\n",
        "if HF_REPO_ID == \"seu-usuario/biobyia-medical-lora\":\n",
        "    print(\"‚ö†Ô∏è  ATEN√á√ÉO: Configure HF_REPO_ID antes de fazer upload!\")\n",
        "    print(\"   Edite a vari√°vel HF_REPO_ID nesta c√©lula\")\n",
        "    print(\"   Formato: seu-usuario/nome-do-modelo\")\n",
        "else:\n",
        "    print(f\"üì§ Preparando upload para: {HF_REPO_ID}\")\n",
        "    \n",
        "    # Login\n",
        "    if HF_TOKEN:\n",
        "        login(token=HF_TOKEN)\n",
        "    else:\n",
        "        print(\"üîê Fa√ßa login no Hugging Face:\")\n",
        "        login()\n",
        "    \n",
        "    # Cria reposit√≥rio\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        create_repo(\n",
        "            repo_id=HF_REPO_ID,\n",
        "            repo_type=\"model\",\n",
        "            private=PRIVATE_REPO,\n",
        "            exist_ok=True\n",
        "        )\n",
        "        print(f\"‚úÖ Reposit√≥rio criado/verificado: {HF_REPO_ID}\")\n",
        "    except HfHubHTTPError as e:\n",
        "        if \"already exists\" in str(e).lower():\n",
        "            print(f\"‚úÖ Reposit√≥rio j√° existe: {HF_REPO_ID}\")\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    # Faz upload\n",
        "    print(f\"\\nüì§ Fazendo upload de: {MODEL_OUTPUT_DIR}\")\n",
        "    print(\"   Isso pode levar alguns minutos...\")\n",
        "    \n",
        "    try:\n",
        "        api.upload_folder(\n",
        "            folder_path=str(MODEL_OUTPUT_DIR),\n",
        "            repo_id=HF_REPO_ID,\n",
        "            repo_type=\"model\",\n",
        "            ignore_patterns=[\"*.pt\", \"*.bin\", \"__pycache__\", \"*.pyc\"]\n",
        "        )\n",
        "        print(f\"\\n‚úÖ Upload conclu√≠do com sucesso!\")\n",
        "        print(f\"   Modelo dispon√≠vel em: https://huggingface.co/{HF_REPO_ID}\")\n",
        "        print(f\"\\nüí° Pr√≥ximos passos:\")\n",
        "        print(f\"   1. Acesse: https://huggingface.co/{HF_REPO_ID}\")\n",
        "        print(f\"   2. Teste o modelo na interface do Hugging Face\")\n",
        "        print(f\"   3. Use no backend com HuggingFaceProvider\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erro durante upload: {e}\")\n",
        "        print(f\"\\nüí° Alternativa: Use o script upload_to_huggingface.py localmente\")\n",
        "        print(f\"   python upload_to_huggingface.py --lora_path {MODEL_OUTPUT_DIR} --repo_id {HF_REPO_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 13: CARREGAMENTO E TESTE DO MODELO SALVO\n",
        "# ============================================================================\n",
        "# Demonstra como carregar o modelo salvo e fazer infer√™ncia.\n",
        "# Esta c√©lula √© √∫til para testar o modelo ap√≥s reiniciar o ambiente.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CARREGANDO MODELO SALVO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Carrega modelo salvo\n",
        "loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(MODEL_OUTPUT_DIR),\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(loaded_model)\n",
        "\n",
        "print(\"‚úÖ Modelo carregado com sucesso!\")\n",
        "\n",
        "# Testa com outro exemplo\n",
        "example_instruction = get_instruction_only()\n",
        "example_input = \"\"\"Contexto: Assessment of visual acuity depends on the optotypes used for measurement.\n",
        "Pergunta: What are the differences between Landolt C and Snellen E acuity in strabismus amblyopia?\"\"\"\n",
        "\n",
        "prompt = get_medical_alpaca_prompt(example_instruction, example_input, \"\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = loaded_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Usa TextStreamer para visualizar gera√ß√£o em tempo real\n",
        "text_streamer = TextStreamer(loaded_tokenizer)\n",
        "_ = loaded_model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=INFERENCE_CONFIG['max_new_tokens']\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
