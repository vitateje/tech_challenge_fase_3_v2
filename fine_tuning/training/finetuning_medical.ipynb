{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning de Modelo LLM para Dom√≠nio M√©dico\n",
        "\n",
        "Este notebook implementa o pipeline completo de fine-tuning de um modelo LLM para tarefas de question-answering m√©dico baseado em evid√™ncias cient√≠ficas.\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANTE: Pr√©-requisito ANTES de usar este notebook\n",
        "\n",
        "**Voc√™ PRECISA processar o dataset m√©dico ANTES de usar este notebook!**\n",
        "\n",
        "### Passo 1: Preparar Dataset (FAZER LOCALMENTE, ANTES DO COLAB)\n",
        "\n",
        "No seu computador local, execute:\n",
        "\n",
        "```bash\n",
        "cd fine_tuning\n",
        "python run_pipeline.py --all\n",
        "```\n",
        "\n",
        "Isso ir√°:\n",
        "1. Processar o dataset m√©dico (`ori_pqal.json`)\n",
        "2. Anonimizar dados sens√≠veis\n",
        "3. Formatar no padr√£o Alpaca\n",
        "4. Gerar o arquivo: `formatted_medical_dataset.json`\n",
        "\n",
        "### Passo 2: Usar no Google Colab\n",
        "\n",
        "1. Fa√ßa upload do arquivo `formatted_medical_dataset.json` para o Colab (c√©lula 4)\n",
        "2. Execute todas as c√©lulas sequencialmente\n",
        "3. O modelo treinado ser√° salvo e voc√™ pode fazer download\n",
        "\n",
        "## Objetivos:\n",
        "1. Carregar dataset m√©dico formatado no padr√£o Alpaca\n",
        "2. Carregar modelo base pr√©-quantizado (Unsloth)\n",
        "3. Configurar LoRA para treinamento eficiente\n",
        "4. Treinar modelo com dados m√©dicos\n",
        "5. Testar e salvar modelo treinado\n",
        "\n",
        "## Requisitos:\n",
        "- **Google Colab** (recomendado) ou ambiente com GPU\n",
        "- GPU com pelo menos 8GB VRAM (recomendado 16GB+)\n",
        "- Dataset formatado: `formatted_medical_dataset.json` (gerado pelo `run_pipeline.py`)\n",
        "\n",
        "## Ordem de Execu√ß√£o:\n",
        "Execute as c√©lulas **sequencialmente** (de cima para baixo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 1: INSTALA√á√ÉO DE DEPEND√äNCIAS (OBRIGAT√ìRIO NO COLAB)\n",
        "# ============================================================================\n",
        "# Esta c√©lula instala todas as bibliotecas necess√°rias para o fine-tuning.\n",
        "# Execute esta c√©lula primeiro se estiver usando Google Colab.\n",
        "\n",
        "print(\"üì¶ Instalando depend√™ncias...\")\n",
        "print(\"‚è≥ Isso pode levar alguns minutos na primeira execu√ß√£o...\")\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets\n",
        "\n",
        "print(\"\\n‚úÖ Depend√™ncias instaladas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2: IMPORTA√á√ïES DE BIBLIOTECAS\n",
        "# ============================================================================\n",
        "# Importa todas as bibliotecas necess√°rias para o fine-tuning\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  GPU n√£o detectada! Fine-tuning ser√° muito lento em CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 3: CONFIGURA√á√ïES COMPLETAS (TUDO INLINE)\n",
        "# ============================================================================\n",
        "# Todas as configura√ß√µes est√£o definidas aqui - n√£o precisa de arquivos externos\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES DO MODELO BASE\n",
        "# ============================================================================\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "DTYPE = None  # Auto-detect\n",
        "LOAD_IN_4BIT = True\n",
        "DEFAULT_MODEL = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES LoRA\n",
        "# ============================================================================\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 16,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0,\n",
        "    \"bias\": \"none\",\n",
        "    \"target_modules\": [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    \"use_gradient_checkpointing\": \"unsloth\",\n",
        "    \"random_state\": 3407,\n",
        "    \"use_rslora\": False,\n",
        "    \"loftq_config\": None,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# HIPERPAR√ÇMETROS DE TREINAMENTO\n",
        "# ============================================================================\n",
        "TRAINING_CONFIG = {\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"warmup_steps\": 5,\n",
        "    \"max_steps\": 100,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"optim\": \"adamw_8bit\",\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"lr_scheduler_type\": \"linear\",\n",
        "    \"seed\": 3407,\n",
        "    \"output_dir\": \"outputs\",\n",
        "    \"logging_steps\": 1,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES DE DATASET\n",
        "# ============================================================================\n",
        "DATASET_CONFIG = {\n",
        "    \"dataset_num_proc\": 2,\n",
        "    \"packing\": False,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ïES DE INFER√äNCIA\n",
        "# ============================================================================\n",
        "INFERENCE_CONFIG = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"use_cache\": True,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.9,\n",
        "    \"top_k\": 50,\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# FUN√á√ïES DE PROMPTS (INLINE)\n",
        "# ============================================================================\n",
        "MEDICAL_ALPACA_PROMPT = \"\"\"Below is a medical instruction that describes a task, paired with medical context and a question. Write a response that appropriately completes the request based on the provided medical evidence.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def get_medical_alpaca_prompt(instruction: str, input_text: str, response: str = \"\") -> str:\n",
        "    \"\"\"Formata um prompt m√©dico usando o template Alpaca\"\"\"\n",
        "    return MEDICAL_ALPACA_PROMPT.format(instruction, input_text, response)\n",
        "\n",
        "def get_instruction_only() -> str:\n",
        "    \"\"\"Retorna apenas a instru√ß√£o padr√£o para tarefas m√©dicas\"\"\"\n",
        "    return \"Responda √† pergunta baseando-se nos contextos fornecidos.\"\n",
        "\n",
        "# ============================================================================\n",
        "# CAMINHOS (AJUSTE PARA COLAB)\n",
        "# ============================================================================\n",
        "# No Colab, voc√™ pode:\n",
        "# 1. Fazer upload do arquivo diretamente (use o caminho abaixo)\n",
        "# 2. Montar Google Drive e usar caminho do Drive\n",
        "# 3. Usar caminho absoluto do arquivo\n",
        "\n",
        "FORMATTED_DATASET_PATH = Path(\"formatted_medical_dataset.json\")  # Arquivo na raiz do Colab\n",
        "MODEL_OUTPUT_DIR = Path(\"lora_model_medical\")\n",
        "TRAINING_OUTPUT_DIR = Path(\"outputs\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURA√á√ïES DE FINE-TUNING\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Modelo: {DEFAULT_MODEL}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"LoRA rank: {LORA_CONFIG['r']}\")\n",
        "print(f\"Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
        "print(f\"Max steps: {TRAINING_CONFIG['max_steps']}\")\n",
        "print(f\"Dataset: {FORMATTED_DATASET_PATH}\")\n",
        "print(f\"Output model: {MODEL_OUTPUT_DIR}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4: UPLOAD DO DATASET FORMATADO\n",
        "# ============================================================================\n",
        "# ‚ö†Ô∏è IMPORTANTE: Este notebook espera que voc√™ j√° tenha processado o dataset!\n",
        "#\n",
        "# ANTES de usar este notebook, voc√™ DEVE ter executado localmente:\n",
        "#   cd fine_tuning\n",
        "#   python run_pipeline.py --all\n",
        "#\n",
        "# Isso gera o arquivo: formatted_medical_dataset.json\n",
        "#\n",
        "# Agora, fa√ßa upload desse arquivo para o Colab:\n",
        "\n",
        "# Verifica se est√° no Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Verifica se o arquivo j√° existe\n",
        "if not FORMATTED_DATASET_PATH.exists():\n",
        "    if IN_COLAB:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"üì§ UPLOAD DO DATASET FORMATADO\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"\\n‚ö†Ô∏è  Arquivo n√£o encontrado!\")\n",
        "        print(\"\\nüìã INSTRU√á√ïES:\")\n",
        "        print(\"   1. Voc√™ deve ter processado o dataset ANTES de usar este notebook\")\n",
        "        print(\"   2. Execute localmente: python run_pipeline.py --all\")\n",
        "        print(\"   3. Isso gera: formatted_medical_dataset.json\")\n",
        "        print(\"   4. Agora fa√ßa upload desse arquivo abaixo:\")\n",
        "        print(\"\\n\" + \"-\" * 80)\n",
        "        \n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        # Verifica se o arquivo foi enviado\n",
        "        if 'formatted_medical_dataset.json' in uploaded:\n",
        "            print(\"\\n‚úÖ Arquivo enviado com sucesso!\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\n",
        "                \"\\n‚ùå Arquivo 'formatted_medical_dataset.json' n√£o foi encontrado no upload.\\n\"\n",
        "                \"Certifique-se de que:\\n\"\n",
        "                \"  1. Voc√™ executou 'python run_pipeline.py --all' localmente\\n\"\n",
        "                \"  2. O arquivo gerado se chama exatamente 'formatted_medical_dataset.json'\\n\"\n",
        "                \"  3. Voc√™ fez upload do arquivo correto\"\n",
        "            )\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"\\n‚ùå Dataset n√£o encontrado: {FORMATTED_DATASET_PATH}\\n\\n\"\n",
        "            f\"üìã INSTRU√á√ïES:\\n\"\n",
        "            f\"   1. Execute localmente: python run_pipeline.py --all\\n\"\n",
        "            f\"   2. Isso gera: formatted_medical_dataset.json\\n\"\n",
        "            f\"   3. Coloque o arquivo no diret√≥rio atual ou ajuste FORMATTED_DATASET_PATH\"\n",
        "        )\n",
        "else:\n",
        "    print(f\"‚úÖ Arquivo j√° existe: {FORMATTED_DATASET_PATH}\")\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 4.1: CARREGAMENTO DO DATASET FORMATADO\n",
        "# ============================================================================\n",
        "# Carrega o dataset m√©dico j√° formatado no padr√£o Alpaca.\n",
        "# Este arquivo foi gerado pelo pipeline run_pipeline.py --all\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì¶ CARREGANDO DATASET FORMATADO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Arquivo: {FORMATTED_DATASET_PATH}\")\n",
        "\n",
        "# load_dataset do Hugging Face carrega JSON diretamente\n",
        "dataset = load_dataset(\"json\", data_files=str(FORMATTED_DATASET_PATH), split=\"train\")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset carregado: {len(dataset)} exemplos\")\n",
        "print(f\"   Estrutura: {dataset.features}\")\n",
        "\n",
        "# Valida estrutura esperada (formato Alpaca)\n",
        "expected_fields = ['instruction', 'input', 'output']\n",
        "if not all(field in dataset.features for field in expected_fields):\n",
        "    raise ValueError(\n",
        "        f\"‚ùå Dataset n√£o est√° no formato Alpaca esperado!\\n\"\n",
        "        f\"   Campos esperados: {expected_fields}\\n\"\n",
        "        f\"   Campos encontrados: {list(dataset.features.keys())}\\n\"\n",
        "        f\"   Certifique-se de que executou 'python run_pipeline.py --all' corretamente.\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nüìÑ Exemplo de entrada:\")\n",
        "print(f\"   Instruction: {dataset[0]['instruction'][:100]}...\")\n",
        "print(f\"   Input: {dataset[0]['input'][:100]}...\")\n",
        "print(f\"   Output: {dataset[0]['output'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 5: CARREGAMENTO DO MODELO BASE\n",
        "# ============================================================================\n",
        "# Carrega modelo pr√©-quantizado do Unsloth.\n",
        "# Unsloth fornece modelos otimizados que reduzem uso de mem√≥ria em ~75%\n",
        "# mantendo qualidade pr√≥xima ao modelo original.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CARREGANDO MODELO BASE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚è≥ Isso pode levar alguns minutos na primeira execu√ß√£o...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=DEFAULT_MODEL,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo carregado!\")\n",
        "print(f\"   Par√¢metros totais: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 6: CONFIGURA√á√ÉO LoRA\n",
        "# ============================================================================\n",
        "# LoRA (Low-Rank Adaptation) permite treinar apenas ~1-5% dos par√¢metros,\n",
        "# reduzindo drasticamente mem√≥ria e tempo de treinamento.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURANDO LoRA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_CONFIG['r'],\n",
        "    target_modules=LORA_CONFIG['target_modules'],\n",
        "    lora_alpha=LORA_CONFIG['lora_alpha'],\n",
        "    lora_dropout=LORA_CONFIG['lora_dropout'],\n",
        "    bias=LORA_CONFIG['bias'],\n",
        "    use_gradient_checkpointing=LORA_CONFIG['use_gradient_checkpointing'],\n",
        "    random_state=LORA_CONFIG['random_state'],\n",
        "    use_rslora=LORA_CONFIG['use_rslora'],\n",
        "    loftq_config=LORA_CONFIG['loftq_config'],\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"‚úÖ LoRA configurado!\")\n",
        "print(f\"   Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "print(f\"   Par√¢metros totais: {total_params:,}\")\n",
        "print(f\"   Fra√ß√£o trein√°vel: {(trainable_params/total_params)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 7: DEFINI√á√ÉO DO PROMPT ALPACA M√âDICO\n",
        "# ============================================================================\n",
        "# Define a fun√ß√£o que formata exemplos do dataset para o formato Alpaca.\n",
        "# Esta fun√ß√£o ser√° aplicada a cada exemplo durante o treinamento.\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Formata exemplos para o formato Alpaca m√©dico\n",
        "    \n",
        "    Combina instruction, input e output em um √∫nico texto formatado\n",
        "    que o modelo aprender√° a gerar.\n",
        "    \"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    \n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        # Usa template Alpaca m√©dico\n",
        "        text = get_medical_alpaca_prompt(instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    \n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"‚úÖ Fun√ß√£o de formata√ß√£o definida!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 8: PREPARA√á√ÉO DO DATASET PARA TREINAMENTO\n",
        "# ============================================================================\n",
        "# Aplica formata√ß√£o de prompts a todos os exemplos do dataset\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FORMATANDO DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "formatted_dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset formatado: {len(formatted_dataset)} exemplos\")\n",
        "print(f\"   Estrutura: {formatted_dataset.features}\")\n",
        "\n",
        "# Mostra exemplo formatado\n",
        "print(f\"\\nExemplo de texto formatado (primeiros 500 caracteres):\")\n",
        "print(\"-\" * 80)\n",
        "print(formatted_dataset[0]['text'][:500] + \"...\")\n",
        "print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 9: CONFIGURA√á√ÉO DO TRAINER\n",
        "# ============================================================================\n",
        "# Configura o SFTTrainer (Supervised Fine-Tuning Trainer) que gerencia\n",
        "# todo o processo de treinamento.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONFIGURANDO TRAINER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=TRAINING_CONFIG['per_device_train_batch_size'],\n",
        "    gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
        "    warmup_steps=TRAINING_CONFIG['warmup_steps'],\n",
        "    max_steps=TRAINING_CONFIG['max_steps'],\n",
        "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=TRAINING_CONFIG['logging_steps'],\n",
        "    optim=TRAINING_CONFIG['optim'],\n",
        "    weight_decay=TRAINING_CONFIG['weight_decay'],\n",
        "    lr_scheduler_type=TRAINING_CONFIG['lr_scheduler_type'],\n",
        "    seed=TRAINING_CONFIG['seed'],\n",
        "    output_dir=str(TRAINING_OUTPUT_DIR),\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=DATASET_CONFIG['dataset_num_proc'],\n",
        "    packing=DATASET_CONFIG['packing'],\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configurado!\")\n",
        "print(f\"   Batch efetivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Max steps: {training_args.max_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 10: TREINAMENTO DO MODELO\n",
        "# ============================================================================\n",
        "# Inicia o processo de treinamento. Este processo pode levar v√°rios minutos\n",
        "# ou horas dependendo do tamanho do dataset e performance da GPU.\n",
        "#\n",
        "# Durante o treinamento, voc√™ ver√° logs mostrando:\n",
        "# - Loss (deve diminuir ao longo do tempo)\n",
        "# - Learning rate atual\n",
        "# - Progresso (steps completados)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INICIANDO TREINAMENTO\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚ö†Ô∏è  Este processo pode levar v√°rios minutos ou horas...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ TREINAMENTO CONCLU√çDO\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Loss final: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"Steps completados: {trainer_stats.global_step}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 11: TESTE DO MODELO TREINADO\n",
        "# ============================================================================\n",
        "# Testa o modelo com um exemplo m√©dico para verificar a qualidade\n",
        "# das respostas geradas.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTANDO MODELO TREINADO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepara modelo para infer√™ncia\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Exemplo de teste\n",
        "example_instruction = get_instruction_only()\n",
        "example_input = \"\"\"Contexto: Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant produces perforations in its leaves through PCD.\n",
        "Pergunta: Do mitochondria play a role in remodelling plant leaves during programmed cell death?\"\"\"\n",
        "\n",
        "# Formata prompt (sem resposta, queremos que o modelo gere)\n",
        "prompt = get_medical_alpaca_prompt(example_instruction, example_input, \"\")\n",
        "\n",
        "# Tokeniza e gera\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=INFERENCE_CONFIG['max_new_tokens'],\n",
        "    use_cache=INFERENCE_CONFIG['use_cache'],\n",
        "    temperature=INFERENCE_CONFIG['temperature'],\n",
        "    top_p=INFERENCE_CONFIG['top_p'],\n",
        "    top_k=INFERENCE_CONFIG['top_k'],\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "print(\"Prompt de entrada:\")\n",
        "print(\"-\" * 80)\n",
        "print(prompt[:300] + \"...\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nResposta gerada:\")\n",
        "print(\"-\" * 80)\n",
        "print(generated_text)\n",
        "print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 12: SALVAMENTO DO MODELO\n",
        "# ============================================================================\n",
        "# Salva o modelo treinado (apenas adaptadores LoRA) e o tokenizer.\n",
        "# O modelo salvo pode ser carregado depois para infer√™ncia.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SALVANDO MODELO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "tokenizer.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "\n",
        "print(f\"‚úÖ Modelo salvo em: {MODEL_OUTPUT_DIR}\")\n",
        "print(\"\\nPara carregar o modelo depois, use:\")\n",
        "print(f\"  model, tokenizer = FastLanguageModel.from_pretrained('{MODEL_OUTPUT_DIR}')\")\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 12.1: DOWNLOAD DO MODELO (OPCIONAL - APENAS NO COLAB)\n",
        "# ============================================================================\n",
        "# No Google Colab, voc√™ pode fazer download do modelo treinado.\n",
        "# Descomente as linhas abaixo para fazer download.\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import shutil\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPARANDO DOWNLOAD DO MODELO\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Compacta o modelo\n",
        "    shutil.make_archive('lora_model_medical', 'zip', str(MODEL_OUTPUT_DIR))\n",
        "    \n",
        "    # Faz download\n",
        "    files.download('lora_model_medical.zip')\n",
        "    \n",
        "    print(\"‚úÖ Download iniciado!\")\n",
        "except ImportError:\n",
        "    print(\"\\nüí° Para fazer download no Colab, descomente o c√≥digo acima nesta c√©lula.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 14: UPLOAD PARA HUGGING FACE (OPCIONAL)\n",
        "# ============================================================================\n",
        "# Faz upload dos adaptadores LoRA para o Hugging Face.\n",
        "# Isso permite usar o modelo via Inference API ou baixar depois.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"UPLOAD PARA HUGGING FACE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Instala huggingface_hub se necess√°rio\n",
        "try:\n",
        "    from huggingface_hub import HfApi, login, create_repo\n",
        "    from huggingface_hub.utils import HfHubHTTPError\n",
        "except ImportError:\n",
        "    print(\"üì¶ Instalando huggingface_hub...\")\n",
        "    !pip install huggingface_hub\n",
        "    from huggingface_hub import HfApi, login, create_repo\n",
        "    from huggingface_hub.utils import HfHubHTTPError\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURA√á√ÉO - AJUSTE AQUI\n",
        "# ============================================================================\n",
        "# Substitua pelos seus dados:\n",
        "HF_REPO_ID = \"seu-usuario/biobyia-medical-lora\"  # Ex: \"joao-silva/biobyia-medical-lora\"\n",
        "HF_TOKEN = None  # Ou coloque seu token aqui: \"hf_xxxxxxxxxxxxx\"\n",
        "PRIVATE_REPO = False  # True para reposit√≥rio privado\n",
        "\n",
        "# ============================================================================\n",
        "# UPLOAD\n",
        "# ============================================================================\n",
        "if HF_REPO_ID == \"seu-usuario/biobyia-medical-lora\":\n",
        "    print(\"‚ö†Ô∏è  ATEN√á√ÉO: Configure HF_REPO_ID antes de fazer upload!\")\n",
        "    print(\"   Edite a vari√°vel HF_REPO_ID nesta c√©lula\")\n",
        "    print(\"   Formato: seu-usuario/nome-do-modelo\")\n",
        "else:\n",
        "    print(f\"üì§ Preparando upload para: {HF_REPO_ID}\")\n",
        "    \n",
        "    # Login\n",
        "    if HF_TOKEN:\n",
        "        login(token=HF_TOKEN)\n",
        "    else:\n",
        "        print(\"üîê Fa√ßa login no Hugging Face:\")\n",
        "        login()\n",
        "    \n",
        "    # Cria reposit√≥rio\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        create_repo(\n",
        "            repo_id=HF_REPO_ID,\n",
        "            repo_type=\"model\",\n",
        "            private=PRIVATE_REPO,\n",
        "            exist_ok=True\n",
        "        )\n",
        "        print(f\"‚úÖ Reposit√≥rio criado/verificado: {HF_REPO_ID}\")\n",
        "    except HfHubHTTPError as e:\n",
        "        if \"already exists\" in str(e).lower():\n",
        "            print(f\"‚úÖ Reposit√≥rio j√° existe: {HF_REPO_ID}\")\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    # Faz upload\n",
        "    print(f\"\\nüì§ Fazendo upload de: {MODEL_OUTPUT_DIR}\")\n",
        "    print(\"   Isso pode levar alguns minutos...\")\n",
        "    \n",
        "    try:\n",
        "        api.upload_folder(\n",
        "            folder_path=str(MODEL_OUTPUT_DIR),\n",
        "            repo_id=HF_REPO_ID,\n",
        "            repo_type=\"model\",\n",
        "            ignore_patterns=[\"*.pt\", \"*.bin\", \"__pycache__\", \"*.pyc\"]\n",
        "        )\n",
        "        print(f\"\\n‚úÖ Upload conclu√≠do com sucesso!\")\n",
        "        print(f\"   Modelo dispon√≠vel em: https://huggingface.co/{HF_REPO_ID}\")\n",
        "        print(f\"\\nüí° Pr√≥ximos passos:\")\n",
        "        print(f\"   1. Acesse: https://huggingface.co/{HF_REPO_ID}\")\n",
        "        print(f\"   2. Teste o modelo na interface do Hugging Face\")\n",
        "        print(f\"   3. Use no backend com HuggingFaceProvider\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erro durante upload: {e}\")\n",
        "        print(f\"\\nüí° Alternativa: Use o script upload_to_huggingface.py localmente\")\n",
        "        print(f\"   python upload_to_huggingface.py --lora_path {MODEL_OUTPUT_DIR} --repo_id {HF_REPO_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 13: CARREGAMENTO E TESTE DO MODELO SALVO\n",
        "# ============================================================================\n",
        "# Demonstra como carregar o modelo salvo e fazer infer√™ncia.\n",
        "# Esta c√©lula √© √∫til para testar o modelo ap√≥s reiniciar o ambiente.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CARREGANDO MODELO SALVO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Carrega modelo salvo\n",
        "loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(MODEL_OUTPUT_DIR),\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(loaded_model)\n",
        "\n",
        "print(\"‚úÖ Modelo carregado com sucesso!\")\n",
        "\n",
        "# Testa com outro exemplo\n",
        "example_instruction = get_instruction_only()\n",
        "example_input = \"\"\"Contexto: Assessment of visual acuity depends on the optotypes used for measurement.\n",
        "Pergunta: What are the differences between Landolt C and Snellen E acuity in strabismus amblyopia?\"\"\"\n",
        "\n",
        "prompt = get_medical_alpaca_prompt(example_instruction, example_input, \"\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "inputs = loaded_tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Usa TextStreamer para visualizar gera√ß√£o em tempo real\n",
        "text_streamer = TextStreamer(loaded_tokenizer)\n",
        "_ = loaded_model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=INFERENCE_CONFIG['max_new_tokens']\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
