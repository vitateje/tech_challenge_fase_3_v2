{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning de Modelo LLM para Dom√≠nio M√©dico (ChatML)\n",
        "\n",
        "Este notebook implementa o pipeline completo de fine-tuning de um modelo LLM para tarefas de question-answering m√©dico usando o formato **ChatML**.\n",
        "\n",
        "## ‚ö†Ô∏è Op√ß√µes de Pr√©-processamento\n",
        "\n",
        "Voc√™ pode usar este notebook de duas maneiras:\n",
        "1. **Upload do arquivo j√° processado**: Fa√ßa upload de `formatted_medical_dataset.jsonl` (se j√° rodou o pipeline localmente).\n",
        "2. **Upload do arquivo bruto**: Fa√ßa upload de `ori_pqal.json` e o notebook processar√° os dados para voc√™.\n",
        "\n",
        "## Objetivos:\n",
        "1. Carregar/Processar dataset m√©dico no padr√£o ChatML\n",
        "2. Carregar modelo base Meta-Llama-3-8B-Instruct pre-quantizado\n",
        "3. Configurar LoRA (QLoRA) para treinamento eficiente\n",
        "4. Treinar modelo com dados m√©dicos\n",
        "5. Testar e salvar modelo treinado\n",
        "\n",
        "## Requisitos:\n",
        "- **Google Colab** com GPU\n",
        "- GPU com pelo menos 8GB VRAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 1: INSTALA√á√ÉO DE DEPEND√äNCIAS\n",
        "print(\"üì¶ Instalando depend√™ncias...\")\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets\n",
        "print(\"\\n‚úÖ Depend√™ncias instaladas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 2: IMPORTA√á√ïES\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 3: CONFIGURA√á√ïES\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "DTYPE = None  # Auto-detect\n",
        "LOAD_IN_4BIT = True\n",
        "DEFAULT_MODEL = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 16,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0,\n",
        "    \"bias\": \"none\",\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    \"use_gradient_checkpointing\": \"unsloth\",\n",
        "    \"random_state\": 3407,\n",
        "    \"use_rslora\": False,\n",
        "}\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"warmup_steps\": 5,\n",
        "    \"max_steps\": 100,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"optim\": \"adamw_8bit\",\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"lr_scheduler_type\": \"linear\",\n",
        "    \"seed\": 3407,\n",
        "    \"output_dir\": \"outputs\",\n",
        "    \"logging_steps\": 1,\n",
        "}\n",
        "\n",
        "print(f\"Configurado para usar modelo: {DEFAULT_MODEL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 4: CARREGAMENTO E PR√â-PROCESSAMENTO\n",
        "FORMATTED_DATASET_PATH = Path(\"formatted_medical_dataset.jsonl\")\n",
        "RAW_DATASET_PATH = Path(\"ori_pqal.json\")\n",
        "\n",
        "def anonymize_text(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{4}', '[DATA]', text)\n",
        "    text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '[DATA]', text)\n",
        "    text = re.sub(r'ID:\\s*\\d+', 'ID: [PACIENTE_ID]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Patient ID:\\s*\\d+', 'Patient ID: [PACIENTE_ID]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\d{3}[-.]?\\d{3}[-.]?\\d{4}', '[TELEFONE]', text)\n",
        "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
        "    return text\n",
        "\n",
        "def process_raw_to_chatml(input_path, output_path):\n",
        "    print(f\"‚è≥ Processando arquivo bruto: {input_path}...\")\n",
        "    with open(input_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = json.load(f)\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        for data_id, content in raw_data.items():\n",
        "            question = content.get(\"QUESTION\", \"\")\n",
        "            context = \" \".join(content.get(\"CONTEXTS\", []))\n",
        "            answer = content.get(\"LONG_ANSWER\", \"\")\n",
        "            \n",
        "            input_text = f\"Contexto: {anonymize_text(context)}\\nPergunta: {question}\"\n",
        "            \n",
        "            chatml_entry = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"Responda √† pergunta baseando-se nos contextos fornecidos.\"},\n",
        "                    {\"role\": \"user\", \"content\": input_text},\n",
        "                    {\"role\": \"assistant\", \"content\": anonymize_text(answer)}\n",
        "                ]\n",
        "            }\n",
        "            f_out.write(json.dumps(chatml_entry, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"‚úÖ Arquivo ChatML gerado: {output_path}\")\n",
        "\n",
        "# Verifica arquivos\n",
        "if not FORMATTED_DATASET_PATH.exists():\n",
        "    if RAW_DATASET_PATH.exists():\n",
        "        process_raw_to_chatml(RAW_DATASET_PATH, FORMATTED_DATASET_PATH)\n",
        "    else:\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(\"üì§ Arquivo n√£o encontrado. Fa√ßa upload de 'ori_pqal.json' ou 'formatted_medical_dataset.jsonl':\")\n",
        "            uploaded = files.upload()\n",
        "            # Checa o que foi subido\n",
        "            if \"ori_pqal.json\" in uploaded:\n",
        "                process_raw_to_chatml(\"ori_pqal.json\", FORMATTED_DATASET_PATH)\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  Erro: Arquivos de dataset n√£o encontrados.\")\n",
        "\n",
        "if FORMATTED_DATASET_PATH.exists():\n",
        "    dataset = load_dataset(\"json\", data_files=str(FORMATTED_DATASET_PATH), split=\"train\")\n",
        "    print(f\"\\n‚úÖ Dataset pronto: {len(dataset)} exemplos\")\n",
        "    print(f\"üìÑ Exemplo: {dataset[0]['messages']}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Dataset n√£o encontrado para carregar.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 5: CARREGAMENTO DO MODELO BASE\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=DEFAULT_MODEL,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo base carregado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 6: CONFIGURA√á√ÉO LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    **LORA_CONFIG\n",
        ")\n",
        "print(\"‚úÖ LoRA configurado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 7: FORMATA√á√ÉO CHATML\n",
        "from unsloth import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"chatml\",\n",
        "    mapping={\"role\": \"role\", \"content\": \"content\", \"user\": \"user\", \"assistant\": \"assistant\"},\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "print(\"‚úÖ Dataset formatado com ChatML template!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 8: TREINAMENTO\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=TrainingArguments(\n",
        "        **TRAINING_CONFIG,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"‚úÖ Treinamento conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 9: TESTE R√ÅPIDO\n",
        "FastLanguageModel.for_inference(model)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente m√©dico prestativo.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Qual a import√¢ncia da vitamina D para os ossos?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=256)\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√âLULA 10: SALVAMENTO\n",
        "MODEL_OUTPUT_DIR = Path(\"lora_model_medical\")\n",
        "model.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "tokenizer.save_pretrained(str(MODEL_OUTPUT_DIR))\n",
        "print(f\"‚úÖ Modelo salvo em: {MODEL_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# MODELO DE UTILIZA√á√ÉO (INFER√äNCIA)\n",
        "# ============================================================================\n",
        "Este formato √© ideal para rodar o modelo em GPUs dom√©sticas para teste r√°pido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# 1. Configura√ß√£o de Quantiza√ß√£o (Para caber na GPU dom√©stica)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 2. Carregar o Modelo BASE\n",
        "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# 3. APLICAR O SEU FINE-TUNING (Sem Merge!)\n",
        "# adapter_id = \"vitateje/biobyia\"\n",
        "adapter_id = \"../lora_model_medical\" \n",
        "print(f\"Injetando conhecimento m√©dico de: {adapter_id}\")\n",
        "model = PeftModel.from_pretrained(model, adapter_id)\n",
        "\n",
        "# 4. Teste R√°pido\n",
        "text = \"Contexto: Paciente com dores agudas... Pergunta: Qual o procedimento?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "print(\"\\nRESPOSTA DO AGENTE:\\n\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
