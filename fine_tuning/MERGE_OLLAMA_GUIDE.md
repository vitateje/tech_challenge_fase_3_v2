# üîÑ Guia: Merge LoRA para Ollama

Este guia explica como fazer merge do modelo LoRA (`vitateje/biobyia`) com o modelo base e configur√°-lo para uso no Ollama.

## üìã Pr√©-requisitos

### 1. Instalar Depend√™ncias

```bash
cd fine_tuning
pip install transformers peft accelerate huggingface_hub torch
```

### 2. Autenticar no HuggingFace

```bash
# Op√ß√£o 1: Via CLI
hf auth login

# Op√ß√£o 2: Via vari√°vel de ambiente
export HUGGINGFACE_API_KEY="seu_token_aqui"
# ou adicione ao .env:
# HUGGINGFACE_API_KEY=seu_token_aqui
```

### 3. Aceitar Condi√ß√µes do Modelo

1. Acesse: https://huggingface.co/vitateje/biobyia
2. Fa√ßa login
3. Aceite as condi√ß√µes do modelo

### 4. Verificar Acesso ao Modelo Base

O modelo base √© `meta-llama/Meta-Llama-3-8B`. Verifique se voc√™ tem acesso:
- https://huggingface.co/meta-llama/Meta-Llama-3-8B
- Se necess√°rio, solicite acesso no HuggingFace

## üöÄ Executar Merge

### Op√ß√£o 1: Script Automatizado (Recomendado)

```bash
cd fine_tuning
python merge_lora_for_ollama.py
```

O script ir√°:
1. ‚úÖ Verificar depend√™ncias
2. ‚úÖ Autenticar no HuggingFace
3. ‚úÖ Baixar modelo LoRA
4. ‚úÖ Carregar modelo base
5. ‚úÖ Fazer merge do LoRA com base
6. ‚úÖ Criar Modelfile para Ollama
7. ‚úÖ Gerar instru√ß√µes finais

### Op√ß√£o 2: Manual (Passo a Passo)

Se preferir fazer manualmente, siga os passos abaixo.

## üì¶ Passo a Passo Manual

### 1. Baixar Modelo LoRA

```python
from huggingface_hub import login, snapshot_download

# Login
login()  # ou use token: login(token="seu_token")

# Baixar LoRA
snapshot_download(
    repo_id="vitateje/biobyia",
    local_dir="./outputs/lora_model"
)
```

### 2. Carregar Modelo Base

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")
```

### 3. Fazer Merge

```python
from peft import PeftModel

# Carregar LoRA
lora_model = PeftModel.from_pretrained(
    model,
    "./outputs/lora_model",
    torch_dtype=torch.float16
)

# Fazer merge
merged_model = lora_model.merge_and_unload()

# Salvar modelo merged
merged_model.save_pretrained("./outputs/merged_model")
tokenizer.save_pretrained("./outputs/merged_model")
```

## ü¶ô Configurar Ollama

### Op√ß√£o A: Usar Modelo Merged Diretamente

Se o Ollama suportar modelos HuggingFace diretamente:

```bash
# Criar modelo no Ollama
ollama create biobyia -f outputs/merged_model/Modelfile

# Testar
ollama run biobyia "Qual √© o papel das mitoc√¥ndrias na apoptose?"
```

### Op√ß√£o B: Converter para GGUF (Recomendado)

#### 1. Instalar llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

#### 2. Converter para GGUF

```bash
# Instalar depend√™ncias do conversor
pip install -r llama.cpp/requirements.txt

# Converter
python llama.cpp/convert_hf_to_gguf.py \
    fine_tuning/outputs/merged_model \
    --outdir fine_tuning/outputs/gguf \
    --outtype f16
```

#### 3. Quantizar (Opcional - Reduz Tamanho)

```bash
cd llama.cpp

# Quantiza√ß√£o Q4_0 (recomendado - boa qualidade/tamanho)
./quantize \
    ../fine_tuning/outputs/gguf/model.gguf \
    ../fine_tuning/outputs/gguf/biobyia-q4_0.gguf \
    q4_0

# Quantiza√ß√£o Q8_0 (melhor qualidade, maior tamanho)
./quantize \
    ../fine_tuning/outputs/gguf/model.gguf \
    ../fine_tuning/outputs/gguf/biobyia-q8_0.gguf \
    q8_0
```

#### 4. Criar Modelfile

Crie `fine_tuning/outputs/gguf/Modelfile`:

```dockerfile
FROM ./biobyia-q4_0.gguf

SYSTEM """Voc√™ √© um assistente m√©dico especializado em question-answering baseado em evid√™ncias cient√≠ficas.
Voc√™ responde perguntas m√©dicas baseando-se em contextos fornecidos, sempre citando evid√™ncias.
Seja preciso, claro e baseado em evid√™ncias cient√≠ficas."""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_predict 2048

TEMPLATE """{{ if .System }}System: {{ .System }}
{{ end }}{{ if .Prompt }}Instruction: {{ .Prompt }}
{{ end }}Response:"""
```

#### 5. Importar no Ollama

```bash
cd fine_tuning/outputs/gguf
ollama create biobyia -f Modelfile
```

#### 6. Testar

```bash
ollama run biobyia "Qual √© o papel das mitoc√¥ndrias na apoptose?"
```

## ‚öôÔ∏è Configurar Backend

### 1. Atualizar .env

Adicione ao `backend/.env`:

```env
# Provider BiobyIA (Ollama)
BIOBYIA_MODEL=biobyia
BIOBYIA_BASE_URL=http://localhost:11434
BIOBYIA_TEMPERATURE=0.7
BIOBYIA_MAX_TOKENS=2048

# Usar BiobyIA como provider padr√£o
LLM_PROVIDER=biobyia
```

### 2. Verificar Ollama est√° Rodando

```bash
# Iniciar Ollama (se n√£o estiver rodando)
ollama serve

# Verificar modelos dispon√≠veis
ollama list

# Deve aparecer: biobyia
```

### 3. Reiniciar Backend

```bash
cd backend
npm run dev
```

## üß™ Testar Integra√ß√£o

### Via API

```bash
curl -X POST http://localhost:4000/api/medical-assistant/query \
  -H "Content-Type: application/json" \
  -d '{
    "doctorId": "seu_doctor_id",
    "patientId": "seu_patient_id",
    "query": "Qual √© o tratamento para hipertens√£o?",
    "queryType": "general_medical"
  }'
```

### Via Script de Teste

```bash
cd backend
node scripts/testMedicalAssistant.js
```

## üîç Troubleshooting

### Erro: "Model not found"

- Verifique se o modelo foi criado: `ollama list`
- Verifique se o nome est√° correto no `.env`: `BIOBYIA_MODEL=biobyia`

### Erro: "Connection refused"

- Verifique se Ollama est√° rodando: `ollama serve`
- Verifique a URL no `.env`: `BIOBYIA_BASE_URL=http://localhost:11434`

### Erro: "Out of memory"

- Use quantiza√ß√£o Q4_0 ou Q8_0
- Reduza `BIOBYIA_MAX_TOKENS` no `.env`
- Feche outros aplicativos que usam mem√≥ria

### Erro: "Model base not accessible"

- Solicite acesso em: https://huggingface.co/meta-llama/Meta-Llama-3-8B
- Aguarde aprova√ß√£o (pode levar alguns dias)

### Erro: "LoRA model not found"

- Verifique se aceitou condi√ß√µes: https://huggingface.co/vitateje/biobyia
- Fa√ßa login: `hf auth login`
- Verifique se o token tem permiss√£o

## üìä Recursos Necess√°rios

### Para Merge

- **RAM**: M√≠nimo 16GB (recomendado 32GB+)
- **Disco**: ~16GB (modelo base + merged)
- **GPU**: Opcional, mas acelera o processo

### Para Ollama

- **RAM**: 8-16GB (depende da quantiza√ß√£o)
- **Disco**: 4-8GB (modelo quantizado)
- **CPU**: Qualquer CPU moderno funciona

## üìö Refer√™ncias

- [Ollama Documentation](https://ollama.ai/docs)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [HuggingFace PEFT](https://huggingface.co/docs/peft)
- [Modelo LoRA](https://huggingface.co/vitateje/biobyia)

## ‚úÖ Checklist Final

- [ ] Depend√™ncias instaladas
- [ ] Autenticado no HuggingFace
- [ ] Condi√ß√µes do modelo aceitas
- [ ] Merge executado com sucesso
- [ ] Modelo convertido para GGUF (se necess√°rio)
- [ ] Modelo importado no Ollama
- [ ] Teste local funcionando
- [ ] `.env` configurado
- [ ] Backend testado com BiobyIA

---

**Pronto!** Seu modelo fine-tunado est√° configurado e pronto para uso no backend! üéâ

