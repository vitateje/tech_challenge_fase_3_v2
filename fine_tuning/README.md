# ğŸ¥ Fine-Tuning de Modelo LLM para DomÃ­nio MÃ©dico

Este diretÃ³rio contÃ©m o pipeline completo de **prÃ©-processamento e fine-tuning** de modelos LLM para tarefas de question-answering mÃ©dico baseado em evidÃªncias cientÃ­ficas.

## ğŸ“– VisÃ£o Geral

O pipeline completo transforma o dataset mÃ©dico bruto (`ori_pqal.json`) do PubMedQA em um modelo LLM fine-tunado especializado em medicina, passando por:

1. **PrÃ©-Processamento**: AnonimizaÃ§Ã£o, formataÃ§Ã£o e validaÃ§Ã£o de dados
2. **FormataÃ§Ã£o Alpaca**: ConversÃ£o para formato padrÃ£o de fine-tuning
3. **Fine-Tuning**: Treinamento do modelo com LoRA (Low-Rank Adaptation)
4. **InferÃªncia**: Teste e uso do modelo treinado

---

## ğŸ“ Estrutura do Projeto

```
fine_tuning/
â”œâ”€â”€ preprocessing/                    # MÃ³dulo de prÃ©-processamento
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processor.py            # Processamento de dados mÃ©dicos
â”‚   â”œâ”€â”€ validate_data.py             # ValidaÃ§Ã£o de dados processados
â”‚   â””â”€â”€ format_dataset.py            # FormataÃ§Ã£o para Alpaca
â”œâ”€â”€ training/                        # MÃ³dulo de treinamento
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ finetuning_medical.py        # Script Python de fine-tuning
â”‚   â”œâ”€â”€ finetuning_medical.ipynb     # Notebook de fine-tuning
â”‚   â””â”€â”€ model_config.py              # ConfiguraÃ§Ãµes centralizadas
â”œâ”€â”€ inference/                       # MÃ³dulo de inferÃªncia
â”‚   â””â”€â”€ test_model.py                # Teste do modelo treinado
â”œâ”€â”€ utils/                           # UtilitÃ¡rios
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ prompts.py                   # Templates de prompts
â”œâ”€â”€ prepare-medical-data.ipynb       # Notebook de prÃ©-processamento
â”œâ”€â”€ run_pipeline.py                  # Pipeline completo
â”œâ”€â”€ medical_tuning_data.json         # Dataset processado (gerado)
â”œâ”€â”€ formatted_medical_dataset.json   # Dataset formatado Alpaca (gerado)
â””â”€â”€ README.md                        # Este arquivo
```

---

## ğŸš€ Guia RÃ¡pido

### Pipeline Completo (Recomendado)

```bash
cd fine_tuning

# 1. PrÃ©-processamento + FormataÃ§Ã£o
python run_pipeline.py --all

# 2. Fine-Tuning (escolha uma opÃ§Ã£o)
# OpÃ§Ã£o A: Notebook (recomendado para exploraÃ§Ã£o)
jupyter notebook training/finetuning_medical.ipynb

# OpÃ§Ã£o B: Script Python
python training/finetuning_medical.py

# 3. Teste do Modelo
python inference/test_model.py --model_path lora_model_medical
```

---

## ğŸ“‹ Parte 1: PrÃ©-Processamento

### Objetivos

- âœ… **AnonimizaÃ§Ã£o** de dados sensÃ­veis (conformidade LGPD/HIPAA)
- âœ… **FormataÃ§Ã£o** em estrutura de instruÃ§Ã£o para modelos LLM
- âœ… **ValidaÃ§Ã£o** de qualidade e integridade dos dados
- âœ… **Enriquecimento** com termos tÃ©cnicos mÃ©dicos (MESH)

### Pipeline de PrÃ©-Processamento

O pipeline Ã© composto por **6 etapas sequenciais**:

1. **ImportaÃ§Ã£o de Bibliotecas**: `json`, `re`, `Path`
2. **Carregamento do Dataset**: LÃª `ori_pqal.json` do PubMedQA
3. **AnonimizaÃ§Ã£o**: Remove dados sensÃ­veis (datas, IDs, telefones, emails)
4. **FormataÃ§Ã£o**: Transforma em formato de instruÃ§Ã£o com delimitadores
5. **Processamento Completo**: Aplica a todas as entradas
6. **Salvamento**: Gera `medical_tuning_data.json`

### Como Usar

#### OpÃ§Ã£o 1: Pipeline Completo

```bash
python run_pipeline.py --all
```

Executa: prÃ©-processamento â†’ validaÃ§Ã£o â†’ formataÃ§Ã£o Alpaca

#### OpÃ§Ã£o 2: Notebook Jupyter

```bash
jupyter notebook prepare-medical-data.ipynb
```

Execute as cÃ©lulas sequencialmente.

#### OpÃ§Ã£o 3: Scripts Individuais

```bash
# Apenas prÃ©-processamento
python preprocessing/data_processor.py

# Apenas validaÃ§Ã£o
python preprocessing/validate_data.py

# Apenas formataÃ§Ã£o Alpaca
python preprocessing/format_dataset.py
```

### Formato dos Dados

**Entrada (ori_pqal.json):**
```json
{
  "21645374": {
    "QUESTION": "Do mitochondria play a role in remodelling plant leaves?",
    "CONTEXTS": ["Programmed cell death (PCD) is..."],
    "LONG_ANSWER": "Results depicted mitochondrial dynamics...",
    "MESHES": ["Mitochondria", "Apoptosis"]
  }
}
```

**SaÃ­da IntermediÃ¡ria (medical_tuning_data.json):**
```json
[
  {
    "id": "21645374",
    "input": "INSTRUÃ‡ÃƒO MÃ‰DICA: ...\n[|Contexto|] ... [|eContexto|]\n[|Pergunta|] ... [|ePergunta|]\n[|Resposta|] ... [|eResposta|]"
  }
]
```

**SaÃ­da Final (formatted_medical_dataset.json):**
```json
{
  "instruction": ["Responda Ã  pergunta baseando-se nos contextos..."],
  "input": ["Contexto: ...\nPergunta: ..."],
  "output": ["Results depicted mitochondrial dynamics..."]
}
```

---

## ğŸ¯ Parte 2: Fine-Tuning

### Requisitos

- **GPU**: MÃ­nimo 8GB VRAM (recomendado 16GB+)
- **CUDA**: Instalado e configurado
- **Bibliotecas**:
  ```bash
  pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'
  pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes
  pip install transformers datasets
  ```

### Pipeline de Fine-Tuning

1. **Carregamento do Dataset**: LÃª `formatted_medical_dataset.json`
2. **Carregamento do Modelo**: Modelo base prÃ©-quantizado (Unsloth)
3. **ConfiguraÃ§Ã£o LoRA**: Adiciona adaptadores LoRA ao modelo
4. **FormataÃ§Ã£o de Prompts**: Aplica template Alpaca mÃ©dico
5. **PreparaÃ§Ã£o do Dataset**: Formata todos os exemplos
6. **ConfiguraÃ§Ã£o do Trainer**: SFTTrainer com hiperparÃ¢metros
7. **Treinamento**: Executa fine-tuning
8. **Salvamento**: Salva adaptadores LoRA treinados
9. **Teste**: Valida qualidade do modelo

### Como Usar

#### OpÃ§Ã£o 1: Notebook Jupyter (Recomendado)

```bash
jupyter notebook training/finetuning_medical.ipynb
```

**Vantagens:**
- VisualizaÃ§Ã£o interativa de cada etapa
- Facilita debugging e ajustes
- ComentÃ¡rios detalhados em cada cÃ©lula

**Ordem de execuÃ§Ã£o:**
1. Execute as cÃ©lulas sequencialmente (de cima para baixo)
2. Aguarde o treinamento (pode levar horas)
3. Verifique os resultados

#### OpÃ§Ã£o 2: Script Python

```bash
python training/finetuning_medical.py
```

**Vantagens:**
- ExecuÃ§Ã£o automatizada
- Melhor para produÃ§Ã£o
- Logs detalhados

### ConfiguraÃ§Ãµes

As configuraÃ§Ãµes estÃ£o centralizadas em `training/model_config.py`:

**Modelo:**
- PadrÃ£o: `unsloth/llama-3-8b-bnb-4bit`
- Outros disponÃ­veis: Mistral, Phi-3, Gemma

**LoRA:**
- Rank: 16
- Alpha: 16
- Dropout: 0

**Treinamento:**
- Learning rate: 2e-4
- Max steps: 100
- Batch size: 2 (por dispositivo)
- Gradient accumulation: 4

**Ajustar configuraÃ§Ãµes:**
Edite `training/model_config.py` antes de executar o fine-tuning.

### HiperparÃ¢metros Recomendados

| ParÃ¢metro | Valor Recomendado | DescriÃ§Ã£o |
|-----------|-------------------|-----------|
| Learning Rate | 1e-4 a 5e-4 | Taxa de aprendizado |
| LoRA Rank | 8-64 | Capacidade do adaptador |
| Max Steps | 50-200 | NÃºmero de steps de treinamento |
| Batch Size | 2-8 | Depende da GPU |
| Gradient Accumulation | 4-16 | Simula batch maior |

---

## ğŸ§ª Parte 3: Teste e InferÃªncia

### Testar Modelo Treinado

```bash
# Modo exemplos (testa com exemplos prÃ©-definidos)
python inference/test_model.py --model_path lora_model_medical --mode examples

# Modo interativo (perguntas customizadas)
python inference/test_model.py --model_path lora_model_medical --mode interactive
```

### Exemplo de Uso ProgramÃ¡tico

```python
from unsloth import FastLanguageModel
from utils.prompts import get_medical_alpaca_prompt, get_instruction_only

# Carrega modelo
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="lora_model_medical",
    max_seq_length=2048,
    load_in_4bit=True,
)
FastLanguageModel.for_inference(model)

# Prepara prompt
instruction = get_instruction_only()
input_text = "Contexto: ...\nPergunta: Qual o tratamento para hipertensÃ£o?"
prompt = get_medical_alpaca_prompt(instruction, input_text, "")

# Gera resposta
inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=256)
response = tokenizer.batch_decode(outputs)[0]
```

---

## ğŸ“Š Formato Alpaca para Fine-Tuning

O formato Alpaca Ã© o padrÃ£o usado para fine-tuning de modelos LLM:

```
Below is a medical instruction that describes a task, paired with medical context and a question. Write a response that appropriately completes the request based on the provided medical evidence.

### Instruction:
{instruÃ§Ã£o}

### Input:
{contexto + pergunta}

### Response:
{resposta esperada}
```

**Por que Alpaca?**
- âœ… PadrÃ£o amplamente adotado
- âœ… CompatÃ­vel com modelos prÃ©-treinados (LLaMA, Mistral, etc.)
- âœ… Estrutura clara para o modelo entender a tarefa
- âœ… Suportado por bibliotecas (Unsloth, TRL, Hugging Face)

---

## ğŸ”’ AnonimizaÃ§Ã£o de Dados

### PadrÃµes Identificados e SubstituÃ­dos

| PadrÃ£o Original | Placeholder | Exemplo |
|----------------|-------------|---------|
| Datas (DD/MM/YYYY) | `[DATA]` | `15/03/2024` â†’ `[DATA]` |
| Datas (YYYY-MM-DD) | `[DATA]` | `2024-03-15` â†’ `[DATA]` |
| IDs de pacientes | `[PACIENTE_ID]` | `ID: 12345` â†’ `ID: [PACIENTE_ID]` |
| Telefones | `[TELEFONE]` | `11987654321` â†’ `[TELEFONE]` |
| Emails | `[EMAIL]` | `email@hospital.com` â†’ `[EMAIL]` |

### Conformidade Legal

- âœ… **LGPD** (Lei Geral de ProteÃ§Ã£o de Dados - Brasil)
- âœ… **HIPAA** (Health Insurance Portability and Accountability Act - EUA)
- âœ… ProteÃ§Ã£o de dados pessoais de pacientes
- âœ… PrevenÃ§Ã£o de vazamento de informaÃ§Ãµes sensÃ­veis

---

## ğŸ“ˆ EstatÃ­sticas e Performance

### Tempo de Processamento

**PrÃ©-Processamento:**
- Dataset pequeno (< 1.000 entradas): ~30 segundos
- Dataset mÃ©dio (1.000 - 10.000 entradas): ~2-5 minutos
- Dataset grande (> 10.000 entradas): ~10-30 minutos

**Fine-Tuning:**
- Depende do tamanho do dataset e GPU
- Com LoRA: ~1-4 horas (GPU moderna)
- Sem LoRA: ~8-24 horas (requer muito mais memÃ³ria)

### Uso de MemÃ³ria

**PrÃ©-Processamento:**
- MÃ­nimo: 4GB RAM
- Recomendado: 8GB+ RAM

**Fine-Tuning:**
- Com LoRA + 4-bit: 8-16GB VRAM
- Sem LoRA: 40GB+ VRAM (modelos grandes)

### Taxa de Sucesso

- PrÃ©-processamento: > 99% de entradas processadas
- Fine-tuning: Depende da qualidade dos dados e configuraÃ§Ãµes

---

## ğŸ› Troubleshooting

### PrÃ©-Processamento

**Erro: Arquivo nÃ£o encontrado**
```
SoluÃ§Ã£o: Verifique o caminho em run_pipeline.py ou ajuste manualmente
```

**Erro: MemÃ³ria insuficiente**
```
SoluÃ§Ã£o: Processe em lotes ou aumente RAM disponÃ­vel
```

### Fine-Tuning

**Erro: CUDA out of memory**
```
SoluÃ§Ã£o:
1. Reduza batch_size em model_config.py
2. Aumente gradient_accumulation_steps
3. Use modelo menor
4. Reduza max_seq_length
```

**Erro: Dataset nÃ£o encontrado**
```
SoluÃ§Ã£o: Execute primeiro python run_pipeline.py --all
```

**Erro: Import unsloth failed**
```
SoluÃ§Ã£o: Instale dependÃªncias:
pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'
```

**Loss nÃ£o diminui**
```
SoluÃ§Ã£o:
1. Verifique qualidade dos dados
2. Ajuste learning rate
3. Aumente nÃºmero de steps
4. Verifique se dataset estÃ¡ formatado corretamente
```

### InferÃªncia

**Erro: Modelo nÃ£o encontrado**
```
SoluÃ§Ã£o: Verifique caminho do modelo treinado
```

**Respostas de baixa qualidade**
```
SoluÃ§Ã£o:
1. Treine por mais steps
2. Ajuste hiperparÃ¢metros
3. Verifique qualidade do dataset
4. Use modelo base maior
```

---

## ğŸ“š ReferÃªncias e Recursos

### Datasets

- **PubMedQA**: Dataset de perguntas e respostas mÃ©dicas baseadas em evidÃªncias
- LocalizaÃ§Ã£o: `../context/pubmedqa-master/data/ori_pqal.json`
- Paper: [PubMedQA: A Dataset for Biomedical Research Question Answering](https://arxiv.org/abs/1909.06146)

### Bibliotecas

- **Unsloth**: Fine-tuning rÃ¡pido e eficiente
  - GitHub: https://github.com/unslothai/unsloth
- **Hugging Face Transformers**: Modelos de linguagem
  - Docs: https://huggingface.co/docs/transformers
- **TRL**: Training Reinforcement Learning
  - Docs: https://huggingface.co/docs/trl

### DocumentaÃ§Ã£o TÃ©cnica

- **Instruction Tuning**: TÃ©cnica de fine-tuning para modelos LLM
- **LoRA**: Low-Rank Adaptation para fine-tuning eficiente
- **Alpaca Format**: Formato padrÃ£o de instruÃ§Ã£o
- **LGPD**: Lei Geral de ProteÃ§Ã£o de Dados (Brasil)
- **HIPAA**: Health Insurance Portability and Accountability Act (EUA)
- **MESH**: Medical Subject Headings (vocabulÃ¡rio controlado)

---

## ğŸ”„ Fluxo Completo do Pipeline

```mermaid
graph TD
    A[ori_pqal.json] --> B[PrÃ©-Processamento]
    B --> C[medical_tuning_data.json]
    C --> D[FormataÃ§Ã£o Alpaca]
    D --> E[formatted_medical_dataset.json]
    E --> F[Fine-Tuning]
    F --> G[lora_model_medical]
    G --> H[Teste/InferÃªncia]
```

### Passo a Passo Detalhado

1. **PrÃ©-Processamento**
   ```bash
   python run_pipeline.py --preprocess
   ```
   - Carrega `ori_pqal.json`
   - Anonimiza dados sensÃ­veis
   - Formata com delimitadores
   - Gera `medical_tuning_data.json`

2. **FormataÃ§Ã£o Alpaca**
   ```bash
   python run_pipeline.py --format
   ```
   - Converte para formato Alpaca
   - Separa instruction, input, output
   - Gera `formatted_medical_dataset.json`

3. **Fine-Tuning**
   ```bash
   python training/finetuning_medical.py
   ```
   - Carrega modelo base
   - Configura LoRA
   - Treina com dados mÃ©dicos
   - Salva adaptadores LoRA

4. **Teste**
   ```bash
   python inference/test_model.py
   ```
   - Carrega modelo treinado
   - Testa com exemplos
   - Valida qualidade

---

## ğŸ“ Suporte

Para dÃºvidas ou problemas:

1. Verifique a seÃ§Ã£o **Troubleshooting** acima
2. Revise os comentÃ¡rios detalhados nos notebooks
3. Execute scripts de validaÃ§Ã£o para diagnosticar problemas
4. Verifique os logs de processamento/treinamento

---

## ğŸ“„ LicenÃ§a

Este cÃ³digo faz parte do projeto Medical Assistant e segue as mesmas diretrizes de licenciamento do projeto principal.

---

**Ãšltima atualizaÃ§Ã£o:** 2024

**VersÃ£o do pipeline:** 2.0 (com fine-tuning)
